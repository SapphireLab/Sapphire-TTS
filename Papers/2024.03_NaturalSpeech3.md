---
PDF: 
标题: 
作者:
机构:
代码: 
ArXiv: 
提出时间: 
出版社: 
发表期刊: 
发表时间: 
引文数量: 
被引次数: 
tags:
 - 语音合成
 - 情感语音
---

# NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models

## A.摘要

> While recent <term>large-scale text-to-speech (TTS)</term> models have achieved significant progress, they still fall short in speech quality, similarity, and prosody. Considering speech intricately encompasses various attributes (e.g., content, prosody, timbre,and acoustic details) that pose significant challenges for generation, a natural idea is to factorize speech into individual subspaces representing different attributes and generate them individually. Motivated by it, we propose <algo>NaturalSpeech 3</algo>, a TTS system with novel factorized diffusion models to generate natural speech in a zero-shot way. 
> Specifically, 
> 1. we design a neural codec with **Factorized Vector Quantization (FVQ)** to disentangle speech waveform into subspaces of content,prosody, timbre, and acoustic details; 
> 2. we propose a factorized diffusion model to generate attributes in each subspace following its corresponding prompt. With this factorization design, NaturalSpeech 3 can effectively and efficiently model the intricate speech with disentangled subspaces in a divide-and-conquer way. 
> 
> Experiments show that NaturalSpeech 3 outperforms the state-of-the-art TTS systems on quality, similarity, prosody, and intelligibility. Furthermore, we achieve better performance by scaling to 1B parameters and 200K hours of training data.

## 1.引言

> In recent years, significant advancements have been achieved in text-to-speech (TTS) synthesis.
> Traditional TTS systems ([Tacotron](), 2, [FastSpeech](), [NaturalSpeech]()) are typically trained on limited datasets recorded in studios,and thus fail to support high-quality zero-shot speech synthesis. Recent works ([NaturalSpeech2](), 6, [Mega-TTS]()] have made considerable progress for zero-shot TTS by largely scaling up both the corpus and the model sizes.
> However, the synthesis results of these large-scale TTS systems are not satisfactory in terms of voice quality, similarity, and prosody.
> The challenges of inferior results stem from the intricate information embedded in speech, since speech encompasses numerous attributes, such as content, prosody, timbre, and acoustic detail. Previous works using raw waveform [[VITS](), 9] and mel-spectrogram [1, 2, 10, 7, 11] as data representations suffer from these intricate complexities during speech generation. A natural idea is to factorize speech into disentangled subspaces representing different attributes and generate them individually.
> However, achieving this kind of disentangled factorization is non-trivial. Previous works [12, 13, 6]encode speech into multi-level discrete tokens using a neural audio codec [14, 15] based on residual vector quantization (RVQ). Although this approach decomposes speech into different hierarchical representations, it does not effectively disentangle the information of different attributes of speech across different RVQ levels and still suffers from modeling complex coupled information.
> To effectively generate speech with better quality, similarity and prosody, we propose a TTS system with novel factorized diffusion models to generate natural speech in a zero-shot way. 
> Specifically,
> 1. we introduce a novel neural speech codec with factorized vector quantization (FVQ), named FACodec, to decompose speech waveform into distinct subspaces of content, prosody, timbre, and acoustic details and reconstruct speech waveform with these disentangled representations, leveraging information bottleneck [16, 17], various supervised losses, and adversarial training [18] to enhance disentanglement; 
> 2. we propose a factorized diffusion model, which generates the factorized speech representations of duration, content, prosody, and acoustic detail, based on their corresponding prompts. This design allows us to use different prompts to control different attributes. The overview of our method, referred to NaturalSpeech 3, is shown in Figure 1.We decompose complex speech into subspaces representing different attributes, thus simplifying the modeling of speech representation. 

> This approach offers several advantages: 
> 1. our factorized diffusion model is able to learn these disentangled representations efficiently, resulting in higher quality speech generation; 
> 2. by disentangling timbre information in our FACodec, we enable our factorized diffusion model to avoid directly modeling timbre. This reduces learning complexity and leads to improved zero-shot speech synthesis; 
> 3. we can use different prompts to control different attributes, enhancing the controllability of NaturalSpeech 3.Benefiting from these designs, NaturalSpeech 3 has achieved significant improvements in speech quality, similarity, prosody, and intelligibility. 

> Specifically, 
> 1. it achieves comparable or better speech quality than the ground-truth speech on the LibriSpeech test set in terms of CMOS; 
> 2. it achieves a new SOTA on the similarity between the synthesized speech and the prompt speech (0.64 → 0.67 on Sim-O, 3.69 → 4.01 on SMOS); 
> 3. it shows a significant improvement in prosody compared to other TTS systems with −0.16 average MCD (lower is better), +0.21 SMOS; 
> 4. it achieves a SOTA on intelligibility (1.94 → 1.81 on WER). Furthermore, we demonstrate the scalability of NaturalSpeech3 by scaling it to 1B parameters and 200K hours of training data. Audio samples can be found in https://speechresearch.github.io/naturalspeech3.

## 2.背景

> In this section, we discuss the recent progress in TTS including: 
> 1. zero-shot TTS; 
> 2. speech representations in TTS;
> 3. generation methods in TTS; 
> 4. speech attribute disentanglement.

> **Zero-shot TTS** 
> Zero-shot TTS aims to synthesize speech for unseen speakers with speech prompts.
> We can systematically categorize these systems into four groups based on data representation and modelling methods: 
> 1. Discrete Tokens + Autoregressive [6, 19, 20]; 
> 2. Discrete Tokens + Nonautoregressive [13, 21, 22]; 
> 3. Continuous Vectors + Autoregressive [23]; 
> 4. Continuous Vectors + Non-autoregressive [NaturalSpeech2](), [VoiceBox](/Papers/2023.06_VoiceBox.md), 24, 25]. 

> Discrete tokens are typically derived from neural codec, while continuous vectors are generally obtained from mel-spectrogram or latents from audio autoencoder or codec. In addition to the aforementioned perspectives, we disentangle speech waveforms into subspaces based on attribute disentanglement and propose a factorized diffusion model to generate attributes within each subspace, motivated by the principle of divide-and-conquer. Meanwhile, we can reuse previous methods, employing discrete tokens along with autoregressive models.

> **Speech Representations in TTS** 
> Traditional works propose using prior-based speech representation such as raw waveform [26, 27, 28] or mel-spectrogram [29, 30, 3, 31]. Recently, large-scale TTS systems [6, 13, 5] leverage data-driven representation, i.e., either discrete tokens or continuous vectors form an auto-encoder [14, 15, 32]. However, these methods ignore that speech contains various complex attributes and encounter intricate complexities during speech generation. In this paper, we factorize speech into individual subspaces representing different attributes which can be effectively and efficiently modeled.

> **Generation Methods in TTS**
> Previous works have demonstrated that NAR-based models [3, 33, 34,7, 5, 11] enjoy better robustness and generation speed than AR-based models, because they explicitly model the duration and predict all features simultaneously. Instead, AR-based models [2, 30, 6, 23, 35]have better diversity, prosody, expressiveness, and flexibility than NAR-based models, due to their implicitly duration modeling and token sampling strategy. In this study, we adopt the NAR modeling approach and propose a factorized diffusion model to support our disentangled speech representations and also extend it to AR modeling approaches. This allows NaturalSpeech 3 to achieve better expressiveness while maintaining stability and generation speed.

> **Speech Attribute Disentanglement** 
> Prior works [36, 37, 38] utilize disentangled representation for speech generation, such as speech content from self-supervised pre-trained models [39, 40, 41],fundamental frequency, and timbre, but speech quality is not satisfying. Recently, some works explore attribute disentanglement in neural speech codec. SpeechTokenizer [42] uses HuBERT [43]for semantic distillation, aiming to render the first-layer RVQ representation as semantic information.
Disen-TF-Codec [44] proposes the disentanglement with content and timbre representation, and applies them for zero-shot voice conversion. In this paper, we achieve better disentanglement with more speech attributes including content, prosody, acoustic details and timbre while ensuring highquality reconstruction. We validate such disentanglement can bring about significant improvements in zero-shot TTS task.



## 3.算法
### 3.1.总体架构

> In this section, we present NaturalSpeech 3, a cutting-edge system for natural and zero-shot textto-speech synthesis with better speech quality, similarity and controllability. As shown in Figure 1, NaturalSpeech 3 consists of 1) a neural speech codec (i.e., FACodec) for attribute disentanglement;2) a factorized diffusion model which generates factorized speech attributes. Since the speech waveform is complex and intricately encompasses various attributes, we factorize speech into five attributes including: duration, prosody, content, acoustic details, and timbre. Specifically, although the duration can be regarded as an aspect of prosody, we choose to model it explicitly due to our non-autoregressive speech generation design. We use our internal alignment tool to alignment speech and phoneme and obtain phoneme-level duration. For other attributes, we implicitly utilize the factorized neural speech codec to learn disentangled speech attribute subspaces (i.e., content, prosody,acoustic details, and timbre). Then, we use the factorized diffusion model to generate each speech attribute representation. Finally, we employ the codec decoder to reconstruct the waveform with the generated speech attributes. We introduce the FACodec in Section 3.2 and the factorized diffusion model in Section 3.3.

### 3.2.FACodec for Attribute Factorization

#### 3.2.1.FACodec Model Overview

> We propose a factorized neural speech codec (i.e., FACodec2) to convert complex speech waveform into disentangled subspaces representing speech attributes of content, prosody, timbre, and acoustic details and reconstruct high-quality speech waveform from these.

As shown in Figure 2, our FACodec consists of a speech encoder, a timbre extractor, three factorized vector quantizers (FVQ) for content, prosody, acoustic detail, and a speech decoder. Given a speech x,1) following [14, 5], we adopt several convolutional blocks for the speech encoder with a downsample rate of 200 for 16KHz speech data (i.e., each frame corresponding to a 12.5ms speech segment) to obtain pre-quantization latent h; 2) the timbre extractor is a Transformer encoder which converts the output of the speech encoder h into a global vector ht representing the timbre attributes; 3)for other attribute i (i = p, c, d for prosody, content, and acoustic detail, respectively), we use a factorized vector quantizer (FVQi) to capture fine-grained speech attribute representation and obtain corresponding discrete tokens; 4) the speech decoder mirrors the structure of speech encoder but with much larger parameter amount to ensure high-quality speech reconstruction. We first add the representation of prosody, content, and acoustic details together and then fuse the timbre information by conditional layer normalization [45] to obtain the input z for the speech decoder. We discuss how to achieve better speech attribute disentanglement in the next section.3.2.2 Attribute Disentanglement Directly factorizing speech into different subspaces does not guarantee the disentanglement of speech.
In this section, we introduce some techniques to achieve better speech attribute disentanglement: 1)information bottleneck, 2) supervision, 3) gradient reverse, and 4) detail dropout. Please refer to Appendix B.1 for more training details.
Information Bottleneck. Inspired by [16, 17], to force the model to remove unnecessary information(such as prosody in content subspace), we construct the information bottleneck in prosody, content,and acoustic details FVQ by projecting the encoder output into a low-dimensional space (i.e., 8-dimension) and subsequently quantize within this low-dimensional space. This technique ensures that each code embedding contains less information, facilitating information disentanglement [32, 46].After quantization, we will project the quantized vector back to original dimension.
Supervision. To achieve high-quality speech disentanglement, we introduce supervision as auxiliary task for each attribute. For prosody, since pitch is an important part of prosody [37], we take the post-quantization latent zp to predict pitch information. We extract the F0 for each frame and use normalized F0 (z-score) as the target. For content, we directly use the phoneme labels as the target(we use our internal alignment tool to get the frame-level phoneme labels). For timbre, we apply speaker classification on ht by predicting the speaker ID.
Gradient Reversal. Avoiding the information leak (such as the prosody leak in content) can enhance disentanglement. Inspired by [47], we adopt adversarial classifier with the gradient reversal layer(GRL) [48] to eliminate undesired information in latent space. Specifically, for prosody, we apply phoneme-GRL (i.e., GRL layer by predicting phoneme labels) to eliminate content information;for content, since the pitch is an important aspect of prosody, we apply F0-GRL to reduce the prosody information for simplicity; for acoustic details, we apply both phoneme-GRL and F0-GRL to eliminate both content and prosody information. In addition, we apply speaker-GRL on the sum of zp, zc, zd to eliminate timbre.
Detail Dropout. We have the following considerations: 1) empirically, we find that the codec tends to preserve undesired information (e.g., content, prosody) in acoustic details subspace since there is no supervision; 2) intuitively, without acoustic details, the decoder should reconstruct speech only with prosody, content and timbre, although in low-quality. Motivated by them, we design the detail dropout by randomly masking out zd during the training process with probability p. With detail dropout, we achieve the trade-off of disentanglement and reconstruction quality: 1) the codec can fully utilize the prosody, content and timbre information to reconstruct the speech to ensure the decouple ability, although in low-quality; 2) we can obtain high-quality speech when the acoustic details are given.3.3 Factorized Diffusion Model3.3.1 Model Overview We generate speech with discrete diffusion for better generation quality. We have the following considerations: 1) we factorize speech into the following attributes: duration, prosody, content, and acoustic details, and generate them in sequential with specific conditions. Firstly, as we mentioned in Section 3.1, due to our non-autoregressive generation design, we first generate duration. Secondly,intuitively, the acoustic details should be generated at last; 2) following the speech factorization design, we only provide the generative model with the corresponding attribute prompt and apply discrete diffusion in its subspace; 3) to facilitate in-context learning in diffusion model, we utilize the codec to factorize speech prompt into attribute prompts (i.e., content, prosody and acoustic details prompt) and generate the target speech attribute with partial noising mechanism following [49, 13].For example, for prosody generation, we directly concatenate prosody prompt (without noise) and target sequence (with noise) and gradually remove noise from target sequence with prosody prompt.
With these thoughts, as shown in Figure 3, we present our factorized diffusion model, which consists of a phoneme encoder and speech attribute (i.e., duration, prosody, content, and acoustic details)diffusion modules with the same discrete diffusion formulation: 1) we generate the speech duration by applying duration diffusion with duration prompt and phoneme-level textural condition encoded by phoneme encoder. Then we apply the length regulator to obtain frame-level phoneme condition cph; 2) we generate prosody zp with prosody prompt and phoneme condition cph; 3) we generate content prosody zc with content prompt and use generated prosody zp and phoneme cph as conditions;4) we generate acoustic details zd with acoustic details prompt and use generated prosody, content and phoneme zp, zc, cph as conditions. Specifically, we do not explicitly generate the timbre attribute.
Due to the factorization design in our FACodec, we can obtain timbre from the prompt directly and do not need to generate it. Finally, we synthesize the target speech by combining attributes zp, zc, zd and ht and decoding it with codec decoder. We discuss the diffusion formulation in Section 3.3.2.

#### 3.3.2.Diffustion Formulation

### 3.4.与 NaturalSpeech 系列的关联

> NaturalSpeech 3 is an advanced TTS system of the NaturalSpeech series. Compared with the previous versions NaturalSpeech [4] and NaturalSpeech 2 [5], NaturalSpeech 3 has the following connections and distinctions:• Goal. The NaturalSpeech series aims to generate natural speech with high quality and diversity.
We approach this goal in several stages: 1) Achieving high-quality speech synthesis in singlespeaker scenarios. To this end, NaturalSpeech [4] generates speech with quality on par with human recordings and only tackles single-speaker recording-studio datasets (e.g., LJSpeech). 2) Achieving high-quality and diverse speech synthesis on multi-style, multi-speaker, and multi-lingual scenarios.
Both NaturalSpeech 2 [5] and NaturalSpeech 3 focus on speech diversity by exploring the zero-shot synthesis ability based on large-scale, multi-speaker, and in-the-wild datasets.• Architecture. The NaturalSpeech series shares the basic components such as encoder/decoder for waveform reconstruction and duration prediction for non-autoregressive speech generation.
Different from NaturalSpeech which utilizes flow-based generative models and NaturalSpeech2 which leverages latent diffusion models, NaturalSpeech 3 proposes the concept of factorized diffusion models to generate each factorized speech attribute in a divide-and-conquer way.• Speech Representations. Due to the complexity of speech waveform, the NaturalSpeech series uses an encoder/decoder to obtain speech latent for high-quality speech synthesis. NaturalSpeech utilizes naive VAE-based continuous representations, NaturalSpeech 2 leverages the continuous representations from the neural audio codec with residual vector quantizers, while NaturalSpeech3 proposes a novel FACodec to convert complex speech signal into disentangled subspaces (i.e.,prosody, content, acoustic details, and timbre) and reduces the speech modeling complexity

## 4.实验设置

## 5.结论


## 6.Boarder Impact

> Since our model could synthesize speech with great speaker similarity, it may carry potential risks in misuse of the model, such as spoofing voice identification or impersonating a specific speaker. We conducted the experiments under the assumption that the user agree to be the target speaker in speech synthesis. To prevent misuse, it is crucial to develop a robust synthesized speech detection model and establish a system for individuals to report any suspected misuse.

## R.参考文献