---
标题: "OpenELM: An Efficient Language Model Family with Open-source Training \rand Inference Framework"
作者: 
- Sachin Mehta
- Mohammad Hossein Sekhavat
- Qingqing Cao
- Maxwell Horton
- Yanzi Jin
- Chenfan Sun
- Iman Mirzadeh
- Mahyar Najibi
- Dmitry Belenko
- Peter Zatloukal
- Mohammad Rastegari
机构: Apple
代码: "https://github.com/apple/corenet"
ArXiv: https://arxiv.org/abs/2404.14619
提出时间: 2024-04-22
出版社: 
发表期刊: 
发表时间: 
引文数量: 56
被引次数: 
tags:
  - 大语言模型_LLM
DOI: 
aliases:
  - OpenELM
ArXiv最新版本: "1"
ArXiv最新时间: 2024-04-22
PageNum: 
Demo:
---

# OpenELM: An Efficient Language Model Family with Open-source Training and Inference Framework

| 作者  | 机构  |
| --- | --- |
|[Sachin Mehta](../Authors/Sachin_Mehta.md)|[Apple](../Institutions/Apple.md)|
|[Mohammad Hossein Sekhavat](../Authors/Mohammad_Hossein_Sekhavat.md)|[Apple](../Institutions/Apple.md)|
|[Qingqing Cao](../Authors/Qingqing_Cao.md)|[Apple](../Institutions/Apple.md)|
|[Maxwell Horton](../Authors/Maxwell_Horton.md)|[Apple](../Institutions/Apple.md)|
|[Yanzi Jin](../Authors/Yanzi_Jin.md)|[Apple](../Institutions/Apple.md)|
|[Chenfan Sun](../Authors/Chenfan_Sun.md)|[Apple](../Institutions/Apple.md)|
|[Iman Mirzadeh](../Authors/Iman_Mirzadeh.md)|[Apple](../Institutions/Apple.md)|
|[Mahyar Najibi](../Authors/Mahyar_Najibi.md)|[Apple](../Institutions/Apple.md)|
|[Dmitry Belenko](../Authors/Dmitry_Belenko.md)|[Apple](../Institutions/Apple.md)|
|[Peter Zatloukal](../Authors/Peter_Zatloukal.md)|[Apple](../Institutions/Apple.md)|
|[Mohammad Rastegari](../Authors/Mohammad_Rastegari.md)|[Apple](../Institutions/Apple.md)|

## Abstract·摘要

> The reproducibility and transparency of large language models are crucial for advancing open research, ensuring the trustworthiness of results, and enabling investigations into data and model biases, as well as potential risks.
> To this end, we release ==**OpenELM**==, a state-of-the-art open language model.
> ==**OpenELM**== uses a layer-wise scaling strategy to efficiently allocate parameters within each layer of the Transformer model, leading to enhanced accuracy.
> For example, with a parameter budget of approximately one billion parameters, ==**OpenELM**== exhibits a 2.36% improvement in accuracy compared to OLMo while requiring 2× fewer pre-training tokens.

> Diverging from prior practices that only provide model weights and inference code, and pre-train on private datasets, our release includes the complete framework for training and evaluation of the language model on publicly available datasets, including training logs, multiple checkpoints, and pre-training configurations.
> We also release code to convert models to MLX library for inference and fine-tuning on Apple devices.
> This comprehensive release aims to empower and strengthen the open research community, paving the way for future open research endeavors.

> Our source code along with pre-trained model weights and training recipes is available at https://github.com/apple/corenet.
> Additionally, ==**OpenELM**== models can be found on HuggingFace at: https://huggingface.co/apple/==**OpenELM**==.

## 1.Introduction·引言

> Transformer-based [48] large language models (LLM) are revolutionizing the field of natural language processing [7,46].
> These models are isotropic, meaning that they have the same configuration (e.g., number of heads and feed-forward network dimensions) for each Transformer layer.
> Though such isotropic models are simple, they may not allocate parameters efficiently inside the model.

> In this work, we develop and release ==**OpenELM**==, a family of pre-trained and fine-tuned models on publicly available datasets.
> At the core of ==**OpenELM**== lies layer-wise scaling [30], enabling more efficient parameter allocation across layers.
> This method utilizes smaller latent dimensions in the attention and feed-forward modules of the Transformer layers closer to the input, and gradually widening the layers as they approach the output.

> We release the complete framework, encompassing data preparation, training, fine-tuning, and evaluation procedures, alongside multiple pre-trained checkpoints and training logs, to facilitate open research.
> Importantly, ==**OpenELM**== outperforms existing open LLMs that are pre-trained using publicly available datasets (Tab.01). 
> For example, ==**OpenELM**== with 1.1 billion parameters outperforms OLMo [17], which has 1.2 billion parameters, by 2.36% while requiring 2× fewer pre-training tokens.

## 2.Pre-Training·预训练

> This section describes the framework, including model architecture (2.1), pre-training data (2.2), training hyper-parameters (2.3), and evaluation (2.4).

### 2.1.OpenELM Architecture·架构

> We adopt the decoder-only Transformer-based architecture.
> Following state-of-the-art LLMs, we:
> 1. do not use learnable bias parameters in any fully-connected (a.k.a., linear) layers, 
> 2. apply pre-normalization using RMSNorm [53] and also, use rotatory positional embedding (ROPE) [43] for encoding positional information, 
> 3. use Grouped Query Attention (GQA) [1] instead of Multi-Head Attention (MHA), 
> 4. replace the feed forward network (FFN) with SwiGLU FFN [41], 
> 5. use flash attention [13] for computing the scaled dot-product attention,
> 6. use the same tokenizer as LLama [46].

> Existing LLMs use the same configuration for each Transformer layer in the model, resulting in a uniform allocation of parameters across layers.
> Unlike these models, each Transformer layer in ==**OpenELM**== has a different configuration (e.g., number of heads and feed forward network dimension), resulting in variable number of parameters in each layer of the model.
> This lets ==**OpenELM**== to better utilize the available parameter budget for achieving higher accuracies.
> We implement this non-uniform allocation of parameters across layers using layer-wise scaling (also referred as block-wise scaling in [30]).

#### Layer-Wise Scaling·逐层缩放

> A standard Transformer layer is composed of multi-head attention (MHA) and feed-forward network (FFN).
> For non-uniform allocation of parameters in the Transformer layer, we adjust the number of attention heads and the FFN multiplier in each Transformer layer.

> Assume that the standard Transformer model with uniform parameter allocation has $N$ Transformer layers and the dimensionality of the input to each layer is $d_{model}$.
> The MHA has $n_h$ heads and dimension of each head is $d_h=\dfrac{d_{model}}{n_h}$.
> Also, the hidden dimension for FFN is $d_{FFN}= m\cdot d_{model}$, where $m$ is a scalar FFN multiplier.

> We introduce parameters $\alpha$ and $\beta$ to scale the number of attention heads $n_h$ and FFN multiplier $m$ per layer respectively.
> For the $i$-th layer, $n_h$ and $m$ are computed as 

$$
  n_h^i = \dfrac{\alpha^i\cdot d_{model}}{d_h},\quad m^i = \beta^i
$$

> where

$$
  \alpha^i = \alpha_{min} + \dfrac{\alpha_{max} - \alpha_{min}}{N-1}\cdot i, \quad \beta^i = \beta_{min} + \dfrac{\beta_{max} - \beta_{min}}{N-1}\cdot i, 0\leq i <N
$$

> Here, $\alpha_{min}$ and $\alpha_{max}$ are the hyper-parameters that allow us to scale the attention heads.
> Similarly, $\beta_{min}$ and $\beta_{max}$ let us to vary the width of FFN layers.
> Therefore, varying the configuration of standard Transformer layers using $\alpha$ and $\beta$ results in non-uniform allocation of parameters in the model.
> Note, setting $\alpha_{min}=\alpha_{max}= 1.0$ and $m_i= m$ produces the standard uniform Transformer model.

### 2.2.Pre-Training Data·预训练数据

> For pre-training, we use public datasets.
> Specifically, our pre-training dataset contains RefinedWeb [35], deduplicated PILE [15], a subset of RedPajama [11], and a subset of Dolma v1.6 [42], totaling approximately 1.8 trillion tokens.
> These details are also summarized in Tab.02.

#### On-the-Fly Tokenization & Data Filtering

> Unlike previous approaches that utilize pre-tokenized data [5,17], we filter and tokenize text data on-the-fly.
> This facilitates seamless experimentation with various tokenizers, thereby significantly simplifying prototyping and research endeavors.
> In our experiments, we use the same tokenizer as used in LLama [46].

> To filter out low-length sequences, we apply two filtering methods.
> The first method operates at the character-level, checking if the number of characters in the sequence is below a specified threshold.
> The second method operates at the token-level, where it examines whether the sequence contains fewer tokens than a specified threshold.
> Sequences that are shorter than either of these thresholds are skipped.
> In our experiments, we use 200 characters and 256 tokens as character and token-level filtering thresholds.

### 2.3.Training Details·训练细节

> We train ==**OpenELM**== variants for 350k iterations (or training steps) using CoreNet (formerly CVNets [29]).
> We use AdamW [28] as an optimizer.
> We use a cosine learning rate schedule [27], with warm up of 5k iterations, and decay the final learning rate down to 10% of maximum learning rate.
> We use a weight decay of 0.1 and gradient clipping of 1.0.
> We train four variants of ==**OpenELM**== (270M, 450M, 1.1B, and 3B), and for some, we use FSDP [56] and activation checkpointing [8].
> Please refer to Appendix A for additional pre-training details.

### 2.4.Evaluation Details·评估细节

> Following previous works, we evaluate the performance across different tasks using LM Evaluation Harness [16]:

> - Standard zero-shot tasks.
> We consider 7 standard common-sense reasoning tasks: ARC easy and challenge [10], BoolQ [9], HellaSwag [52], PIQA [6], SciQ [49], and WinoGrande [39].
> - OpenLLM leaderboard tasks.
> We use 5 tasks from OpenLLM leaderboard [4]: ARC challenge, HellaSwag, MMLU [20], TruthfulQA [24], and WinoGrande.
> - LLM360 leaderboard tasks.
> We use 7 tasks from LLM360 leaderboard [26] for evaluation: ARC challenge, CrowS-Pairs (English version) [32], HellaSwag, WinoGrande, MMLU, PIQA, and RACE [23].

> These evaluation frameworks, built on top of LM Evaluation Harness, allows us to comprehensively evaluate ==**OpenELM**== in terms of reasoning (e.g., ARC-c, HellaSwag, and PIQA), knowledge understanding (e.g., MMLU and RACE), and misinformation & bias (e.g., TruthfulQA and CrowS-Pairs).

> While there may be some overlap in tasks among these frameworks, they primarily differ in the few-shot settings, as outlined in Tab.03.

## 3.Experimental Results·实验结果

### Pre-Training Results·预训练结果

> We evaluate the performance of ==**OpenELM**== on zero-shot and few-shot settings (Tab.03).
> We compare ==**OpenELM**== with publicly available LLMs, namely PyThia [5], Cerebras-GPT [14], TinyLlama [54], OpenLM [18], MobiLlama [44], and OLMo [17].
> The works most closely related to ours are MobiLlama and OLMo.
> These models are trained on comparable dataset mixtures, with similar or larger number of pre-training tokens.

> In Fig.01, the accuracy of ==**OpenELM**== is plotted against training iterations for 7 standard zero-shot tasks.
> We observe an overall increase in accuracy with longer training durations across most tasks.
> Additionally, the checkpoint obtained by averaging the last five checkpoints, collected at intervals of 5000 iterations, demonstrates comparable or slightly better accuracy compared to the final checkpoint obtained after 350k iterations.
> This improvement is likely due to noise reduction through weight averaging.
> Consequently, we use the averaged checkpoint for our main evaluations in Tab.04, instruction tuning experiments in Tab.05, and parameter-efficient tuning experiments in Tab.06.

> The results in Tab.04 span across various evaluation frameworks, and highlights ==**OpenELM**==’s effectiveness over existing methods.
> For instance, an ==**OpenELM**== variant with 1.1 billion parameters achieves 1.28% (Tab.04a), 2.36% (Tab.04b), and 1.72% (Tab.04c) higher accuracy compared to OLMo with 1.2 billion parameters.

> Remarkably, ==**OpenELM**== achieves this level of accuracy while using 2× less pretraining data.

### Instruction Tuning Results·指令调优结果

> We use the cleaned variant of UltraFeedback [3, 12] dataset that consists of 60k prompts for instruction tuning.
> We do instruction tuning using Alignment Handbook library [47].
> For optimization, we use either the statistical rejection sampling method [25] or the direct preference optimization method [37].
> These sampling method details along with other hyper-parameters and fine-tuning details are given in Appendix B.

> Tab.05 shows that instruction tuning consistently improves ==**OpenELM**==’s average accuracy by 1-2% across different evaluation frameworks.

### Parameter-Efficient Fine-Tuning (PEFT) Results·参数高效微调结果

> We use the CommonSense reasoning training and evaluation setup [22].
> This setup provides 170k training samples across 8 multiple-choice datasets for PEFT studies with different methods, including LoRA [21] and DoRA [51].
> We integrate ==**OpenELM**== with these methods, and finetune the resulting model for three epochs using 8 NVIDIA H100 GPUs.
> Tab.06 shows that PEFT methods can be applied to ==**OpenELM**==.
> LoRA and DoRA deliver similar accuracy on average across the given CommonSense reasoning datasets.

## 4.Benchmarking·基准测试

### Hardware·硬件

> We benchmark on modern, consumer-grade hardware with BFloat16 as the data type.
> Specifically, CUDA benchmarks were performed on a workstation with an Intel i9-13900KF CPU, equipped with 64 GB of DDR5-4000 DRAM, and an NVIDIA RTX 4090 GPU with 24 GB of VRAM, running Ubuntu 22.04.
> PyTorch v2.2.2 [34] was used, with the most recent versions of models and the associated libraries.
> HuggingFace Transformers v4.39.3 [50] was used to benchmark HuggingFace models.
> We did not use Torch Inductor for model compilation.

> To benchmark ==**OpenELM**== models on the Apple silicon, we used an Apple MacBook Pro with an M2 Max system-on-chip and 64GiB of RAM, running macOS 14.4.1.
> We ported the code and the weights of ==**OpenELM**== to Apple MLX v0.10.0 [19].
> To maximize the throughput, lazy evaluation was used in MLX with 8 tokens evaluated at a time.

### Evaluation·评估

> We provide two separate measurements for token throughput (measured in terms of tokens processed per second): (1) prompt processing (pre-fill), and (2) token generation.
> Additionally, we also report the total combined throughput.
> We benchmark all models sequentially, and execute one full “dry run” generating 1024 tokens for the first model, since we found that this significantly increases the throughput of generation for subsequent models.
> Before measurement for each individual model, we warm up the model by executing a single forward pass to allow the frameworks to perform further auto-tuning, if any.
> In all experiments, we use key-value caching and generate 1024 tokens in addition to the prompt tokens in all tests.
> Static key-value cache was used whenever supported.
> The same prompt was used for all runs, resulting in prompt lengths of 35-36 tokens (depending on the tokenizer).

### Results·结果

> Tab.07a and Tab.07b shows the benchmarking results on GPU and MacBook Pro respectively.
> Despite ==**OpenELM**==’s higher accuracy for a similar parameter count, we observe that it is slower than OLMo.
> While the primary focus of this study is reproducibility rather than inference performance, we did comprehensive profiling to understand the bottlenecks.
> Our analysis reveals that a significant portion of ==**OpenELM**==’s processing time can be attributed to our naive implementation of RMSNorm (Tab.08).
> Specifically, naive RMSNorm implementation results in many individual kernel launches each of which processes a small input, rather than a launch of a single, fused kernel, as would be the case with e.g. LayerNorm.
> By replacing the naive RMSNorm with Apex’s RMSNorm [33], we observe a notable increase in ==**OpenELM**==’s throughput.
> However, a substantial performance gap persists compared to the models that use optimized LayerNorm, in part because (1) ==**OpenELM**== has 113 RMSNorm layers as compared to 33 LayerNorm layers in OLMo and (2) Apex’s RMSNorm is not optimized for small inputs.
> To further illustrate the performance degradation attributable to RMSNorm, we replaced the LayerNorm in OLMo with RMSNorm, and observed a significant drop in generation throughput.
> In future work, we plan to explore optimization strategies to further improve the inference efficiency of ==**OpenELM**==.

## 5.Conclusion·结论

> This work releases ==**OpenELM**==, a decoder-only Transformer-based open language model.
> The ==**OpenELM**== uses a layer-wise scaling method for efficient parameter allocation within the Transformer model,resulting in improved accuracy compared to existing models.
> Additionally, we have made the entire framework open-source, including training logs, multiple checkpoints, pre-training configurations, and MLX inference code.
> This extensive release aims to empower and strengthen the open research community, facilitating future research efforts.

## Broader Impact·更广泛的影响

> The release of ==**OpenELM**== models aims to empower and enrich the open research community by providing access to state-of-the-art language models.
> Trained on publicly available datasets, these models are made available without any safety guarantees.
> Consequently, there exists the possibility of these models producing outputs that are inaccurate, harmful, biased, or objectionable in response to user prompts.
> Thus, it is imperative for users and developers to undertake thorough safety testing and implement appropriate filtering mechanisms tailored to their specific requirements.