# CLaM-TTS: Improving Neural Codec Language Modeling for Zero-Shot Text-to-Speech

## Abstract

> With the emergence of neural audio codecs, which encode multiple streams of discrete tokens from audio, large language models have recently gained attention as a promising approach for zero-shot Text-to-Speech (TTS) synthesis. 
> Despite the ongoing rush towards scaling paradigms, audio tokenization ironically amplifies the scalability challenge, stemming from its long sequence length and the complexity of modelling the multiple sequences. 
> To mitigate these issues, we present CLaM-TTS that employs a probabilistic residual vector quantization to (1) achieve superior compression in the token length, and (2) allow a language model to generate multiple tokens at once, thereby eliminating the need for cascaded modeling to handle the number of token streams. 
> Our experimental results demonstrate that CLaM-TTS is better than or comparable to state-of-the-art neural codec-based TTS models regarding naturalness, intelligibility, speaker similarity, and inference speed. 
> In addition, we examine the impact of the pretraining extent of the language models and their text tokenization strategies on performances. 

## 1. Introduction
> Large language models (LLMs), characterized by a considerable number of model parameters and trained on massive text data, have demonstrated remarkable zero-shot learning capabilities (Brown et al., 2020; Chung et al., 2022; Kaplan et al., 2020). 
> While scaling paradigm affects not only the natural language processing domain but also other fields such as image generation (Ramesh et al., 2021; Saharia et al., 2022), image recognition (Radford et al., 2021), and speech recognition (Baevski et al., 2020b; Radford et al., 2023), significant challenges in their efficient training and inference simultaneously arise. 
> In the realm of image processing, discretizing image representation (Razavi et al., 2019; Ramesh et al., 2021; Esser et al., 2021) has been shown to mitigate these issues by effectively reducing the input length to a manageable size. 

> Language modeling in the speech domain has become feasible with the emergence of neural audio codecs (Zeghidour et al., 2021; D´ efossez et al., 2023) that enable high-fidelity audio tokenization. 
> For Text-to-Speech (TTS) synthesis, there have been several attempts to adopt the LLMs for zeroshot TTS, which namely synthesize the diverse speech of any human voice (Zhang et al., 2023; Wang et al., 2023; Kharitonov et al., 2023; Rubenstein et al., 2023). 
> These attempts move away from the previous research direction to train models on curated high-quality recording datasets and produce human-like voices on benchmark datasets (Li et al., 2019; Kim et al., 2021; Tan et al., 2024; Casanova et al., 2022). 
> It is demonstrated that, by training LLMs on tens of thousands of hours of diverse audio data, zero-shot adaptation can be accomplished with just a few seconds of audio input. 

> Despite the significant advancements in TTS at scale, it still poses challenges to further scale up the models. 
> Neural audio codecs typically generate multiple sequences of audio tokens. 
> For instance, Encodec (D´ efossez et al., 2023) encodes a 5-second speech into 8 sequences of 375 audio tokens. 
> Several work (Kharitonov et al., 2023; Borsos et al., 2023b) employ the semantic tokens from selfsupervised speech representation learning (Chung et al., 2021) as an intermediary between text and audio tokens. 
> Although semantic tokens compress information more concisely than audio tokens, a 5-second speech segment still demands 125 semantic tokens, presenting a hurdle even setting aside the further complexities of audio token modeling from them. 

> In this work, we aim to bring the capability of efficient training and inference of large-language models within the TTS domain. 
> To this end, we propose an improved **C**odec **La**nguage **M**odel-based **TTS** (**CLaM-TTS**) system that encodes speech into multiple token sequences similar to existingmethods but in a more concise way. 
> With CLaM-TTS, all multiple tokens at each timestep in these sequences are generated through a single autoregressive step of a language model, eliminating the need for iterative generative modeling along the number of sequences. 
> The core of our method lies in the probabilistic discrete representation learning, ensuring that all discrete latent codes participate in the training process, resulting in a high-quality autoencoder for speech. 
> Furthermore, we provide a principled framework enabling a latent language model to efficiently generate a stack of tokens at once; the latent language model produces a continuous latent audio representation and converts it to a discrete representation with the proposed probabilistic quantization method. 
> We scale up the training dataset to 100K hours. 
> Our experimental findings indicate that CLaM-TTS either surpasses or is on par with leading zero-shot neural codec-based TTS models in aspects such as naturalness, intelligibility, speaker similarity, and inference speed. 
> Furthermore, we investigate how the depth of pretraining in the language models and their methods of text tokenization influence TTS outcomes. 
> Our generated samples are available on our [demo page](https://clam-tts.github.io).

## 2. Related Work
> **Neural audio codec**
> The neural discrete representation learning within a variational autoencoder(VAE) framework, called the vector-quantized VAE (VQ-VAE), has been proven effective in encoding raw-waveforms into discrete tokens (Baevski et al., 2020a), employed as a speech codec (Oord et al., 2017; Gˆ arbacea et al., 2019). 
> Similar to VQ-VAE, the neural audio codec methods usually use a framework that jointly trains an encoder, a decoder, and a quantizer (Li et al.; Zeghidour et al., 2021; Jiang et al., 2022; Jayashankar et al., 2022; D´ efossez et al., 2023; Kumar et al., 2023; Wu et al., 2023). 
> Zeghidour et al.(2021) pioneers using residual vector quantization (RVQ) (Gray, 1984; Vasuki & Vanathi, 2006; Lee et al., 2022) in a neural audio codec model. 
> It operates efficiently on clean and noisy speech and music, even at low bitrates. 
> EnCodec (D´ efossez et al., 2023) employs a similar model structure with improved training efficiency and stability to achieve a downsampling rate of 320 for input waveforms. 
> Kumar et al.(2023) identify the issue of codebook under-utilization in EnCodec and improve the codebook utilization with the techniques introduced in Yu et al.(2021) resulting in state-of-the-art performance as a neural audio codec. 

> Building on these advancements, we focus more on the discrete representation learning of speech rather than general audio and optimize the compression level to be suitable for the TTS task. 
> In other words, we compress mel-spectrograms rather than raw waveforms, delegating the task of converting mel-spectrograms back into raw waveforms to standard vocoders. 

> **Large-scale TTS**
> AudioLM (Borsos et al., 2023a) is a language model directly trained on audio to-kens. 
> In AudioLM, semantic tokens are first generated. 
> These tokens originate from self-supervised discrete speech representation methods (Hsu et al., 2021; Chung et al., 2021) that have previously been utilized for speech resynthesis or generation without text (Lakhotia et al., 2021; Polyak et al., 2021; Nguyen et al., 2023). 
> Following this, the model produces acoustic tokens of neural audio codes given the semantic tokens. 
> Wang et al.(2023) propose the first neural codec language model, Vall-E, for text-to-speech that utilizes a pre-trained neural audio codec, EnCodec (D´ efossez et al., 2023). 
> In a different approach, following AudioLM, text-to-speech has been realized by applying language modeling to generate the semantic tokens from text, as demonstrated by Kharitonov et al.(2023). 
> A shared characteristic among these neural codec language models is their two-stage pipeline; they autoregressively generate coarse-grained audio tokens and decode them into fine-grained representations. 
> Recent work in music generation hints at an efficient way to eliminate the second-stage modeling by interleaving audio tokens in a delayed pattern (Copet et al., 2023), but its application in TTS remains unexplored. 

> Given the complexities in modeling long audio sequences, several studies have incorporated phonemes and durations to alleviate the need for speech synthesizers to predict speech rates (Shen et al., 2023; Le et al., 2023; Jiang et al., 2023). 
> Some work shows that non-autoregressive generative models, such as a diffusion model and flow-matching (Ho et al., 2020; Lipman et al., 2023), can produce diverse and natural-sounding speech with large-scale training. 
> A hybrid method is utilized in another approach, employing non-autoregressive architecture except prosody modeling (Jiang et al., 2023). 
> This method aligns with previous work that applies VQ-VAEs to capture fine-grained speech features so that the prosody is controllable by them (Sun et al., 2020; Ren et al., 2022). 

> To address the challenges associated with neural codec language models while not relying on the phoneme and its duration that requires significant domain expertise, we design a language model that generates from coarse to fine-grained tokens without needing a two-stage pipeline. 
> Our approach is similar to recent work that utilizes pre-trained language models, Spectron (Nachmani et al., 2023) and SoundStorm (Borsos et al., 2023b). 
> While Spectron employs pre-trained transformer decoders to directly model the mel-spectrogram and then fine-tunes it, our method preserves the pre-trained text encoder and decodes speech tokens that are shorter than the mel-spectrogram using a latent transformer decoder. 
> SoundStorm freezes a pre-trained text encoder similar to ours, but it generates semantic tokens and subsequently decodes acoustic tokens using an iterative generative model. 

## 3. Preliminaries
> Building upon the approach proposed by Wang et al.(2023), we explore zero-shot TTS through the lens of neural codec language modeling task. 
> We consider a setting that includes two types of data: (i) text data and (ii) mel-spectrogram representation of its corresponding speech data, denoted by  
, respectively. 
> We model a sequence of $T$ discrete codes $ $ from latent representations $ $ of a mel-spectrogram $ $, using the framework of a Variational Autoencoder(VAE) with Residual Vector Quantization (RVQ). 
> Here, $ $ represents the $D$-depth of quantized, discrete codes. 
> We interchangeably use $ $ with $ $. 
> Subsequently, a neural language model $ $ is employed, aiming to predict $ $ from the text transcript $ $. 
> During the inferencephase, the language model generates $ $ for a given text $ $, which is subsequently transformed intospeech through the VAE decoder and a pre-trained vocoder. 

### 3.1. Residual Quantized Ariational Utoencoder  (RQ-VAE) 

> An RQ-VAE (Lee et al., 2022) is a neural network architecture representing data as discrete codes using residual vector quantization. 
> It comprises of three components: (1) an encoder parameterized  
 that maps data 
 into a sequence of latent representations 
 ; (2) a residual vector quantizer 
, converting the latent vector 
 at each time 
 into the discrete code representation 
 
, or the corresponding quantized embedding 
; and (3) a decoder parameterized by 
reconstructs the data  
 from a sequence of the quantized latent representations 
 represents the set 
, . 
> . 
> . 
> , c 
 indicating the total depth of the quantizer.The latent representation from the encoder is quantized through the multi-stage nearest-neighbour lookup over the codebook embeddings, of which the vocab size is  
 . 
> The process is defined asfinding the optimal code from the codebook, which minimizes the residual error at each depth  
 = arg min 
,...,V 
for all 
 corresponds to the 
-th embedding vector in the codebook at depth 
The sum of embeddings  
 becomes the quantized latent representation 
, which isconverted back to the input space through the decoder. 
> The codebook embeddings are updated with the clustered latents by the exponential moving average updates (Razavi et al., 2019). 

### 3.2. Mean-Field Variational Inference 

> Consider a latent variable model characterized by a joint distribution parameterized 
> Here,  denotes an observed random variable, indicates a set of latent random variable

> In this model, variational inference is a method to approximate the intractabledistribution  
 by solving an optimization problem with respect to parameters of approx-imate distribution  
. 
> We can derive a lower bound on the marginal log-likelihood $ $ known as the evidence lower bound (ELBO) (Blei et al., 2017):  
> $$
> 
> $$

> Mean-field variational inference (Koller & Friedman, 2009; Blei et al., 2017) is a specific approachof variational inference that assumes the independence among the latent variables conditioned on the observed variable: $ $.
> We can show that each optimal variationalposterior distribution $ $, which maximizes the ELBO, satisfies: 
> $$
>
> $$
>
> where $ $ denotes the latent variables of all depth $ $ except $ $. 
> An iterative coordinate ascentalgorithm based on Eq.2 can be used to update the distribution  
 (Bishop & Nasrabadi, 2006), andthe complexity of the algorithm mainly lies on the computation of the expectation over $ $.

## 4. Method
### 4.1. Mel-VAE
> We aim to develop a neural codec that can generate discrete speech codes within a short sequence length to make speech audios suitable for language model utilization. 
> To achieve this, we employ a RQ-VAE that compresses mel-spectrograms of speech audios (see Fig. 
> 1a). 
> We introduce a variational inference based method for learning residual codewords to address the codeword collapse issue found in conventional vector quantization methods (Kaiser et al., 2018; Roy et al., 2018; Zeghidour et al., 2021; Kumar et al., 2023). 
> We illustrate Mel-VAE similar to RQ-VAE following most of notations from Sec.3.1. 
> The encoder maps a mel-spectrogram $ $ into a sequence of latent representations 
 , and a residual vectorquantizer  
, converting the latent vector 
 at each time 
 into the discrete code representation 
, or its corresponding quantized embedding 
. 
> The decoder reconstructs themel-spectrogram  
 from a sequence of quantized latent representations 
With the assumptions that  
 is uniformly dis-tributed, mean-field variational inference yields the condition of such distribution as the following (see Eq.2):  
where the latents follows a normal distribution:  
However, the mutual interdependence of codes at every depth in the latter equation makes it difficult to solve it without an iterative approach. 
> Instead of using an iterative coordinate update approach, we approximate  
 pointwisely as 
 for all 
. 
> The posterior then has a form: 
We independently optimize the codebook embeddings at each depth  
, in a variational inferenceframework:  
The other modules of Mel-VAE, including the encoder parameters  
 and the decoder parameters 
are trained with commitment loss, reconstruction loss, and adversarial losses. 
>  
 corresponds to coefficients of the reconstruction loss, commitment loss, and ad-versarial losses, respectively. 
> For adversarial training, we adopt the multi-length discriminator (Chen et al., 2020) that distinguishes mel-spectrograms at different lengths and a modified multi-resolution spectrogram discriminator (Lee et al., 2023) that accepts mel-spectrogram rather than linear spectrogram. 
> We combine the least squares GAN objective (Mao et al., 2017) and the  
 feature matchingloss (Kumar et al., 2023) into a loss function denoted by  

### 4.2. Latent Language Modeling 

> We propose a conditional speech code language model given text $\pmb{x}$ aimed at enhancing the efficiency of the model. 
> This improvement stems from the insight that speech codes are generated through vector quantization. 
> Our approach leverages this by predicting the vector itself, which can then be converted into multiple tokens at each layer via residual vector quantization. 
> This is a departure from previous methods that predict speech code tokens sequentially. 

> Specifically, rather than directly predicting tokens from text, we consider a continuous latent representation $\pmb{z}_t$ of speech that can be converted into a speech code using residual vector quantization: 
> $$
> $$
>
> where $ $ indicate $ $, and we employ $ $, the probabilistic quantizer distribution learnedtogether with the Mel-VAE model (see Sec.4.1), as a replacement of $ $. 
> Here, wedefine the conditional distribution $ $ as a Gaussian mixture model: 

In this model, we can derive the following variational lower bound on the log-likelihood: 
)) + log 
for any 
. 
> The derivation of the lower bound and the definition of 
 are providedin Appendix A. 
> Here, we set  
With the second loss  
, which is associated with training a binary classifier to identify theend of speech (  
), the total loss for training the latent language model is the sum of the twolosses above:  
As shown in Fig. 
> 1, we implement an autoregressive latent model that yields three distinctive outputs:the mixture weights and the means of the Gaussian mixture distribution as well as the probability of concluding the generation. 
> Specifically, it incorporates a transformer decoder followed three parallel modules, comprising (1) a prediction layer with softmax activation for  
; (2) a predictionlayer for  
; (3) a binary classifier layer for 
 prediction. 
> Additionally, we use thepre-trained quantizer  
 of Mel-VAE.

### 4.3. Model Architecture and Inference  

> **Model Architecture**
> For the Mel-VAE, we adopt a causal 1d convolutional U-Net, a variant of themodel used in Ho et al. 
> (2020). 
> We remove the skip connections and attention layers and append 1-d ConvNeXt (Liu et al., 2022) blocks used in Siuzdak (2023) to the final layer of the decoder. 
> We employ 32-stage residual vector quantization with a codebook size of 1,024 for each depth. 
> For the text-to-code latent language model, we adopt a transformer-based encoder-decoder LM, especially a pre-trained ByT5-large (Xue et al., 2021a) similar to Borsos et al. 
> (2023b). 
> We keep the text encoder frozen. 
> Please refer to Appendix C for more detailed model configuration. 
>  

> **Inference**
> The text-to-code generation unfolds in three steps: (1) at time step 
, we randomly se
 from the distribution 
; (2) following this, randomly sample the latent vector 
. 
> Consequently, at time step 
, the discrete code is obtained through the learnedquantizer,  
; (3) if the probability of 
 exceeeds 0.5, conclude the generation, orproceed to step, otherwise. 
> Subsequently, the generated codes are decoded to mel-spectrograms using the decoder of Mel-VAE, then converted to raw-waveforms through an off-the-shelf pre-trained vocoder, BigVGAN (Lee et al., 2023). 

## 5. Experimental Setup 
Training Dataset 
We employ 100K hours of over 12K distinct speakers’ speech-transcript datasetspanning 11 languages: English, Korean, Chinese, Japanese, German, Dutch, French, Spanish, Italian, Portuguese, and Polish. 
> We train two models: (1) CLaM-en: an English-only model on 55Khour English dataset and (2) CLaM-multi: a multilingual model trained on 11-language dataset. 
> We provide details of dataset for each language in Appendix B.1, and data pre-processing in Appendix B.2 and B.3. 
>  
Training 
(1) Mel-VAE: We train the model on 4 NVIDIA A100 40GB GPUs for around 2Msteps. 
> Each GPU processes a size one minibatch containing concatenated mel-spectrograms of several audios. 
> We trim the trailing end to have it precisely 32,768 frames long. 
> We use Adam optimizer (Kingma & Ba, 2015) with a constant learning rate of 0.0002 throughout the training. 
> (2) Text-to-code: We train only the decoder and use a learned codebook from Mel-VAE. 
> The model is trained on 4 NVIDIA A100 40GB GPUs for around 4M steps with dynamic batching while keeping a maximum code size of 2,560. 
> We use AdamW optimizer (Loshchilov & Hutter, 2019), and the learning rate is fixed to 0.0002 throughout the training. 
> Throughout all our experiments, during the model inference, we sample  
 using top
 sampling (Holtzman et al., 2020) with 0.5 and 
 is sam-pled with temperature (Kingma & Dhariwal, 2018) of 2.6, which matches the empirical standard deviation in our validation dataset. 
>  
Baselines 
compare proposed following baselines: YourTTS (Casanova et al., 2022), a zero-shot TTS built on VITS (Kim et al., 2021) which is flowbased end-to-end TTS (representing  
 Conventional TTS 
), (2) Vall-E (Wang et al., 2023) and (3)SPEAR-TTS (Kharitonov et al., 2023) (representing  
 Neural Codec LM 
), and (4) VoiceBox (Leet al., 2023), a flow-matching-based TTS model trained on large-scale training data (representing  
Non-Autoregressive Model with Phoneme Input and Duration 
Metrics 
 Intelligibility and Robustness 
: We measure these attributes by character error rate(CER) and word error rate (WER) of the synthesized transcription from generated speech concerning the input text. 
> We follow the procedure in Wang et al. 
> (2023). 
> In English-only Evaluation, we synthesize the transcription by using the automatic speech recognition (ASR) model, the CTC-based HuBERT-Large (Hsu et al., 2021) model pre-trained on LibriLight (Kahn et al., 2020) and then fine-tuned on LibriSpeech (Panayotov et al., 2015). 
> In the Multilingual Evaluation, we use OpenAI’s Whisper (Radford et al., 2023) model . 
> We adopt NVIDIA’s NeMo-text-processing (Zhang et al., 2021; Bakhturina et al., 2022) for text normalization; (2)  
 Speak Similarity 
: We assess thespeaker similarity of two separate speech audio clips by following the same procedure outlined in Wang et al. 
> (2023). 
> We employ WavLM-TDCNN (Chen et al., 2022) which outputs the embed 
https://huggingface.co/google/byt5-large 
https://huggingface.co/facebook/hubert-large-ls960-ft 
https://github.com/openai/whisper/blob/main/model-card.md 
: ”large-v2” 
https://github.com/NVIDIA/NeMo-text-processing 
https://github.com/microsoft/UniSpeech/tree/main/downstreams/speaker_verification 
ding vector representing the speaker’s voice attribute. 
> We measure the cosine similarity between the two embedding vectors to get a score in  
, where a higher score indicates a higher speakersimilarity of the audios. 
> We borrow the definition of SIM-o and SIM-r from Le et al. 
> (2023). 
> SIM-o measures the similarity between the generated and the original target speeches, while SIM-r measures the similarity concerning the target speech reconstructed from the original speech by MelVAE and the pre-trained vocoder; (3)  
 Subjective Speech Quality 
: We measure the quality of thegenerated speech from human evaluations via three types of Mean Opinion Score (MOS) (Ribeiro et al., 2011): i) Quality MOS (QMOS) for an overall audio assessment, ii) Similarity MOS (SMOS) to measure speaker similarity between the prompt and the generated speech, and iii) Comparative MOS (CMOS) to compare our model with available baselines. 
> Detailed settings of subjective tests are described in Sec. 
> B.5. 
>  
We measure the performances of the proposed model under two different tasks: 1) 
 continu
: Given a text and corresponding initial 3 seconds of the Ground Truth speech as a prompt, thetask is to seamlessly synthesize the subsequent portion of the speech, and 2)  
 cross-sentence 
model is given a text, a 3-second speech segment, and its corresponding transcript (the transcript is different from the text). 
> The task is to synthesize a speech reading the text in the style of the provided 3-second speech. 
> We include our samples across the tasks discussed above, covering speaker diversity, text prompting, and other aspects, on our demo page. 
> XPERIMENTAL ESULTS NGLISH VALUATIONS  
Evaluation Methodology 
We evaluate performances of CLaM-en across 
 continuation 
 cross-sentence  
 tasks. 
> Following the evaluation setting in Wang et al. 
> (2023), we employ a subset of theLibriSpeech test-clean dataset. 
> This subset comprises speech clips ranging from 4 to 10 seconds, each with a corresponding transcript. 
> Note that YourTTS has official checkpoints, Vall-E has an unofficial checkpoint , and others do not have checkpoints. 
> We use checkpoints of YourTTS and Vall-E for evaluations. 
> We compare the other baselines with ours via the performances reported in their papers (Wang et al., 2023; Kharitonov et al., 2023; Le et al., 2023). 
> Since SPEAR-TTS and VoiceBox also evaluate using the same approach with Vall-E, they can be directly compared with our model as well. 
> Details of evaluation are provided in Appendix B.4. 
>  
Analysis 
Tab. 
> 1 and 2 show the results of 
 continuation 
 cross-sentence 
 task, respectively. 
> Oursoffers great performances for all measures, ranking either first or second, except SIM-r in crosssentence task. 
> It is worth noting that VoiceBox (Le et al., 2023), a phoneme-based duration model, shows better performances than ours. 
> However, it requires both phoneme and duration for speech synthesis, whereas our model directly employs a pretrained language model. 
> This allows for the seamless integration of LMs, which are trained across a broad spectrum of texts and tasks, enabling a plug-and-play methodology. 
> Experimental results of training several T5 variants is shown in Appendix D.2, illustrating the trade-off between leveraging the inherent capacity of LMs and ensuring robustness. 
> We also compare the end-to-end inference time for a 10-second utterance. 
> Our method is faster than the generation speed of Vall-E reported in Le et al. 
> (2023). 
> While ours is faster than VoiceBox with 64 decoding steps, VoiceBox can use fewer iterations of decoding steps. 
> Tab. 
> 3 presents the subjective audio evaluations. 
> CLaM-en significantly outperforms the baseline, YourTTS, in quality and intelligibility, as indicated by QMOS. 
> Our adherence to the prompt audio surpasses that of the baseline, as measured by the SMOS. 
> The comparative scores (CMOS) highlight CLaM-en’s proximity to the Ground Truth regarding naturalness, clarity, and comprehensibility. 
> Overall, CLaM-en’s generated speech naturalness, quality, intelligibility, and similarity exceed the baseline. 
> ULTILINGUAL VALUATIONS We evaluate our model, CLaM-multi trained on the multilingual dataset. 
> On the test set, we measure WER, CER, and SIM-o which are defined in Sec. 
> 5. 
> Here, we only consider  
 continuation 
 task inthis experiment since we cannot get full alignmnets between audio and text for all languages and datasets. 
> Tab. 
> 4 shows the partial results of the multilingual  
 continuation 
 task. 
> We sample a hundred 
https://github.com/lifeiteng/vall-e 
Table 1: Performances for the English-only  
 continuation 
 task. 
> The boldface indicates the bestresult, the underline denotes the second best, and the asterisk denotes the score reported in the baseline paper. 
> Ours offers great performances for all measures, ranking either first or second. 
> The inference time indicates the generation time of 10s speech. 
>  
Inference Time 
Ground Truth0.754* 0.754* YourTTS (Casanova et al., 2022) 0.3928 Vall-E (Wang et al., 2023) 0.452* 0.508*  
Vall-E (unofficial) 0.2875 0.3433 Voicebox (Le et al., 2023)  
0.593*0.616*  
6.4s* (64 NFE)CLaM-en  
0.47670.5128  
Table 2: Performances for the English-only 
 cross-sentence 
 task. 
> 
YourTTS (Casanova et al., 2022)7.92 (7.7*) 0.3755 (0.337*) Vall-E (Wang et al., 2023) 0.580* Vall-E (unofficial) 0.3031 0.3700 SPEAR-TTS (Kharitonov et al., 2023)  
0.560* Voicebox (Le et al., 2023)  
0.662*0.681*  
CLaM-en0.4951 0.5382 random samples from the test set of each dataset, ranging from 4 to 20 seconds, and average the scores of three trials. 
> Refer to Tab. 
> 10 for evaluation on other languages and datasets. 
> BLATION  
Effectiveness of Proposed RVQ 
To demonstrate the effect of the proposed RVQ on Mel-VAE,we conduct an ablation study by assessing speech reconstruction capability. 
> We train two MelVAEs: one with the proposed RVQ and the other with the baseline RVQ (D´ efossez et al., 2023). 
> We train both for 500k steps on the same training dataset described in Sec. 
> 5. 
> The generated speech is compared to the Ground Truth speech using two metrics: Perceptual Evaluation of Speech Quality (PESQ) (Rix et al., 2001) and Virtual Speech Quality Objective Listener (ViSQOL) (Chinen et al., 2020) in speech mode. 
> For evaluation, we randomly select 1,800 samples from the test set of each dataset, proportional to the size of each dataset, each being at least 3 seconds long. 
> The scores of these samples are then averaged for comparison. 
> Tab. 
> 5a shows that ours is more effective than the baseline RVQ. 
> See Fig. 
> 2 to verify the superior codebook usage of our approach. 
> We also compare the fully trained Mel-VAE with Encodec at 6K bitrates (D´ efossez et al., 2023), which is widely employed in neural codec language models (Wang et al., 2023; Zhang et al., 2023). 
> Tab. 
> 5b confirms that ours outperforms Encodec in speech reconstruction performance across both measures. 
>  
Comparision of Pre-trained LM and Input Variants 
Our language model is based on T5 (Raffelet al., 2020). 
> We conduct an ablation studty to compare T5, its variants and a phoneme encoder of comparable size. 
> The results indicate that ByT5 surpasses other T5 variants with the sole exception of the phoneme model. 
> This suggests that: 1) the more the pretraining phase is leveraged, the greater the potential increase in TTS performance, and 2) in moderate-sized language modeling, phonemes remain an effective input representation. 
> For the experimental results and a comprehensive analysis, refer to Appendix D.2. 
> In addition to the ablation studies presented, we have conducted further experiments detailed in the appendix, which explore the effects of codeword emitting rate on speech codec quality and language modeling as well as the scale of training data on model efficacy. 
> For comprehensive results and discussions, refer to Appendix D.3 and D.4. 
> ISCUSSION  
Choice of Codeword Rate 
Our approach enjoys a 10Hz codeword rate for efficient modeling.We set the codeword rate following the average phoneme rate in English speech (Roach, 2009) since phoneme is the minimum spoken unit. 
> Nevertheless, we conjecture this may have to be adTable 3: Human evaluations with 40 LibriSpeech test-clean speakers show CLaM-en’s speech generation surpasses the baseline in quality, intelligibility, similarity, and naturalness, nearing Ground Truth. 
> QMOS and SMOS scores include a 95% confidence interval. 
>  
CMOS (vs. 
> CLaM-en)  
YourTTS (Casanova et al., 2022) 
CLaM-en  
Ground Truth  
Table 4: Performances of CLaM-multi for the multilingual  
 continuation 
 task. 
> 
Language / Dataset 
English / MLS English0.4000 English (HuBERT) / MLS English 0.4000 German / MLS German 0.4219 Dutch / MLS Dutch 0.5983 French / MLS French 0.5671 Spanish / MLS Spanish 0.5292 Italian / MLS Italian 0.5459 Portuguese / MLS Portuguese 0.5658 Polish / MLS Polish 0.5519 Table 5: Effectiveness of our proposed RVQ. 
> The results show that ours outperforms the conventional RVQ in (a) and Encodec in (b) across both measures, even with a higher compression rate. 
>  
ViSQOL 
ours + BigVGAN 
RVQ + BigVGAN 
ViSQOL 
ours + BigVGAN 
Encodecjusted depending on the language or speaker. 
> A more compressed codeword rate, for example, 5Hz, might lead to more significant information loss than their efficiency. 
> There exists an efficiencyperformance tradeoff for rates above 10Hz, which can be optimized as needed. 
>  
Robustness 
We have noticed some words can be muddled, omitted, or repeated, which predom-inantly stems from autoregressive modeling. 
> We will address it by employing non-autoregressive architecture or improving the attention mechanism in future work. 
>  
Expressiveness 
100K hours of training data may not ensure a complete representation of all voicetypes, especially accentuated ones. 
> Our datasets predominantly capture audiobook reading styles, leading to limited diversity in speaking styles. 
> We believe that increasing the model and data size can significantly tackle the expressiveness challenges in zero-shot TTS. 
>  
Instruction Prompting 
We suggest various ways to use the full knowledge of the language model.One can incorporate speaker metadata into each transcript to perform various intriguing tasks. 
> Such tasks might include synthesizing speech or even conversations characterized by specific genders, voice ages, or accents. 
> We leave the other tasks for future work. 
> ONCLUSION We introduce CLaM-TTS, which leverages mean-field variational inference based probabilistic residual vector quantization (1) achieving significant compression in token length, and (2) allowing a latent language model to generate multiple tokens at once, thereby eliminating the need for cascaded modeling to handle the number of token streams. 
> We scale up the training dataset to 100K hours. 
> We empirically show that CLaM-TTS is better than or comparable to state-of-the-art neural codec-based TTS models regarding naturalness, intelligibility, speaker similarity, and inference speed. 
> CKNOWLEDGMENTS The authors would like to thank Kangwook Lee for helpful discussions, as well as Beomsoo Kim, Gibum Seo, and Dongwon Kim for their essential support throughout the processes of data handling and evaluation of the implementation. 
> TATEMENTS CLaM-TTS is a zero-shot TTS model that leverages a pre-trained large language model, offering efficient learning and inference at a vast scale. 
> The model’s capability to produce any voice and mimic with only minimal voice input presents potential dangers, including spoofing misuse. 
> Given the escalating risks associated with such models, it should be imperative to develop a detection model to identify audio outputs from the model and to establish a rigorous protocol for evaluating its effectiveness. 
> EPRODUCIBILITY TATEMENTS For the implementation of our model, we provide Fig. 
> 1 and description of the model architecture in Sec. 
> 4.3 along with the hyperparameters of the model configuration in Tab. 
> 7. 
> To ensure the reproducibility of our experiments, we also share details, including a list and statistics of our training data in Sec. 
> 5 and Appx. 
> B.1, data preprocessing procedures in Appx. 
> B.2 and Appx. 
> B.3, training configuration and the evaluation methodology in Sec. 
> 5. 
> If our potential legal concerns can be addressed, we are prepared to progressively disclose, for research purposes, the inference code, pre-trained weights, and ultimately, the full training implementation. 
