# BASE TTS

<details>
<summary>基本信息</summary>

- 标题:
- 作者:
  - ??
- 机构:
  - 机构 
- 时间:
  - 预印时间: 20??.??.?? ArXiv v1
  - 更新笔记: 20??.??.??
- 发表:
  - 期刊/会议 
- 链接:
  - [ArXiv]()
  - [DOI]()
  - [Github]()
  - [Demo]()
  - [Scholar](https://scholar.google.com/scholar?cluster=)
- 标签:
  - ?
- 页数: ?
- 引用: ?
- 被引: ?
- 数据:
  - ? 
- 对比:
  - ?
- 复现:
  - ?

</details>

## Abstract: 摘要

> We introduce a text-to-speech (TTS) model called BASE TTS, which stands for \textbf{B}ig \textbf{A}daptive \textbf{S}treamable TTS with \textbf{E}mergent abilities.
> BASE TTS is the largest TTS model to-date, trained on 100K hours of public domain speech data, achieving a new state-of-the-art in speech naturalness.
> It deploys a 1-billion-parameter autoregressive Transformer that converts raw texts into discrete codes ("speechcodes") followed by a convolution-based decoder which converts these speechcodes into waveforms in an incremental, streamable manner.
> Further, our speechcodes are built using a novel speech tokenization technique that features speaker ID disentanglement and compression with byte-pair encoding.
> Echoing the widely-reported "emergent abilities" of large language models when trained on increasing volume of data, we show that BASE TTS variants built with 10K+ hours and 500M+ parameters begin to demonstrate natural prosody on textually complex sentences.
> We design and share a specialized dataset to measure these emergent abilities for text-to-speech.
> We showcase state-of-the-art naturalness of BASE TTS by evaluating against baselines that include publicly available large-scale text-to-speech systems: YourTTS, Bark and TortoiseTTS.
> Audio samples generated by the model can be heard at \url{https://amazon-ltts-paper.com/}.

## 1.Introduction: 引言

> Generative deep learning models are progressing at a rapid pace.
> Natural Language Processing (NLP) and Computer Vision (CV) are undergoing a fundamental shift from specialized models with supervised training, to generalized models that can achieve miscellaneous tasks with limited explicit instruction \cite{sparks}.
> In NLP, tasks such as question answering, sentiment analysis and text summarization can now be performed by a large language model (LLM) which was not specifically targeted for these tasks \cite{brown2020language,openai2023gpt4,scao2022bloom,touvron2023llama,hoffmann2022training}.
> In CV, pre-trained models that learn from hundreds of millions of image-caption pairs have achieved top performance on image-to-text benchmarks \cite{clip1,tejankar2021fistful,shen2022k}, while delivering remarkably photo-realistic results in text-to-image tasks \cite{rombach2022high, ramesh2022hierarchical, nichol2022glide, saharia2022photorealistic, yu2022scaling}.
> This progress has been enabled by Transformer-based architectures \cite{originaltransformerpaper} that drive improvements using many orders of magnitude more data than previous models.
> Similar advances are now occurring in Speech Processing and Text-to-Speech (TTS), with models leveraging thousands of hours of data that push synthesis ever closer towards human-like speech.
> Some of these models, described in depth in Section \ref{sec:related_work}, rely on causal language modeling tasks, like AudioLM \cite{borsos2023audiolm} or VALL-E \cite{wang2023neural}, whereas others use non-causal modules, such as SoundStorm \cite{borsos2023soundstorm} or SpeechX \cite{wang2023speechx}, or diffusion decoders \cite{NaturalSpeech2,tortoise}.
>
> Until 2022, leading Neural TTS models were almost exclusively trained on a few hundreds of hours of recorded audio \cite{tacotron2, ecat,kim2021conditional,DBLP:conf/iclr/0006H0QZZL21,popov2021grad}.
> Such systems can create well-enunciated speech that is occasionally expressive for the target speakers, but typically cannot generalize beyond the small amount of training data to render ambiguous and complex texts with truly expressive spoken performance \cite{triantafyllopoulos2023overview,tahon2018can,schnell2022controllability}.
> To achieve such higher levels of expressiveness, TTS systems historically had to rely on labeled datasets for specific speech phenomena; and even so, achieving human-like prosody for certain types of textual inputs has remained elusive \cite{tan2021survey}.
> For example, in English, compound nouns and questions are notoriously hard to render correctly without accurate syntactic parsing and semantic understanding \cite{kenter2020improvingprosodywithbert}.
> 
> In this paper, we introduce BASE TTS: Big Adaptive Streamable TTS with Emergent abilities.
> It is a multi-lingual and multi-speaker Large TTS (LTTS) system trained on around 100K (doubling previous high in \cite{wang2023neural}) hours of public domain speech data.
> BASE TTS follows the approach of casting TTS as a next-token-prediction problem \cite{borsos2023audiolm,wang2023neural,tortoise}, inspired by the success of LLMs.
> This approach is usually applied in combination with large amount of training data to achieve strong multi-lingual and multi-speaker capabilities (e.g.
> one-shot voice cloning).
> Our goal is to improve general TTS quality and study how scaling affects the model's ability to produce appropriate prosody and expression for challenging text inputs, similar to how LLMs acquire new abilities through data and parameter scaling, a phenomenon known as "emergence" or "emergent abilities" in the LLM literature \cite{wei2022emergent, webb2023emergent}. \cite{wei2022emergent} defines \textit{emergent abilities of large language models} as "abilities that are not present in smaller-scale models but are present in large-scale models;" for example, they show that on a range of few-shot tasks, model capability stays at a low level from 10\textsuperscript{18} to 10\textsuperscript{22} training FLOPs, but makes a drastic jump from 10\textsuperscript{22} to 10\textsuperscript{24}.
> To test the hypothesis that this also holds for LTTS, we propose an evaluation scheme to assess potential emergent abilities in TTS, identifying seven categories that are challenging from the literature \cite{triantafyllopoulos2023overview,tahon2018can,schnell2022controllability,tan2021survey,kenter2020improvingprosodywithbert}: compound nouns, emotions, foreign words, paralinguistics, punctuations, questions, and syntactic complexities.
> See more details in Section \ref{sec:evaluation}.
>
> We design BASE TTS to model a joint distribution of text tokens followed by discrete speech representations, which we refer to as speechcodes.
> Discretization of speech through audio codecs \cite{vqvae,zeghidour2021soundstream,DBLP:journals/corr/abs-2210-13438} is central to our design, as it enables the direct application of methods developed for LLMs, which underlie recent works on LTTS \cite{borsos2023audiolm,wang2023neural,borsos2023soundstorm,wang2023speechx,NaturalSpeech2,tortoise, kharitonov2023speak}.
> Specifically, we model speechcodes using a decoder-only autoregressive Transformer with a cross-entropy training objective.
> Despite its simplicity, this objective can capture complex probability distributions of expressive speech, thus alleviating the oversmoothing problem seen in early neural TTS systems \cite{ren2022revisiting}.
> As an implicit language model, BASE TTS is also observed to make a qualitative jump in prosody rendering once a large enough variant is trained on sufficient data. 
>
> Further, we propose speaker-disentangled speechcodes that are built on top of a WavLM \cite{wavlm} Self-Supervised Learning (SSL) speech model.
> We follow \cite{borsos2023audiolm} which introduces semantic tokens constructed by discretizing activations of an SSL model.
> We extend this approach to better control information captured by the speechcodes.
> Our strategy is to limit the responsibilities of the autoregressive speechcode predictor ("speechGPT") to segmental contents, prosody, and duration, while designating a separate, speechcode-to-waveform decoder (called "speechcode decoder") with the reconstruction of speaker identity and recording conditions.
> We show that this convolution-based speechcode decoder is compute-efficient and reduces the whole-system synthesis time by over 70\% compared to the baseline diffusion-based decoder.
>
> Our main contributions are summarized as follows:
>
> - We introduce BASE TTS, which to our knowledge is the largest TTS model to date, featuring 1B parameters and trained on a dataset consisting of 100K hours of public domain speech data.
> In subjective evaluations, BASE TTS performs better than publicly available LTTS baseline models. 
> - We demonstrate how scaling BASE TTS to larger dataset and model sizes improves its capability to render appropriate prosody for complex texts.
> To this end, we develop and make available an "emergent abilities" testset that can serve as a subjective evaluation benchmark for text understanding and rendering of large-scale TTS models.
> We report performance on different variants of BASE TTS over this benchmark, showing monotonic improvement in quality with increased dataset size and parameter count.
> - We introduce novel discrete speech representations that are built on top of a WavLM SSL model, intended to capture only phonemic and prosodic information of the speech signal.
> We demonstrate that these representations outperform the baseline quantization method.
> We also show that they can be decoded to high quality waveforms with a simple, fast, and streamable decoder, despite high level of compression (only 400 bits/s).

## 2.Related Works: 相关工作

### Text-to-Speech as Language Modeling

> Casting TTS problem as next token prediction has gained popularity in recent years due to how easy it is to scale language models to large data and model sizes.
>
> The first model using this approach was TortoiseTTS \cite{tortoise} released in 2022.
> It combines a GPT2-style speechcode language model with a diffusion decoder and an off-the-shelf vocoder, achieving remarkable few-shot capability.
> VALL-E \cite{wang2023neural} followed a similar approach: the model is scaled to 60k hours of speech data and uses a pre-trained audio encoder EnCodec for speechcode extraction \cite{DBLP:journals/corr/abs-2210-13438}. 
> VALL-EX \cite{VALLEX} replicated VALL-E with 70k hours, with an additional cross-lingual speech-to-speech translation task in English and Chinese by using target language token as prompt.
> VioLa \cite{wang2023viola} extended VALL-EX with both text-to-text translation and speech-to-text recognition tasks.
> SpeechX \cite{wang2023speechx} extended VALL-E by adding noise suppression, target speaker extraction, clean and noisy speech editing, and speech removal tasks.
>
> While VALL-EX, VioLA and SpeechX are versatile models reporting improved performance on word error rate and speaker similarity, no effects on core TTS aspects such as naturalness or prosody improvement were reported.
> VALL-E reported significant improvements over industry baselines, but we are unable to compare as the model is not publicly available.
> Here, we propose a multi-lingual and multi-speaker TTS leveraging 100K hours of data, with strong proven naturalness results in English and Spanish.
> Evaluations show improved speech naturalness compared to Tortoise-TTS.
> Qualitative linguistic analysis shows "emergent abilities" \cite{wei2022emergent} - BASE TTS can render complex prosody patterns, taking cues from texts without explicit labels for emotions.

### Discrete Speech Representations

> In audio generative AI, the first acoustic encoders used in GPT-TTS are VQ-VAE \cite{vqvae}, SoundStream \cite{zeghidour2021soundstream} and EnCodec \cite{DBLP:journals/corr/abs-2210-13438}, which are deep learning audio compression techniques producing discrete acoustic token based on vector quantization.
> These acoustic encoders are shown to be superior to Mel-spectrograms \cite{DBLP:journals/corr/abs-2309-10922, yang2023towards}, but ignore semantic information and are unnecessarily complex due to lack of disentanglement \cite{huang2023repcodec}.
> AudioLM \cite{borsos2023audiolm} combines SoundStream tokens with semantic BERT tokens \cite{chung2021w2v}.
> SpeechTokenizer \cite{zhang2023speechtokenizer} and RepCodec \cite{huang2023repcodec} aim to capture semantics by adding self-supervised embedding prediction-based losses \cite{mohamed2022self}.
> BASE TTS builds acoustic tokens by leveraging semantic information from self-supervised embeddings and disentangling speaker information from the acoustic tokens.
> Then, it applies byte-pair encoding on these tokens to reduce memory requirement, allowing the model to train on longer sequences.

### LTTS Simplifies Modeling

> Since Tacotron~2 \cite{tacotron2}, a successful TTS paradigm has been to separate speech creation into three sub-systems: (1) a Frontend responsible for text normalization, grapheme-to-phoneme conversion and  SSML tag handling e.g.
> to mark emphasis \cite{DBLP:journals/corr/abs-2307-07062}, (2) a Mel-spectrogram generator, inferring the amplitudes of the speech signal and responsible for the model expressivity, and (3) a dedicated Vocoder \cite{DBLP:conf/ssw/OordDZSVGKSK16, jiao2021universal}, generating the waveform by inferring the phase information.
> Expert knowledge can be induced to improve its performance such as using pre-computed features \cite{DBLP:conf/iclr/0006H0QZZL21}, coarse or fine-grained prosody information (\cite{Copycat}, and style control \cite{DBLP:conf/naacl/PrateekLBDLMRW19}.
> It has also proven successful with architectures such as Transformers \cite{li2019neural, DBLP:conf/iclr/0006H0QZZL21}, Flows \cite{kim2021conditional}, and Diffusion \cite{popov2021grad}.
>
> These systems require a complex pipeline.
> Any error made during an earlier stage is propagated to the next.
> BASE TTS and other LTTS models are breaking these limitations.
> We simplify the data preparation by requiring only a large amount of audio, where texts can be obtained from a speech-to-text system, and require no phoneme extraction.
> GPT-based architectures \cite{radford2019language} have flourished by enabling versatile prompt-based task formulation, integration of expert feedback \cite{ouyang2022training}, and use as a foundational model with quick fine-tuning  \cite{hu2022lora, fu2023effectiveness} e.g.
> on a specific task, a set of speakers or a new locale.
> BASE TTS shows that an end-to-end approach can achieve high expressivity on a few audio examples and in a multilingual setting, marking a high bar of quality in Spanish.

### Contextual and Emotional TTS

> This usually requires separate, dedicated TTS sub-systems.
> Multiple approaches rely on predicting prosody representations using contextualized word embeddings \cite{copycat2, xin2023improving, wu2022self, DBLP:journals/corr/abs-2309-01576}.
> While prosody predictors of these models usually generate appropriate prosody, they often ignore strong text cues that would force a dramatic change e.g.
> in emotions or speech cues like shouting or whispering.
> Context-aware emotional TTS systems \cite{pan2021chapter, liu2023emotion, emotts, mukherjee2022text, schnell2021improving} are even more limited.
> They usually favor a text-based emotion predictor coupled with an emotion controlable TTS system.
> These systems require high-quality recordings with a forced speaking style and annotated audio and text data, limiting their usefulness due to the sheer number of emotions expressible in human speech \cite{cambria2012hourglass}.
> BASE TTS benefits from being a language model which is both acoustic and semantically/syntactically aware.
> In this paper, we systemize an approach to produce and evaluate emergent contextual understanding of the text with a wide range of styles, without requiring supervised training or annotation.

### BASE TTS has Data Efficiency Built-In

> Low-resource TTS has focused on reducing the required amount of high quality training data through e.g.
> voice conversion \cite{huybrechts2021low}, data generation \cite{DBLP:conf/icassp/LajszczakPKBBJN22}, modeling techniques \cite{DBLP:journals/corr/abs-2307-07062}, or recording script optimizations \cite{shamsi2020tts}.
> This stems from the early difficulty of ingesting a high volume of data from multiple speakers in potentially different styles or languages.
> A step up in that direction is zero-shot inference such as in Bark \footnote{\url{https://github.com/suno-ai/bark}} and \cite{casanova2022yourtts, wu2022adaspeech}, which aim to clone a voice with only a few seconds of recordings.
> Due to leveraging a substantially larger dataset, a bigger model, and a dedicated speaker encoder, BASE TTS and the recent LTTS models set a new standard in data efficiency.

## 3.Methodology: 方法

### 3.1.Overview: 概览

> Similar to recent works in speech modeling, we adopt an LLM-based method for the TTS task (\cref{fig:overview}).
> We consider a dataset $\mathcal{D} = \{ \mathbf{x}_i, \mathbf{y}_i \}_{i=0}^N$, where $\mathbf{y}$ is an audio sample and $\mathbf{x} = \{x_1, \cdots, x_T \}$ is the corresponding text transcription. 
> The audio $\mathbf{y} = \{y_1, \cdots, y_S \}$ is represented by a sequence of $S$ discrete tokens (speechcodes), learnt using a separately trained speech tokenizer. 
> We use a Transformer-based autoregressive model with parameters $\phi$ in order to learn the joint probability of the text and audio sequences:
>
> $$
    p(\mathbf{y}, \mathbf{x}) = p(\mathbf{y} | \mathbf{x}) p(\mathbf{x}) = \prod_{s=1}^S p(\mathbf{y}_s | \mathbf{y}_{<s}, \mathbf{x}; \phi) \prod_{t=1}^T p(\mathbf{x}_t | \mathbf{x}_{<t}; \phi).
> $$
>
> The predicted speech tokens are concatenated with speaker embeddings and decoded into waveforms using a separately trained speechcode decoder consisting of linear and convolutional layers. 

### 3.2.Discrete Speech Representations: 离散语音表示

> Discrete representation is foundational to the success of LLM, but identifying a compact and informative representation is less obvious in speech than in text, and less explored.
> For BASE TTS, we first experiment with a  Vector Quantized Variational Autoencoder (VQ-VAE) baseline \cite{vqvae}, where an auto-encoder based architecture reconstructs mel-spectrograms through a discrete bottleneck, as described in Section \ref{sec:vqvae}.
> VQ-VAE has been a successful paradigm in speech and image representation, and especially as a unit of modeling for TTS \cite{tjandra2020vqvae, tortoise}.
>
> In Section \ref{sec:wavlm}, however, we introduce a novel method of learning speech representations through WavLM-based speechcodes.
> In this method, we discretize features extracted from a WavLM SSL model to reconstruct a mel-spectrogram.
> We apply additional loss functions to promote speaker disentanglement, and compress the resulting speechcodes using Byte-Pair Encoding (BPE) \cite{bpe} to reduce sequence length, enabling us to model longer audio with Transformers.
>
> Both representations are compressed (325 bits/s and 400 bits/s respectively) to allow more efficient autoregressive modeling compared to popular audio codecs (e.g. 6k bits/s in \cite{wang2023neural}).
> With this level of compression, we aim to remove information from speechcodes that can be reconstructed during decoding (speaker, audio noise, etc.) to ensure that the capacity in speechcodes is primarily dedicated to encoding phonetic and prosodic information.

#### 3.2.1.Autoencoder-Based Speech Tokens

> Our baseline discretization method is a VQ-VAE  trained to reconstruct mel-spectrograms. 
> The encoder and decoder are convolutional neural networks with residual connections, which downsample the speech representation to a frequency of 25Hz. 
> To (partially) disentangle speaker information from the speech representations, we introduce a global reference encoder \cite{skerry2018towards}. 
> This encoder learns a fixed-size utterance-level representation, which is concatenated to the speechcodes before reconstructing with the VQ-VAE decoder.
>
> From informal listening, we find that speechcodes produced by the autoencoder-based speech tokenizer still contain speaker information.
> This motivates us to develop representations with improved speaker disentanglement.

#### 3.2.2.WavLM-Based Speechcodes

> We aim to develop speechcodes that contain phonetic and prosodic information, but which are disentangled from speaker identity, recording conditions, and other spurious features in the audio signal.
> To this end, we introduce a speech tokenizer based on features extracted from a pretrained WavLM model \cite{wavlm}, further trained with losses that encourage disentangling the speaker identity.
> Our approach similar to the one introduced in \cite{martíncortinas2024enhancing} with modifications that reduce bitrate of the codes.
> The overall architecture of the speech tokenizer is shown in \cref{fig:wavlm-based}.
>
> We first pass the waveform through the WavLM model and extract the hidden states. 
> These hidden states are then passed through separate content and speaker linear regressors. 
> The output of these regressors is then fed into a convolutional residual encoder \cite{he2016deep}. 
> The content encodings are passed through a vector quantization module that outputs one speechcode per one WavLM frame (i.e. 20ms of speech). 
>
> The speaker encodings are passed through a Transformer-based speaker extractor \cite{originaltransformerpaper} to obtain the speaker embeddings.
> The model only extracts, and we only use non-specific features that cannot be used for identification.
>
> The speaker embeddings are concatenated with the speechcodes, and decoded into a spectrogram using a convolutional decoder.
> We then compute L1 distance between decoded and target spectrograms and use it as the reconstruction loss.
> While L1 is not the optimal reconstruction objective, we prioritize representations that are conducive for autoregressive modeling \cite{de2019hierarchical}, and demonstrate accordingly that the final audio quality can be kept high when this learned representation is decoded with our speechcode decoder, in Section \ref{sec:decoder}.
> The speaker embeddings are used in a contrastive loss, maximizing the similarity between samples from the same speaker and minimizing it for those from different speakers \cite{oord2018representation}.
> Furthermore, we maximize the cosine distance between the speaker embeddings and embeddings obtained by passing the output of the content regressor through the frozen speaker extractor and applying gradient reversal \cite{ganin2015unsupervised}. 
> We hypothesize that this encourages disentanglement between content and speaker information. 
>
> In addition to better disentanglement of speaker information, we also believe that using features from a pretrained WavLM model as input (as opposed to a jointly learnt audio encoding) keeps speechcodes more robust to recording conditions.
> Our intuition is that WavLM was trained with data augmentation to encourage disentanglement from background noise.
> The total loss is given by a weighted combination of these losses, in addition to the commitment loss for the vector quantizer: 

$$
    L = L_\text{recon} + \alpha L_\text{commitment} + \beta L_\text{contrastive} + \gamma L_\text{cosine}
$$

#### 3.2.3.Byte-pair encoding on speechcodes

> The WavLM-based speech representations are learned at a frequency of 50 Hz to ensure that they offer enough resolution in time to be able to discriminate between single phonemes even in fast-paced speech (the average length of English phonemes tends not to dip below 20ms \cite{kuwabara1996acoustic}).
> We apply Byte-Pair Encoding \cite{bpe} to reduce the average sequence length of speechcodes by around 40\%, in order to mitigate the quadratic memory increases in Transformers sequence length and minimize complexity for the autoregressive model.
> Similar to the BPE of texts, we iteratively join the most frequent pairs of speechcodes into new tokens and add them to the vocabulary until a pre-determined vocabulary size is reached. 

### 3.3.Autoregressive Speech Modeling (SpeechGPT)

> We train a GPT2-architecture autoregressive model \cite{radford2019language} that we call "SpeechGPT" to predict the speechcodes conditioned on text and reference speech. 
> The reference speech conditioning consists of a randomly selected utterance from the same speaker, which is encoded to a fixed-size embedding. 
> The reference speech embedding, text, and speechcodes are concatenated into a single sequence that is modeled by a Transformer-based autoregressive model. 
> We use separate positional embeddings and separate prediction heads for text and speech. 
>
> We train the autoregressive model from scratch, without pretraining on text (e.g.
> as done in \cite{twist}).
> In order to retain textual information to guide prosody, we also train SpeechGPT with an objective to predict the next token in the text portion of the input sequence, so that speechGPT is partially a text-only LM.
> We apply a lower weight to this text loss compared to the speech loss.

### 3.4.Waveform Generation

> Our baseline uses a diffusion-based spectrogram decoder and a separately trained UnivNet \cite{univnet} vocoder, as proposed in \cite{tortoise}.
> Diffusion-based TTS decoding can generate high-quality speech, but it suffers from slow inference and cannot generate samples incrementally - This lack of streamabilility forces us to obtain the audio output for the entire sequence in one go.  
> Furthermore, the diffusion model in \cite{tortoise} predicts spectrograms, and requires a separately trained vocoder to generate audio, complicating the training and inference pipeline. 
>
> Our proposed decoder, inspired by \cite{ecat}, is trained in an end-to-end fashion to predict waveforms.
> Our variant uses speechcodes as an input to the model instead of phoneme encodings and prosody latents.
> Additionally, to make the model more scalable, we replace LSTM layers \cite{hochreiter1997long} with convolutional ones to decode an intermediate representation. 
> The output of a HiFi-GAN based decoder block \cite{hifigan} is fed into BigVGAN vocoder \cite{bigvgan} to predict the waveform.
> In training, we use the same set of adversarial and non-adversarial losses as \cite{ecat}.
> We call our proposed system a "speechcode decoder" and depict it in \cref{fig:scv}.
> In addition to simplifying the overall system, we hypothesize that training the decoder and vocoder end-to-end yields higher-quality speech. 
>
> In practice, instead of speechcodes, the speechcode decoder takes as an input the last hidden state of the autoregressive Transformer. 
> We do so because the dense latent representation provides much richer information than a single speechcode, following \cite{tortoise}.
> During training, we feed the text and target codes to the trained SpeechGPT (with parameters frozen) and then condition the decoder on the last hidden state.
> Feeding the last hidden states of SpeechGPT helps improving the segmental and acoustic quality of speech, but also couples the decoder to a specific version of SpeechGPT.
> This complicates experimentation because it forces the two components to  be always built sequentially. 
> This limitation needs to be addressed in future work.

## 4.Experiments: 实验

## 5.Results: 结果

## 6.Conclusions: 结论

> We introduced BASE TTS, a GPT-style TTS system using novel SSL-based speechcodes as an intermediate representation and a speechcode decoder that offers an efficient, streamable alternative to diffusion.
> This is the largest model of its kind known to us, both in terms of parameters and training data.
> We demonstrated new state-of-the-art TTS results against baselines including Tortoise, Bark and YourTTS.
> We proposed a new way to measure textual understanding of TTS models, and showed that LTTS models built with 10K hours of data and 400 million parameters start to exhibit an advanced grasp of text that enables contextually appropriate prosody.
> From BASE TTS's strong performance on English and Spanish, we caught a first glimpse of a multilingual TTS approach that achieves high expressiveness, adapts to textual clues, is data efficient, uses only public domain data, and works for streaming TTS usecases such as voicing LLM outputs.
> Our approach points towards potential Scaling Laws \cite{scaling-law} of LTTS models, where an even larger amount of speech and other (text, image) data are needed to support multimodal objectives \cite{facebook-multimodal} and to break new grounds in TTS. 
>
> Our approach still contains some limitations: 
> a) BASE TTS occasionally produces hallucinations and cutoffs, where we produce either extra or incomplete audio than intended by the text.
> This is an inherent problem with the autoregressive LM approach, made worse by the misalignment between audio data and the ASR-generated text; 
> b) Selecting the right discrete representation for GPT-style TTS is crucial.
> More research is needed to establish how different properties of speechcodes translate into end-to-end system quality.
> We only report results for one speechcode configuration and leave more comprehensive study for future work.
