# BASE TTS

<details>
<summary>基本信息</summary>

- 标题:
- 作者:
  - ??
- 机构:
  - 机构 
- 时间:
  - 预印时间: 20??.??.?? ArXiv v1
  - 更新笔记: 20??.??.??
- 发表:
  - 期刊/会议 
- 链接:
  - [ArXiv]()
  - [DOI]()
  - [Github]()
  - [Demo]()
  - [Scholar](https://scholar.google.com/scholar?cluster=)
- 标签:
  - ?
- 页数: ?
- 引用: ?
- 被引: ?
- 数据:
  - ? 
- 对比:
  - ?
- 复现:
  - ?

</details>

## Abstract: 摘要

> We introduce a text-to-speech (TTS) model called BASE TTS, which stands for \textbf{B}ig \textbf{A}daptive \textbf{S}treamable TTS with \textbf{E}mergent abilities. BASE TTS is the largest TTS model to-date, trained on 100K hours of public domain speech data, achieving a new state-of-the-art in speech naturalness. It deploys a 1-billion-parameter autoregressive Transformer that converts raw texts into discrete codes ("speechcodes") followed by a convolution-based decoder which converts these speechcodes into waveforms in an incremental, streamable manner.
> Further, our speechcodes are built using a novel speech tokenization technique that features speaker ID disentanglement and compression with byte-pair encoding.
> Echoing the widely-reported "emergent abilities" of large language models when trained on increasing volume of data, we show that BASE TTS variants built with 10K+ hours and 500M+ parameters begin to demonstrate natural prosody on textually complex sentences. We design and share a specialized dataset to measure these emergent abilities for text-to-speech. We showcase state-of-the-art naturalness of BASE TTS by evaluating against baselines that include publicly available large-scale text-to-speech systems: YourTTS, Bark and TortoiseTTS. Audio samples generated by the model can be heard at \url{https://amazon-ltts-paper.com/}.

## 1.Introduction: 引言

> Generative deep learning models are progressing at a rapid pace. Natural Language Processing (NLP) and Computer Vision (CV) are undergoing a fundamental shift from specialized models with supervised training, to generalized models that can achieve miscellaneous tasks with limited explicit instruction \cite{sparks}. In NLP, tasks such as question answering, sentiment analysis and text summarization can now be performed by a large language model (LLM) which was not specifically targeted for these tasks \cite{brown2020language,openai2023gpt4,scao2022bloom,touvron2023llama,hoffmann2022training}. In CV, pre-trained models that learn from hundreds of millions of image-caption pairs have achieved top performance on image-to-text benchmarks \cite{clip1,tejankar2021fistful,shen2022k}, while delivering remarkably photo-realistic results in text-to-image tasks \cite{rombach2022high, ramesh2022hierarchical, nichol2022glide, saharia2022photorealistic, yu2022scaling}. This progress has been enabled by Transformer-based architectures \cite{originaltransformerpaper} that drive improvements using many orders of magnitude more data than previous models. Similar advances are now occurring in Speech Processing and Text-to-Speech (TTS), with models leveraging thousands of hours of data that push synthesis ever closer towards human-like speech. Some of these models, described in depth in Section \ref{sec:related_work}, rely on causal language modeling tasks, like AudioLM \cite{borsos2023audiolm} or VALL-E \cite{wang2023neural}, whereas others use non-causal modules, such as SoundStorm \cite{borsos2023soundstorm} or SpeechX \cite{wang2023speechx}, or diffusion decoders \cite{NaturalSpeech2,tortoise}.
>
> Until 2022, leading Neural TTS models were almost exclusively trained on a few hundreds of hours of recorded audio \cite{tacotron2, ecat,kim2021conditional,DBLP:conf/iclr/0006H0QZZL21,popov2021grad}. Such systems can create well-enunciated speech that is occasionally expressive for the target speakers, but typically cannot generalize beyond the small amount of training data to render ambiguous and complex texts with truly expressive spoken performance \cite{triantafyllopoulos2023overview,tahon2018can,schnell2022controllability}. To achieve such higher levels of expressiveness, TTS systems historically had to rely on labeled datasets for specific speech phenomena; and even so, achieving human-like prosody for certain types of textual inputs has remained elusive \cite{tan2021survey}. For example, in English, compound nouns and questions are notoriously hard to render correctly without accurate syntactic parsing and semantic understanding \cite{kenter2020improvingprosodywithbert}.
> 
> In this paper, we introduce BASE TTS: Big Adaptive Streamable TTS with Emergent abilities. It is a multi-lingual and multi-speaker Large TTS (LTTS) system trained on around 100K (doubling previous high in \cite{wang2023neural}) hours of public domain speech data. BASE TTS follows the approach of casting TTS as a next-token-prediction problem \cite{borsos2023audiolm,wang2023neural,tortoise}, inspired by the success of LLMs. This approach is usually applied in combination with large amount of training data to achieve strong multi-lingual and multi-speaker capabilities (e.g. one-shot voice cloning). Our goal is to improve general TTS quality and study how scaling affects the model's ability to produce appropriate prosody and expression for challenging text inputs, similar to how LLMs acquire new abilities through data and parameter scaling, a phenomenon known as "emergence" or "emergent abilities" in the LLM literature \cite{wei2022emergent, webb2023emergent}. \cite{wei2022emergent} defines \textit{emergent abilities of large language models} as "abilities that are not present in smaller-scale models but are present in large-scale models;" for example, they show that on a range of few-shot tasks, model capability stays at a low level from 10\textsuperscript{18} to 10\textsuperscript{22} training FLOPs, but makes a drastic jump from 10\textsuperscript{22} to 10\textsuperscript{24}. To test the hypothesis that this also holds for LTTS, we propose an evaluation scheme to assess potential emergent abilities in TTS, identifying seven categories that are challenging from the literature \cite{triantafyllopoulos2023overview,tahon2018can,schnell2022controllability,tan2021survey,kenter2020improvingprosodywithbert}: compound nouns, emotions, foreign words, paralinguistics, punctuations, questions, and syntactic complexities. See more details in Section \ref{sec:evaluation}.
>
> We design BASE TTS to model a joint distribution of text tokens followed by discrete speech representations, which we refer to as speechcodes. Discretization of speech through audio codecs \cite{vqvae,zeghidour2021soundstream,DBLP:journals/corr/abs-2210-13438} is central to our design, as it enables the direct application of methods developed for LLMs, which underlie recent works on LTTS \cite{borsos2023audiolm,wang2023neural,borsos2023soundstorm,wang2023speechx,NaturalSpeech2,tortoise, kharitonov2023speak}. Specifically, we model speechcodes using a decoder-only autoregressive Transformer with a cross-entropy training objective. Despite its simplicity, this objective can capture complex probability distributions of expressive speech, thus alleviating the oversmoothing problem seen in early neural TTS systems \cite{ren2022revisiting}. As an implicit language model, BASE TTS is also observed to make a qualitative jump in prosody rendering once a large enough variant is trained on sufficient data. 
>
> Further, we propose speaker-disentangled speechcodes that are built on top of a WavLM \cite{wavlm} Self-Supervised Learning (SSL) speech model. We follow \cite{borsos2023audiolm} which introduces semantic tokens constructed by discretizing activations of an SSL model. We extend this approach to better control information captured by the speechcodes.
> Our strategy is to limit the responsibilities of the autoregressive speechcode predictor ("speechGPT") to segmental contents, prosody, and duration, while designating a separate, speechcode-to-waveform decoder (called "speechcode decoder") with the reconstruction of speaker identity and recording conditions. We show that this convolution-based speechcode decoder is compute-efficient and reduces the whole-system synthesis time by over 70\% compared to the baseline diffusion-based decoder.
>
> Our main contributions are summarized as follows:
>
> - We introduce BASE TTS, which to our knowledge is the largest TTS model to date, featuring 1B parameters and trained on a dataset consisting of 100K hours of public domain speech data. In subjective evaluations, BASE TTS performs better than publicly available LTTS baseline models. 
> - We demonstrate how scaling BASE TTS to larger dataset and model sizes improves its capability to render appropriate prosody for complex texts. To this end, we develop and make available an "emergent abilities" testset that can serve as a subjective evaluation benchmark for text understanding and rendering of large-scale TTS models. We report performance on different variants of BASE TTS over this benchmark, showing monotonic improvement in quality with increased dataset size and parameter count.
> - We introduce novel discrete speech representations that are built on top of a WavLM SSL model, intended to capture only phonemic and prosodic information of the speech signal. We demonstrate that these representations outperform the baseline quantization method. We also show that they can be decoded to high quality waveforms with a simple, fast, and streamable decoder, despite high level of compression (only 400 bits/s).

## 2.Related Works: 相关工作

None

## 3.Methodology: 方法

### 3.1.Overview: 概览

> Similar to recent works in speech modeling, we adopt an LLM-based method for the TTS task (\cref{fig:overview}).
> We consider a dataset $\mathcal{D} = \{ \mathbf{x}_i, \mathbf{y}_i \}_{i=0}^N$, where $\mathbf{y}$ is an audio sample and $\mathbf{x} = \{x_1, \cdots, x_T \}$ is the corresponding text transcription. 
> The audio $\mathbf{y} = \{y_1, \cdots, y_S \}$ is represented by a sequence of $S$ discrete tokens (speechcodes), learnt using a separately trained speech tokenizer. 
> We use a Transformer-based autoregressive model with parameters $\phi$ in order to learn the joint probability of the text and audio sequences:
>
> $$
    p(\mathbf{y}, \mathbf{x}) = p(\mathbf{y} | \mathbf{x}) p(\mathbf{x}) = \prod_{s=1}^S p(\mathbf{y}_s | \mathbf{y}_{<s}, \mathbf{x}; \phi) \prod_{t=1}^T p(\mathbf{x}_t | \mathbf{x}_{<t}; \phi).
> $$
>
> The predicted speech tokens are concatenated with speaker embeddings and decoded into waveforms using a separately trained speechcode decoder consisting of linear and convolutional layers. 

### 3.2.Discrete Speech Representations: 离散语音表示

> Discrete representation is foundational to the success of LLM, but identifying a compact and informative representation is less obvious in speech than in text, and less explored. For BASE TTS, we first experiment with a  Vector Quantized Variational Autoencoder (VQ-VAE) baseline \cite{vqvae}, where an auto-encoder based architecture reconstructs mel-spectrograms through a discrete bottleneck, as described in Section \ref{sec:vqvae}. VQ-VAE has been a successful paradigm in speech and image representation, and especially as a unit of modeling for TTS \cite{tjandra2020vqvae, tortoise}.
>
> In Section \ref{sec:wavlm}, however, we introduce a novel method of learning speech representations through WavLM-based speechcodes. In this method, we discretize features extracted from a WavLM SSL model to reconstruct a mel-spectrogram. We apply additional loss functions to promote speaker disentanglement, and compress the resulting speechcodes using Byte-Pair Encoding (BPE) \cite{bpe} to reduce sequence length, enabling us to model longer audio with Transformers.
>
> Both representations are compressed (325 bits/s and 400 bits/s respectively) to allow more efficient autoregressive modeling compared to popular audio codecs (e.g. 6k bits/s in \cite{wang2023neural}). With this level of compression, we aim to remove information from speechcodes that can be reconstructed during decoding (speaker, audio noise, etc.) to ensure that the capacity in speechcodes is primarily dedicated to encoding phonetic and prosodic information.

#### 3.2.1.Autoencoder-Based Speech Tokens

> Our baseline discretization method is a VQ-VAE  trained to reconstruct mel-spectrograms. 
> The encoder and decoder are convolutional neural networks with residual connections, which downsample the speech representation to a frequency of 25Hz. 
> To (partially) disentangle speaker information from the speech representations, we introduce a global reference encoder \cite{skerry2018towards}. 
> This encoder learns a fixed-size utterance-level representation, which is concatenated to the speechcodes before reconstructing with the VQ-VAE decoder.
>
> From informal listening, we find that speechcodes produced by the autoencoder-based speech tokenizer still contain speaker information. This motivates us to develop representations with improved speaker disentanglement.

#### 3.2.2.WavLM-Based Speechcodes

> We aim to develop speechcodes that contain phonetic and prosodic information, but which are disentangled from speaker identity, recording conditions, and other spurious features in the audio signal. To this end, we introduce a speech tokenizer based on features extracted from a pretrained WavLM model \cite{wavlm}, further trained with losses that encourage disentangling the speaker identity. Our approach similar to the one introduced in \cite{martíncortinas2024enhancing} with modifications that reduce bitrate of the codes.
> The overall architecture of the speech tokenizer is shown in \cref{fig:wavlm-based}.
>
> We first pass the waveform through the WavLM model and extract the hidden states. 
> These hidden states are then passed through separate content and speaker linear regressors. 
> The output of these regressors is then fed into a convolutional residual encoder \cite{he2016deep}. 
> The content encodings are passed through a vector quantization module that outputs one speechcode per one WavLM frame (i.e. 20ms of speech). 
>
> The speaker encodings are passed through a Transformer-based speaker extractor \cite{originaltransformerpaper} to obtain the speaker embeddings. The model only extracts, and we only use non-specific features that cannot be used for identification.
>
> The speaker embeddings are concatenated with the speechcodes, and decoded into a spectrogram using a convolutional decoder. We then compute L1 distance between decoded and target spectrograms and use it as the reconstruction loss. While L1 is not the optimal reconstruction objective, we prioritize representations that are conducive for autoregressive modeling \cite{de2019hierarchical}, and demonstrate accordingly that the final audio quality can be kept high when this learned representation is decoded with our speechcode decoder, in Section \ref{sec:decoder}.
> The speaker embeddings are used in a contrastive loss, maximizing the similarity between samples from the same speaker and minimizing it for those from different speakers \cite{oord2018representation}.
> Furthermore, we maximize the cosine distance between the speaker embeddings and embeddings obtained by passing the output of the content regressor through the frozen speaker extractor and applying gradient reversal \cite{ganin2015unsupervised}. 
> We hypothesize that this encourages disentanglement between content and speaker information. 
>
> In addition to better disentanglement of speaker information, we also believe that using features from a pretrained WavLM model as input (as opposed to a jointly learnt audio encoding) keeps speechcodes more robust to recording conditions. Our intuition is that WavLM was trained with data augmentation to encourage disentanglement from background noise.
> The total loss is given by a weighted combination of these losses, in addition to the commitment loss for the vector quantizer: 

$$
    L = L_\text{recon} + \alpha L_\text{commitment} + \beta L_\text{contrastive} + \gamma L_\text{cosine}
$$

#### 3.2.3.Byte-pair encoding on speechcodes

> The WavLM-based speech representations are learned at a frequency of 50 Hz to ensure that they offer enough resolution in time to be able to discriminate between single phonemes even in fast-paced speech (the average length of English phonemes tends not to dip below 20ms \cite{kuwabara1996acoustic}).
> We apply Byte-Pair Encoding \cite{bpe} to reduce the average sequence length of speechcodes by around 40\%, in order to mitigate the quadratic memory increases in Transformers sequence length and minimize complexity for the autoregressive model. Similar to the BPE of texts, we iteratively join the most frequent pairs of speechcodes into new tokens and add them to the vocabulary until a pre-determined vocabulary size is reached. 

### 3.3.Autoregressive Speech Modeling (SpeechGPT)

> We train a GPT2-architecture autoregressive model \cite{radford2019language} that we call "SpeechGPT" to predict the speechcodes conditioned on text and reference speech. 
> The reference speech conditioning consists of a randomly selected utterance from the same speaker, which is encoded to a fixed-size embedding. 
> The reference speech embedding, text, and speechcodes are concatenated into a single sequence that is modeled by a Transformer-based autoregressive model. 
> We use separate positional embeddings and separate prediction heads for text and speech. 
>
> We train the autoregressive model from scratch, without pretraining on text (e.g. as done in \cite{twist}). In order to retain textual information to guide prosody, we also train SpeechGPT with an objective to predict the next token in the text portion of the input sequence, so that speechGPT is partially a text-only LM. We apply a lower weight to this text loss compared to the speech loss.

### 3.4.Waveform Generation

> Our baseline uses a diffusion-based spectrogram decoder and a separately trained UnivNet \cite{univnet} vocoder, as proposed in \cite{tortoise}. Diffusion-based TTS decoding can generate high-quality speech, but it suffers from slow inference and cannot generate samples incrementally - This lack of streamabilility forces us to obtain the audio output for the entire sequence in one go.  
> Furthermore, the diffusion model in \cite{tortoise} predicts spectrograms, and requires a separately trained vocoder to generate audio, complicating the training and inference pipeline. 
>
> Our proposed decoder, inspired by \cite{ecat}, is trained in an end-to-end fashion to predict waveforms. Our variant uses speechcodes as an input to the model instead of phoneme encodings and prosody latents. Additionally, to make the model more scalable, we replace LSTM layers \cite{hochreiter1997long} with convolutional ones to decode an intermediate representation. 
> The output of a HiFi-GAN based decoder block \cite{hifigan} is fed into BigVGAN vocoder \cite{bigvgan} to predict the waveform. In training, we use the same set of adversarial and non-adversarial losses as \cite{ecat}. We call our proposed system a "speechcode decoder" and depict it in \cref{fig:scv}. In addition to simplifying the overall system, we hypothesize that training the decoder and vocoder end-to-end yields higher-quality speech. 
>
> In practice, instead of speechcodes, the speechcode decoder takes as an input the last hidden state of the autoregressive Transformer. 
> We do so because the dense latent representation provides much richer information than a single speechcode, following \cite{tortoise}.
> During training, we feed the text and target codes to the trained SpeechGPT (with parameters frozen) and then condition the decoder on the last hidden state.
> Feeding the last hidden states of SpeechGPT helps improving the segmental and acoustic quality of speech, but also couples the decoder to a specific version of SpeechGPT.
> This complicates experimentation because it forces the two components to  be always built sequentially. 
> This limitation needs to be addressed in future work.

## 4.Experiments: 实验

## 5.Results: 结果

## 6.Conclusions: 结论
