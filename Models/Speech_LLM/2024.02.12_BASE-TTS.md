# BASE TTS

<details>
<summary>基本信息</summary>

- 标题:
- 作者:
  - ??
- 机构:
  - 机构 
- 时间:
  - 预印时间: 20??.??.?? ArXiv v1
  - 更新笔记: 20??.??.??
- 发表:
  - 期刊/会议 
- 链接:
  - [ArXiv]()
  - [DOI]()
  - [Github]()
  - [Demo]()
  - [Scholar](https://scholar.google.com/scholar?cluster=)
- 标签:
  - ?
- 页数: ?
- 引用: ?
- 被引: ?
- 数据:
  - ? 
- 对比:
  - ?
- 复现:
  - ?

</details>

## Abstract: 摘要

> We introduce a text-to-speech (TTS) model called BASE TTS, which stands for \textbf{B}ig \textbf{A}daptive \textbf{S}treamable TTS with \textbf{E}mergent abilities. BASE TTS is the largest TTS model to-date, trained on 100K hours of public domain speech data, achieving a new state-of-the-art in speech naturalness. It deploys a 1-billion-parameter autoregressive Transformer that converts raw texts into discrete codes ("speechcodes") followed by a convolution-based decoder which converts these speechcodes into waveforms in an incremental, streamable manner.
> Further, our speechcodes are built using a novel speech tokenization technique that features speaker ID disentanglement and compression with byte-pair encoding.
> Echoing the widely-reported "emergent abilities" of large language models when trained on increasing volume of data, we show that BASE TTS variants built with 10K+ hours and 500M+ parameters begin to demonstrate natural prosody on textually complex sentences. We design and share a specialized dataset to measure these emergent abilities for text-to-speech. We showcase state-of-the-art naturalness of BASE TTS by evaluating against baselines that include publicly available large-scale text-to-speech systems: YourTTS, Bark and TortoiseTTS. Audio samples generated by the model can be heard at \url{https://amazon-ltts-paper.com/}.

## 1.Introduction: 引言

> Generative deep learning models are progressing at a rapid pace. Natural Language Processing (NLP) and Computer Vision (CV) are undergoing a fundamental shift from specialized models with supervised training, to generalized models that can achieve miscellaneous tasks with limited explicit instruction \cite{sparks}. In NLP, tasks such as question answering, sentiment analysis and text summarization can now be performed by a large language model (LLM) which was not specifically targeted for these tasks \cite{brown2020language,openai2023gpt4,scao2022bloom,touvron2023llama,hoffmann2022training}. In CV, pre-trained models that learn from hundreds of millions of image-caption pairs have achieved top performance on image-to-text benchmarks \cite{clip1,tejankar2021fistful,shen2022k}, while delivering remarkably photo-realistic results in text-to-image tasks \cite{rombach2022high, ramesh2022hierarchical, nichol2022glide, saharia2022photorealistic, yu2022scaling}. This progress has been enabled by Transformer-based architectures \cite{originaltransformerpaper} that drive improvements using many orders of magnitude more data than previous models. Similar advances are now occurring in Speech Processing and Text-to-Speech (TTS), with models leveraging thousands of hours of data that push synthesis ever closer towards human-like speech. Some of these models, described in depth in Section \ref{sec:related_work}, rely on causal language modeling tasks, like AudioLM \cite{borsos2023audiolm} or VALL-E \cite{wang2023neural}, whereas others use non-causal modules, such as SoundStorm \cite{borsos2023soundstorm} or SpeechX \cite{wang2023speechx}, or diffusion decoders \cite{NaturalSpeech2,tortoise}.
>
> Until 2022, leading Neural TTS models were almost exclusively trained on a few hundreds of hours of recorded audio \cite{tacotron2, ecat,kim2021conditional,DBLP:conf/iclr/0006H0QZZL21,popov2021grad}. Such systems can create well-enunciated speech that is occasionally expressive for the target speakers, but typically cannot generalize beyond the small amount of training data to render ambiguous and complex texts with truly expressive spoken performance \cite{triantafyllopoulos2023overview,tahon2018can,schnell2022controllability}. To achieve such higher levels of expressiveness, TTS systems historically had to rely on labeled datasets for specific speech phenomena; and even so, achieving human-like prosody for certain types of textual inputs has remained elusive \cite{tan2021survey}. For example, in English, compound nouns and questions are notoriously hard to render correctly without accurate syntactic parsing and semantic understanding \cite{kenter2020improvingprosodywithbert}.
> 
> In this paper, we introduce BASE TTS: Big Adaptive Streamable TTS with Emergent abilities. It is a multi-lingual and multi-speaker Large TTS (LTTS) system trained on around 100K (doubling previous high in \cite{wang2023neural}) hours of public domain speech data. BASE TTS follows the approach of casting TTS as a next-token-prediction problem \cite{borsos2023audiolm,wang2023neural,tortoise}, inspired by the success of LLMs. This approach is usually applied in combination with large amount of training data to achieve strong multi-lingual and multi-speaker capabilities (e.g. one-shot voice cloning). Our goal is to improve general TTS quality and study how scaling affects the model's ability to produce appropriate prosody and expression for challenging text inputs, similar to how LLMs acquire new abilities through data and parameter scaling, a phenomenon known as "emergence" or "emergent abilities" in the LLM literature \cite{wei2022emergent, webb2023emergent}. \cite{wei2022emergent} defines \textit{emergent abilities of large language models} as "abilities that are not present in smaller-scale models but are present in large-scale models;" for example, they show that on a range of few-shot tasks, model capability stays at a low level from 10\textsuperscript{18} to 10\textsuperscript{22} training FLOPs, but makes a drastic jump from 10\textsuperscript{22} to 10\textsuperscript{24}. To test the hypothesis that this also holds for LTTS, we propose an evaluation scheme to assess potential emergent abilities in TTS, identifying seven categories that are challenging from the literature \cite{triantafyllopoulos2023overview,tahon2018can,schnell2022controllability,tan2021survey,kenter2020improvingprosodywithbert}: compound nouns, emotions, foreign words, paralinguistics, punctuations, questions, and syntactic complexities. See more details in Section \ref{sec:evaluation}.
>
> We design BASE TTS to model a joint distribution of text tokens followed by discrete speech representations, which we refer to as speechcodes. Discretization of speech through audio codecs \cite{vqvae,zeghidour2021soundstream,DBLP:journals/corr/abs-2210-13438} is central to our design, as it enables the direct application of methods developed for LLMs, which underlie recent works on LTTS \cite{borsos2023audiolm,wang2023neural,borsos2023soundstorm,wang2023speechx,NaturalSpeech2,tortoise, kharitonov2023speak}. Specifically, we model speechcodes using a decoder-only autoregressive Transformer with a cross-entropy training objective. Despite its simplicity, this objective can capture complex probability distributions of expressive speech, thus alleviating the oversmoothing problem seen in early neural TTS systems \cite{ren2022revisiting}. As an implicit language model, BASE TTS is also observed to make a qualitative jump in prosody rendering once a large enough variant is trained on sufficient data. 
>
> Further, we propose speaker-disentangled speechcodes that are built on top of a WavLM \cite{wavlm} Self-Supervised Learning (SSL) speech model. We follow \cite{borsos2023audiolm} which introduces semantic tokens constructed by discretizing activations of an SSL model. We extend this approach to better control information captured by the speechcodes.
> Our strategy is to limit the responsibilities of the autoregressive speechcode predictor ("speechGPT") to segmental contents, prosody, and duration, while designating a separate, speechcode-to-waveform decoder (called "speechcode decoder") with the reconstruction of speaker identity and recording conditions. We show that this convolution-based speechcode decoder is compute-efficient and reduces the whole-system synthesis time by over 70\% compared to the baseline diffusion-based decoder.
>
> Our main contributions are summarized as follows:
>
> - We introduce BASE TTS, which to our knowledge is the largest TTS model to date, featuring 1B parameters and trained on a dataset consisting of 100K hours of public domain speech data. In subjective evaluations, BASE TTS performs better than publicly available LTTS baseline models. 
> - We demonstrate how scaling BASE TTS to larger dataset and model sizes improves its capability to render appropriate prosody for complex texts. To this end, we develop and make available an "emergent abilities" testset that can serve as a subjective evaluation benchmark for text understanding and rendering of large-scale TTS models. We report performance on different variants of BASE TTS over this benchmark, showing monotonic improvement in quality with increased dataset size and parameter count.
> - We introduce novel discrete speech representations that are built on top of a WavLM SSL model, intended to capture only phonemic and prosodic information of the speech signal. We demonstrate that these representations outperform the baseline quantization method. We also show that they can be decoded to high quality waveforms with a simple, fast, and streamable decoder, despite high level of compression (only 400 bits/s).

## 2.Related Works: 相关工作

## 3.Methodology: 方法

## 4.Experiments: 实验

## 5.Results: 结果

## 6.Conclusions: 结论
