# MELLE

<details>
<summary>基本信息</summary>

- 标题: Autoregressive Speech Synthesis without Vector Quantization
- 作者:
  - 01 [Lingwei Meng](../../Authors/Lingwei_Meng.md)
  - 02 [Long Zhou](../../Authors/Long_Zhou_(周龙).md)
  - 03 [Shujie Liu](../../Authors/Shujie_Liu_(刘树杰).md)
  - 04 [Sanyuan Chen](../../Authors/Sanyuan_Chen_(陈三元).md)
  - 05 [Bing Han](../../Authors/Bing_Han.md)
  - 06 [Shujie Hu](../../Authors/Shujie_Hu.md)
  - 07 [Yanqing Liu](../../Authors/Yanqing_Liu.md)
  - 08 [Jinyu Li](../../Authors/Jinyu_Li_(李劲宇).md)
  - 09 [Sheng Zhao](../../Authors/Sheng_Zhao_(赵胜).md)
  - 10 [Xixin Wu](../../Authors/Xixin_Wu.md)
  - 11 [Helen Meng](../../Authors/Helen_Meng_(蒙美玲).md)
  - 12 [Furu Wei](../../Authors/Furu_Wei_(韦福如).md)
- 机构:
  - [香港中文大学](../../Institutions/CUHK_香港中文大学.md)
  - [Microsoft](../../Institutions/Microsoft.md)
- 时间:
  - 预印时间: 2024.07.11 ArXiv v1
  - 更新笔记: 2024.07.12
- 发表:
  - 期刊/会议 
- 链接:
  - [ArXiv](https://arxiv.org/abs/2407.08551)
  <!-- - [DOI]() -->
  <!-- - [Github]() -->
  - [Demo](https://aka.ms/melle)
  <!-- - [Scholar](https://scholar.google.com/scholar?cluster=) -->
- 标签:
  - [自回归](../../Tags/Autoregressive.md)
- 页数: 17
- 引用: ?
- 被引: ?
- 数据:
  - 训练 [Libriheavy](../../Datasets/2023.09.15_Libriheavy.md) MELLE
  - 训练 [LibriSpeech](../../Datasets/2015.04.19_LibriSpeech.md) MELLE-limited
- 对比:
  - [ELLA-V](../../Models/Speech_LLM/2024.01.14_ELLA-V.md)
  - [VALL-E R](../../Models/Speech_LLM/2024.06.12_VALL-E_R.md)
  - [RALL-E](../../Models/Speech_LLM/2024.04.04_RALL-E.md)
  - [CLaM-TTS](../../Models/Speech_LLM/2024.04.03_CLaM-TTS.md)
  - [VALL-E](../../Models/Speech_LLM/2023.01.05_VALL-E.md)
  - [VALL-E 2](../../Models/Speech_LLM/2024.06.08_VALL-E2.md)
  - [Voicebox](../../Models/Speech_LLM/2023.06.23_VoiceBox.md)
- 复现:
  - ?

</details>

## Abstract: 摘要

> We present ***MELLE***, a novel continuous-valued tokens based language modeling approach for text to speech synthesis (TTS).
> ***MELLE*** autoregressively generates continuous mel-spectrogram frames directly from text condition, bypassing the need for vector quantization, which are originally designed for audio compression and sacrifice fidelity compared to mel-spectrograms.
> Specifically, (i) instead of cross-entropy loss, we apply regression loss with a proposed spectrogram flux loss function to model the probability distribution of the continuous-valued tokens. (ii) we have incorporated variational inference into ***MELLE*** to facilitate sampling mechanisms, thereby enhancing the output diversity and model robustness.
> Experiments demonstrate that, compared to the two-stage codec language models VALL-E and its variants, the single-stage ***MELLE*** mitigates robustness issues by avoiding the inherent flaws of sampling discrete codes, achieves superior performance across multiple metrics, and, most importantly, offers a more streamlined paradigm.
> See \url{https://aka.ms/melle} for demos of our work.

## 1.Introduction: 引言

## 2.Related Works: 相关工作

## 3.Methodology: 方法

## 4.Experiments: 实验

## 5.Results: 结果

## 6.Conclusions: 结论

> In this study, we propose a continuous acoustic representation-based language modeling approach for zero-shot text-to-speech synthesis tasks, thereby eliminating the use of discrete vector quantization.
> By exploring the potential of mel-spectrograms within the paradigm of language modeling, the proposed ***MELLE*** directly predicts mel-spectrograms conditioned on text content and speech prompt. This approach eliminates the need for the two-pass training and inference procedures typical of neural codec language model VALL-E, and can further accelerate decoding by setting the reduction factor. 
> With the aid of latent sampling and spectrogram flux loss, ***MELLE*** is capable of producing more diverse and robust predictions, attaining results comparable to human performance in subjective evaluations.

