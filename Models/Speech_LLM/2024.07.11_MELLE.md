# MELLE

<details>
<summary>基本信息</summary>

- 标题: Autoregressive Speech Synthesis without Vector Quantization
- 作者:
  - 01 [Lingwei Meng](../../Authors/Lingwei_Meng.md)
  - 02 [Long Zhou](../../Authors/Long_Zhou_(周龙).md)
  - 03 [Shujie Liu](../../Authors/Shujie_Liu_(刘树杰).md)
  - 04 [Sanyuan Chen](../../Authors/Sanyuan_Chen_(陈三元).md)
  - 05 [Bing Han](../../Authors/Bing_Han.md)
  - 06 [Shujie Hu](../../Authors/Shujie_Hu.md)
  - 07 [Yanqing Liu](../../Authors/Yanqing_Liu.md)
  - 08 [Jinyu Li](../../Authors/Jinyu_Li_(李劲宇).md)
  - 09 [Sheng Zhao](../../Authors/Sheng_Zhao_(赵胜).md)
  - 10 [Xixin Wu](../../Authors/Xixin_Wu.md)
  - 11 [Helen Meng](../../Authors/Helen_Meng_(蒙美玲).md)
  - 12 [Furu Wei](../../Authors/Furu_Wei_(韦福如).md)
- 机构:
  - [香港中文大学](../../Institutions/CUHK_香港中文大学.md)
  - [Microsoft](../../Institutions/Microsoft.md)
- 时间:
  - 预印时间: 2024.07.11 ArXiv v1
  - 更新笔记: 2024.07.12
- 发表:
  - 期刊/会议 
- 链接:
  - [ArXiv](https://arxiv.org/abs/2407.08551)
  <!-- - [DOI]() -->
  <!-- - [Github]() -->
  - [Demo](https://aka.ms/melle)
  <!-- - [Scholar](https://scholar.google.com/scholar?cluster=) -->
- 标签:
  - [自回归](../../Tags/Autoregressive.md)
- 页数: 17
- 引用: ?
- 被引: ?
- 数据:
  - 训练 [Libriheavy](../../Datasets/2023.09.15_Libriheavy.md) MELLE
  - 训练 [LibriSpeech](../../Datasets/2015.04.19_LibriSpeech.md) MELLE-limited
- 对比:
  - [ELLA-V](../../Models/Speech_LLM/2024.01.14_ELLA-V.md)
  - [VALL-E R](../../Models/Speech_LLM/2024.06.12_VALL-E_R.md)
  - [RALL-E](../../Models/Speech_LLM/2024.04.04_RALL-E.md)
  - [CLaM-TTS](../../Models/Speech_LLM/2024.04.03_CLaM-TTS.md)
  - [VALL-E](../../Models/Speech_LLM/2023.01.05_VALL-E.md)
  - [VALL-E 2](../../Models/Speech_LLM/2024.06.08_VALL-E2.md)
  - [Voicebox](../../Models/Speech_LLM/2023.06.23_VoiceBox.md)
- 复现:
  - ?

</details>

## Abstract: 摘要

> We present ***MELLE***, a novel continuous-valued tokens based language modeling approach for text to speech synthesis (TTS).
> ***MELLE*** autoregressively generates continuous mel-spectrogram frames directly from text condition, bypassing the need for vector quantization, which are originally designed for audio compression and sacrifice fidelity compared to mel-spectrograms.
> Specifically, (i) instead of cross-entropy loss, we apply regression loss with a proposed spectrogram flux loss function to model the probability distribution of the continuous-valued tokens. (ii) we have incorporated variational inference into ***MELLE*** to facilitate sampling mechanisms, thereby enhancing the output diversity and model robustness.
> Experiments demonstrate that, compared to the two-stage codec language models VALL-E and its variants, the single-stage ***MELLE*** mitigates robustness issues by avoiding the inherent flaws of sampling discrete codes, achieves superior performance across multiple metrics, and, most importantly, offers a more streamlined paradigm.
> See https://aka.ms/melle for demos of our work.

## 1.Introduction: 引言

> The objective of next-token prediction, which involves predicting the next discrete token based on the previous tokens as condition, is foundational to the recent progress observed in autoregressive large language models (LLMs) \citep{brown2020language,OpenAI2023GPT4TR}.
> Recently, the success of LLMs in natural language processing (NLP) tasks has encouraged the exploration of autoregressive language modeling approaches in audio synthesis fields \citep{borsos2022audiolm,wang2023valle}. 
> Neural codec language models, exemplified by VALL-E~\citep{wang2023valle} and VALL-E X \citep{zhang2023vallex}, reveal the potential of such principle in the zero-shot text-to-speech (TTS) task via leveraging large-scale multi-lingual multi-speaker multi-domain training corpus. 
> Unlike traditional TTS systems that rely heavily on complex and multi-step pipelines, they utilize a decoder-only approach to predict neural codec codes, which are discrete tokens encoded from continuous waveforms leveraging neural codec models \citep{zeghidour2021soundstream,dfossez2023encodec}. 
>
> Although achieving impressive naturalness and diversity in synthesized audios, VALL-E and its variants are plagued by following several drawbacks.
> First, neural codec codes, originally designed for audio compression, exhibit lower fidelity compared to the well-established mel-spectrogram \citep{puvvada2024discrete}. This phenomenon is also observed in the field of graphics, where the reconstruction quality of vector-quantized tokenizers typically lags behind that of their continuous-valued counterparts \citep{rombach2022high,huang2023not,kaiming2024autoregressive}.
> Second, the codec language model VALL-E suffers from robustness issues stemming from its random sampling strategy, which is inherited from text language model for selecting discrete tokens.
> This issue is more pronounced with acoustic tokens as opposed to textual ones due to the greater similarity among consecutive codec codes, which can lead to continuous stretches of silence or persistent noise \citep{wang2023valle, song2024ellav}.
> Third, neural codec language models typically necessitate a complicated two-pass decoding process, involving an autoregressive (AR) model for generating coarse primary audio tokens, followed by a non-autoregressive (NAR) model to iteratively predict the rest multiple codebook codes for refinement. 
> This multi-step procedure compromises inference efficiency, leading to increased computational demands and doubled storage requirements.
> 
> To address the limitations associated with discrete tokens based codec language models, we are rethinking the potential of continuous speech representations and aim to determine whether continuous-valued tokens can supplant discrete-valued tokens within the paradigm of autoregressive speech synthesis models.
> The successful implementation of the autoregressive model without discrete vector quantization faces two key challenges.
> - \textbf{How to set training objective for continuous representation?} 
> The continuous space significantly differs from that of vector-quantized tokens, for which autoregressive language models typically adopt a next-token prediction objective, with cross-entropy loss to measure the discrepancy between the predicted distribution and the ground truth. 
> - \textbf{How to enable sampling mechanism in continuous space?} The sampling strategy is a critical component in both text generation and speech synthesis systems, as it introduces diversity into the output and enhances the generalization ability. However, continuous-valued tokens based models can not employ top-p random sampling method used in discrete codec language models.
>
> In this work, we propose a continuous mel-spectrogram\footnote{We leave the exploration of other continuous representations, such as VAE latent hidden states, for future research endeavors.} based autoregressive language model (called ***MELLE***) for text-to-speech synthesis, as illustrated in Figure \ref{fig:overview}. ***MELLE*** is a robust single-pass zero-shot TTS model which autoregressively predicts mel-spectrogram frames based on previous mel-spectrogram and text tokens, thereby avoiding the inherent flaws associated with sampling discrete codec codes. 
> The mel-spectrogram is then converted into waveform utilizing an off-the-shelf vocoder. 
> In response to the aforementioned challenges, we first substitute  cross-entropy loss with regression loss and introduce a spectrogram flux loss to promote variation of predicted mel-spectrograms and eliminate repetition issues.
> Second, we design a latent sampling module, derived from variational inference, functions as a sequence sampling strategy thereby enhancing the diversity of the generated audio samples. 
> As an option, by adjusting the reduction factor, ***MELLE*** can predict multiple frames at one step and accelerate inference, thereby further alleviating the robustness issues associated with long-sequence modeling and maintaining satisfactory performance.  
>
> We conducted evaluations of our ***MELLE*** on both the large-scale 50K-hour Libriheavy \citep{kang2024libriheavy} training dataset and the relatively small 960-hour LibriSpeech \citep{panayotov2015librispeech} training dataset. 
> Following recent works, we use LibriSpeech test-clean set for zero-shot TTS evaluation.
> Experimental results demonstrate that the proposed ***MELLE*** is on par with VALL-E 2 \citep{chen2024valle2} in objective metrics, and surpasses VALL-E 2 in subjective metrics. It also outperforms previous neural codec language models, including VALL-E and its other variants, achieving superior performance across multiple metrics that reflect naturalness, robustness, similarity, and inference efficiency. 
>
> Specifically, ***MELLE*** surpasses the ground truth audios in WER (1.47\% vs. 1.61\%), achieving a 47.9\% relative reduction in WER compared to VALL-E and an 8.1\% reduction compared to VALL-E 2 on the continuation inference task for zero-shot TTS. 
> For subjective evaluations, ***MELLE*** is more favorably received by human listeners than previous models, achieving comparable performance to the original ground truth in terms of MOS (4.20 vs. 4.29) and CMOS (-0.032 for ours vs. ground truth), and an even higher SMOS (4.40 vs. 3.94) than the ground-truth speech.

## 2.Related Works: 相关工作

## 3.Methodology: 方法

## 4.Experiments: 实验

## 5.Results: 结果

## 6.Conclusions: 结论

> In this study, we propose a continuous acoustic representation-based language modeling approach for zero-shot text-to-speech synthesis tasks, thereby eliminating the use of discrete vector quantization.
> By exploring the potential of mel-spectrograms within the paradigm of language modeling, the proposed ***MELLE*** directly predicts mel-spectrograms conditioned on text content and speech prompt. This approach eliminates the need for the two-pass training and inference procedures typical of neural codec language model VALL-E, and can further accelerate decoding by setting the reduction factor. 
> With the aid of latent sampling and spectrogram flux loss, ***MELLE*** is capable of producing more diverse and robust predictions, attaining results comparable to human performance in subjective evaluations.

