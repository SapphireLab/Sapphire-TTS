# MELLE

<details>
<summary>基本信息</summary>

- 标题: Autoregressive Speech Synthesis without Vector Quantization
- 作者:
  - 01 [Lingwei Meng](../../Authors/Lingwei_Meng.md)
  - 02 [Long Zhou](../../Authors/Long_Zhou_(周龙).md)
  - 03 [Shujie Liu](../../Authors/Shujie_Liu_(刘树杰).md)
  - 04 [Sanyuan Chen](../../Authors/Sanyuan_Chen_(陈三元).md)
  - 05 [Bing Han](../../Authors/Bing_Han.md)
  - 06 [Shujie Hu](../../Authors/Shujie_Hu.md)
  - 07 [Yanqing Liu](../../Authors/Yanqing_Liu.md)
  - 08 [Jinyu Li](../../Authors/Jinyu_Li_(李劲宇).md)
  - 09 [Sheng Zhao](../../Authors/Sheng_Zhao_(赵胜).md)
  - 10 [Xixin Wu](../../Authors/Xixin_Wu.md)
  - 11 [Helen Meng](../../Authors/Helen_Meng_(蒙美玲).md)
  - 12 [Furu Wei](../../Authors/Furu_Wei_(韦福如).md)
- 机构:
  - [香港中文大学](../../Institutions/CUHK_香港中文大学.md)
  - [Microsoft](../../Institutions/Microsoft.md)
- 时间:
  - 预印时间: 2024.07.11 ArXiv v1
  - 更新笔记: 2024.07.12
- 发表:
  - 期刊/会议 
- 链接:
  - [ArXiv](https://arxiv.org/abs/2407.08551)
  <!-- - [DOI]() -->
  <!-- - [Github]() -->
  - [Demo](https://aka.ms/melle)
  <!-- - [Scholar](https://scholar.google.com/scholar?cluster=) -->
- 标签:
  - [自回归](../../Tags/Autoregressive.md)
- 页数: 17
- 引用: ?
- 被引: ?
- 数据:
  - 训练 [Libriheavy](../../Datasets/2023.09.15_Libriheavy.md) MELLE
  - 训练 [LibriSpeech](../../Datasets/2015.04.19_LibriSpeech.md) MELLE-limited
- 对比:
  - [ELLA-V](../../Models/Speech_LLM/2024.01.14_ELLA-V.md)
  - [VALL-E R](../../Models/Speech_LLM/2024.06.12_VALL-E_R.md)
  - [RALL-E](../../Models/Speech_LLM/2024.04.04_RALL-E.md)
  - [CLaM-TTS](../../Models/Speech_LLM/2024.04.03_CLaM-TTS.md)
  - [VALL-E](../../Models/Speech_LLM/2023.01.05_VALL-E.md)
  - [VALL-E 2](../../Models/Speech_LLM/2024.06.08_VALL-E2.md)
  - [Voicebox](../../Models/Speech_LLM/2023.06.23_VoiceBox.md)
- 复现:
  - ?

</details>

## Abstract: 摘要

> We present ***MELLE***, a novel continuous-valued tokens based language modeling approach for text to speech synthesis (TTS).
> ***MELLE*** autoregressively generates continuous mel-spectrogram frames directly from text condition, bypassing the need for vector quantization, which are originally designed for audio compression and sacrifice fidelity compared to mel-spectrograms.
> Specifically, (i) instead of cross-entropy loss, we apply regression loss with a proposed spectrogram flux loss function to model the probability distribution of the continuous-valued tokens. (ii) we have incorporated variational inference into ***MELLE*** to facilitate sampling mechanisms, thereby enhancing the output diversity and model robustness.
> Experiments demonstrate that, compared to the two-stage codec language models VALL-E and its variants, the single-stage ***MELLE*** mitigates robustness issues by avoiding the inherent flaws of sampling discrete codes, achieves superior performance across multiple metrics, and, most importantly, offers a more streamlined paradigm.
> See https://aka.ms/melle for demos of our work.

## 1.Introduction: 引言

> The objective of next-token prediction, which involves predicting the next discrete token based on the previous tokens as condition, is foundational to the recent progress observed in autoregressive large language models (LLMs) \citep{brown2020language,OpenAI2023GPT4TR}.
> Recently, the success of LLMs in natural language processing (NLP) tasks has encouraged the exploration of autoregressive language modeling approaches in audio synthesis fields \citep{borsos2022audiolm,wang2023valle}. 
> Neural codec language models, exemplified by VALL-E~\citep{wang2023valle} and VALL-E X \citep{zhang2023vallex}, reveal the potential of such principle in the zero-shot text-to-speech (TTS) task via leveraging large-scale multi-lingual multi-speaker multi-domain training corpus. 
> Unlike traditional TTS systems that rely heavily on complex and multi-step pipelines, they utilize a decoder-only approach to predict neural codec codes, which are discrete tokens encoded from continuous waveforms leveraging neural codec models \citep{zeghidour2021soundstream,dfossez2023encodec}. 
>
> Although achieving impressive naturalness and diversity in synthesized audios, VALL-E and its variants are plagued by following several drawbacks.
> First, neural codec codes, originally designed for audio compression, exhibit lower fidelity compared to the well-established mel-spectrogram \citep{puvvada2024discrete}. This phenomenon is also observed in the field of graphics, where the reconstruction quality of vector-quantized tokenizers typically lags behind that of their continuous-valued counterparts \citep{rombach2022high,huang2023not,kaiming2024autoregressive}.
> Second, the codec language model VALL-E suffers from robustness issues stemming from its random sampling strategy, which is inherited from text language model for selecting discrete tokens.
> This issue is more pronounced with acoustic tokens as opposed to textual ones due to the greater similarity among consecutive codec codes, which can lead to continuous stretches of silence or persistent noise \citep{wang2023valle, song2024ellav}.
> Third, neural codec language models typically necessitate a complicated two-pass decoding process, involving an autoregressive (AR) model for generating coarse primary audio tokens, followed by a non-autoregressive (NAR) model to iteratively predict the rest multiple codebook codes for refinement. 
> This multi-step procedure compromises inference efficiency, leading to increased computational demands and doubled storage requirements.
> 
> To address the limitations associated with discrete tokens based codec language models, we are rethinking the potential of continuous speech representations and aim to determine whether continuous-valued tokens can supplant discrete-valued tokens within the paradigm of autoregressive speech synthesis models.
> The successful implementation of the autoregressive model without discrete vector quantization faces two key challenges.
> - \textbf{How to set training objective for continuous representation?} 
> The continuous space significantly differs from that of vector-quantized tokens, for which autoregressive language models typically adopt a next-token prediction objective, with cross-entropy loss to measure the discrepancy between the predicted distribution and the ground truth. 
> - \textbf{How to enable sampling mechanism in continuous space?} The sampling strategy is a critical component in both text generation and speech synthesis systems, as it introduces diversity into the output and enhances the generalization ability. However, continuous-valued tokens based models can not employ top-p random sampling method used in discrete codec language models.
>
> In this work, we propose a continuous mel-spectrogram\footnote{We leave the exploration of other continuous representations, such as VAE latent hidden states, for future research endeavors.} based autoregressive language model (called ***MELLE***) for text-to-speech synthesis, as illustrated in Figure \ref{fig:overview}. ***MELLE*** is a robust single-pass zero-shot TTS model which autoregressively predicts mel-spectrogram frames based on previous mel-spectrogram and text tokens, thereby avoiding the inherent flaws associated with sampling discrete codec codes. 
> The mel-spectrogram is then converted into waveform utilizing an off-the-shelf vocoder. 
> In response to the aforementioned challenges, we first substitute  cross-entropy loss with regression loss and introduce a spectrogram flux loss to promote variation of predicted mel-spectrograms and eliminate repetition issues.
> Second, we design a latent sampling module, derived from variational inference, functions as a sequence sampling strategy thereby enhancing the diversity of the generated audio samples. 
> As an option, by adjusting the reduction factor, ***MELLE*** can predict multiple frames at one step and accelerate inference, thereby further alleviating the robustness issues associated with long-sequence modeling and maintaining satisfactory performance.  
>
> We conducted evaluations of our ***MELLE*** on both the large-scale 50K-hour Libriheavy \citep{kang2024libriheavy} training dataset and the relatively small 960-hour LibriSpeech \citep{panayotov2015librispeech} training dataset. 
> Following recent works, we use LibriSpeech test-clean set for zero-shot TTS evaluation.
> Experimental results demonstrate that the proposed ***MELLE*** is on par with VALL-E 2 \citep{chen2024valle2} in objective metrics, and surpasses VALL-E 2 in subjective metrics. It also outperforms previous neural codec language models, including VALL-E and its other variants, achieving superior performance across multiple metrics that reflect naturalness, robustness, similarity, and inference efficiency. 
>
> Specifically, ***MELLE*** surpasses the ground truth audios in WER (1.47\% vs. 1.61\%), achieving a 47.9\% relative reduction in WER compared to VALL-E and an 8.1\% reduction compared to VALL-E 2 on the continuation inference task for zero-shot TTS. 
> For subjective evaluations, ***MELLE*** is more favorably received by human listeners than previous models, achieving comparable performance to the original ground truth in terms of MOS (4.20 vs. 4.29) and CMOS (-0.032 for ours vs. ground truth), and an even higher SMOS (4.40 vs. 3.94) than the ground-truth speech.

## 2.Related Works: 相关工作

### Traditional TTS

> Traditional speech synthesis methods can be categorized concatenative systems, parametric systems, and end-to-end neural systems. Concatenative TTS systems deconstruct original audio waves into smaller segments and then reassembles them using algorithms like Viterbi, followed by signal processing techniques, to create new audio waves \citep{hunt1996unit,black1997automatically}.
> Parametric TTS systems convert speech waves into spectrograms, utilizing acoustic parameters such as fundamental frequency and duration to synthesize new audio outputs \citep{zen2013statistical,tokuda2013speech}.
> With the rapid development of neural networks, end-to-end neural TTS systems are proposed to simplify previous speech synthesis pipeline via a single neural network, eliminating the need for the production of these linguistic and acoustic features \citep{wang2017tacotron,li2019transformertts,ren2019fastspeech}.
>
> The advanced end-to-end neural TTS models, such as Tacotron \citep{wang2017tacotron}, TransformerTTS \citep{li2019transformertts}, and FastSpeech \citep{ren2019fastspeech}, usually generate mel-spectrograms directly from texts, then synthesize the audio results from the mel-spectrogram by a vocoder such as WaveNet \citep{oord2016wavenet}. TransformerTTS employs Transformer-based encoder-decoder network as the main framework to replace RNN structures in Tacotron. FastSpeech further improve the speech quality and decoding efficiency using the non-autoregressive generation model with a duration module. These model are trained on small-scale, clean, single-speaker or few-speaker dataset, such as LJSpeech \citep{ljspeech17} and LibriTTS \citep{zen2019libritts}. Our ***MELLE*** leverages the well-established mel-spectrogram as the intermediate representation, however, it differs significantly in two key aspects: (1) we adopt decoder-only network as foundational structure with improved methods, such as variational inference and spectrogram flux loss, (2) ***MELLE*** is capable of zero-shot TTS via training on large-scale speech-text paired data, like Libriheavy \citep{kang2024libriheavy}.

### Zero-Shot TTS

> Motivated by the zero-shot and in-context learning capabilities of large language models (LLMs) on natural language processing (NLP) tasks \citep{OpenAI2023GPT4TR, touvron2023llama}, various research works are proposed to address zero-shot TTS through a language modeling approach \citep{wang2023valle,zhang2023vallex,jiang2023mega}.
> VALL-E \citep{wang2023valle} first regards TTS tasks as a conditional language task, which utilizes neural codec codes as intermediate representation instead of mel-spectrogram, then uses a codec decoder to recover the waveform from predicted codec codes.
> VALL-E employs two-stage modeling method, with an autoregressive model for generating coarse audio tokens, followed by a non-autoregressive model to iteratively predict multi-codebook codes for refinement. VALL-E X \citep{zhang2023vallex} extends VALL-E into multi-lingual scenario to support zero-shot cross-lingual speech synthesis and speech-to-speech translation.
> Mega-TTS \citep{jiang2023mega} proposes to disentangle the multiple attributes in speech, such as content, timbre, prosody, and phase attributes, then model each of them according to their intrinsic properties with a language modeling approach.
> ELLA-V \citep{song2024ellav}, RALL-E \citep{xin2024ralle}, and VALL-E R \citep{han2024valler} aims to improve robustness and stability of VALL-E via additional fine-grained speech-text alignment information. 
> BASE TTS \citep{lajszczak2024base} employs discrete tokens derived from WavLM \citep{chen2022wavlm} and scales the codec language model to larger parameters and training data. Seed-TTS \citep{anastassiou2024seed} replaces NAR model in VALL-E with a diffusion model, which generates continuous speech representations according generated speech tokens from AR stage. 
> In parallel to our work, VALL-E 2 \citep{chen2024valle2} shares the same architecture as VALL-E but employs a repetition-aware sampling strategy that promotes more deliberate sampling choices.
>
> Other studies have investigated fully non-autoregressive approaches to increase the speed of model inference. For instance, SoundStorm \citep{borsos2023soundstorm} adapts a parallel, non-autoregressive, confidence-based decoding scheme for the generation of codec codes.
> StyleTTS 2 \citep{li2024styletts} and NaturalSpeech 3 \citep{ju2024naturalspeech3} use diffusion model to achieve better TTS synthesis.
> Voicebox \citep{le2024voicebox} and Audiobox \citep{vyas2023audiobox} employ non-autoregressive flow-matching based models for transcript-guided speech generation.
> Recently, E2 TTS \citep{eskimez2024e2} presents a fully non-autoregressive TTS systems consisting of flow-matching-based mel-spectrogram generator trained with the audio infilling task, and a vocoder.
> Different previous works, ***MELLE*** leverages continuous-valued tokens based autoregressive language modeling approach with variational inference for text-to-speech synthesis. 

## 3.Methodology: 方法

## 4.Experiments: 实验

## 5.Results: 结果

## 6.Conclusions: 结论

> In this study, we propose a continuous acoustic representation-based language modeling approach for zero-shot text-to-speech synthesis tasks, thereby eliminating the use of discrete vector quantization.
> By exploring the potential of mel-spectrograms within the paradigm of language modeling, the proposed ***MELLE*** directly predicts mel-spectrograms conditioned on text content and speech prompt. This approach eliminates the need for the two-pass training and inference procedures typical of neural codec language model VALL-E, and can further accelerate decoding by setting the reduction factor. 
> With the aid of latent sampling and spectrogram flux loss, ***MELLE*** is capable of producing more diverse and robust predictions, attaining results comparable to human performance in subjective evaluations.

