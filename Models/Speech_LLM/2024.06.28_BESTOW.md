# BESTOW

<details>
<summary>基本信息</summary>

- 标题: BESTOW: Efficient and Streamable Speech Language Model with the Best of Two Worlds in GPT and T5
- 作者:
  - 01 [Zhehuai Chen](../../Authors/Zhehuai_Chen.md)
  - 02 [He Huang](../../Authors/He_Huang.md)
  - 03 [Oleksii Hrinchuk](../../Authors/Oleksii_Hrinchuk.md)
  - 04 [Krishna C. Puvvada](../../Authors/Krishna_C._Puvvada.md)
  - 05 [Nithin Rao Koluguri](../../Authors/Nithin_Rao_Koluguri.md)
  - 06 [Piotr Żelasko](../../Authors/Piotr_Żelasko.md)
  - 07 [Jagadeesh Balam](../../Authors/Jagadeesh_Balam.md)
  - 08 [Boris Ginsburg](../../Authors/Boris_Ginsburg.md)
- 机构:
  - [NVIDIA](../../Institutions/Nvidia.md)
- 时间:
  - 预印时间: 2024.06.28 ArXiv v1
  - 更新笔记: 2024.07.01
- 发表:
  - 期刊/会议 
- 链接:
  - [ArXiv](https://arxiv.org/abs/2406.19954)
  - [DOI]()
  - [Github]()
  - [Demo]()
  - [Scholar](https://scholar.google.com/scholar?cluster=)
- 标签:
  - [语言模型](../../Tags/LanguageModel.md)
- 页数: 9
- 引用: 56
- 被引: ?
- 数据:
  - ? 
- 对比:
  - ?
- 复现:
  - ?

</details>

## Abstract: 摘要

> Incorporating speech understanding capabilities into pretrained large-language models has become a vital research direction (SpeechLLM). 
> The previous architectures can be categorized as:
> i) {\em GPT-style}, prepend speech prompts to the text prompts as a sequence of LLM inputs like a decoder-only model; 
> ii) {\em T5-style}, introduce speech cross-attention to each layer of the pretrained LLMs. 
> 
> We propose BESTOW architecture to bring {\em the BESt features from TwO Worlds} into a single model that is highly efficient and has strong multitask capabilities.
> Moreover, there is no clear streaming solution for either style, especially considering the solution should generalize to speech multitask.
> We reformulate streamable SpeechLLM as a read-write policy problem and unifies the offline and streaming research with BESTOW architecture.  
> Hence we demonstrate the first open-source SpeechLLM solution that enables {\em Streaming} and  {\em Multitask at scale} (beyond ASR) at the same time. 
> This streamable solution achieves very strong performance on a wide range of speech tasks (ASR, AST, SQA, unseen DynamicSuperb). It is end-to-end optimizable, with {\em lower training/inference cost}, and demonstrates LLM knowledge transferability to speech. 

## 1.Introduction: 引言

## 2.Related Works: 相关工作

## 3.Methodology: 方法

## 4.Experiments: 实验

## 5.Conclusions: 结论

> In this work, we propose the first open SpeechLLM solution that enables {\em Streaming} and  {\em Multitask at scale} (beyond ASR) at the same time. %Moreover, the solution is end-to-end optimizable and allows LLM knowledge transfer to speech.
> The solution is based on a different backbone architecture from the popular Speech-LLaMA variants that is based on cross-attention and read-write policy. The novel backbone unifies the offline and streaming modes and achieves state-of-the-art on several large-scale and multitask  speech-to-text benchmarks, with {\em lower training/inference cost}.
> We will release the code and checkpoints to promote next-generation SpeechLLM  using this backbone design. 
