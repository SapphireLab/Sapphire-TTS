# BESTOW

<details>
<summary>基本信息</summary>

- 标题: BESTOW: Efficient and Streamable Speech Language Model with the Best of Two Worlds in GPT and T5
- 作者:
  - 01 [Zhehuai Chen](../../Authors/Zhehuai_Chen.md)
  - 02 [He Huang](../../Authors/He_Huang.md)
  - 03 [Oleksii Hrinchuk](../../Authors/Oleksii_Hrinchuk.md)
  - 04 [Krishna C. Puvvada](../../Authors/Krishna_C._Puvvada.md)
  - 05 [Nithin Rao Koluguri](../../Authors/Nithin_Rao_Koluguri.md)
  - 06 [Piotr Żelasko](../../Authors/Piotr_Żelasko.md)
  - 07 [Jagadeesh Balam](../../Authors/Jagadeesh_Balam.md)
  - 08 [Boris Ginsburg](../../Authors/Boris_Ginsburg.md)
- 机构:
  - [NVIDIA](../../Institutions/Nvidia.md)
- 时间:
  - 预印时间: 2024.06.28 ArXiv v1
  - 更新笔记: 2024.07.01
- 发表:
  - 期刊/会议 
- 链接:
  - [ArXiv](https://arxiv.org/abs/2406.19954)
  - [DOI]()
  - [Github]()
  - [Demo]()
  - [Scholar](https://scholar.google.com/scholar?cluster=)
- 标签:
  - [语言模型](../../Tags/LanguageModel.md)
- 页数: 9
- 引用: 56
- 被引: ?
- 数据:
  - ? 
- 对比:
  - ?
- 复现:
  - ?

</details>

## Abstract: 摘要

> Incorporating speech understanding capabilities into pretrained large-language models has become a vital research direction (SpeechLLM). 
> The previous architectures can be categorized as:
> i) {\em GPT-style}, prepend speech prompts to the text prompts as a sequence of LLM inputs like a decoder-only model; 
> ii) {\em T5-style}, introduce speech cross-attention to each layer of the pretrained LLMs. 
> 
> We propose BESTOW architecture to bring {\em the BESt features from TwO Worlds} into a single model that is highly efficient and has strong multitask capabilities.
> Moreover, there is no clear streaming solution for either style, especially considering the solution should generalize to speech multitask.
> We reformulate streamable SpeechLLM as a read-write policy problem and unifies the offline and streaming research with BESTOW architecture.  
> Hence we demonstrate the first open-source SpeechLLM solution that enables {\em Streaming} and  {\em Multitask at scale} (beyond ASR) at the same time. 
> This streamable solution achieves very strong performance on a wide range of speech tasks (ASR, AST, SQA, unseen DynamicSuperb). It is end-to-end optimizable, with {\em lower training/inference cost}, and demonstrates LLM knowledge transferability to speech. 

## 1.Introduction: 引言

> With the huge success of large language models (LLMs)~\cite{brown2020gpt,team2023gemini}, researchers start to explore the possibilities of extending the capabilities of LLMs with multi-modal understanding skills, and many works have been proposed to support image and audio understanding~\cite{alayrac2022flamingo,liu2024llava,zhang2023speechgpt,gong2023ltu-as,tang2023salmonn}.
>
> This work focuses on leveraging speech encoder and LLM ({\em SpeechLLM}) to build a speech foundational model  for  many speech-and-audio to text applications (STT). 
> One popular framework in the direction is Speech-LLaMA~\cite{wu2023decoder,fathullah2023prompting} and its extensions, which  updates the input of LLM by prepending speech prompts to the text prompts while keeping the rest of LLM unchanged or LoRA finetuned. This design shows good modularity which allows knowledge transfer from LLMs to speech and results in strong ASR and AST performance~\cite{wang2023slm,qwen2023}. 
> The modular design also shows strong in-context learning ability \cite{chen2024salm}. 
> \cite{tang2023salmonn} further introduces a bespoke Q-former module before prepending speech prompts to better bring speech, audio, and music features to LLM space.

> Nevertheless, there are several potential drawbacks in this popular design:
> i) Efficiency problem raised from the interaction between self-attention and the longer speech embeddings than text, which will be elaborated in Section~\ref{sec:efficiency}. Workarounds like massive downsampling of speech embeddings usually come with information loss and cannot completely avoid. % the per-layer and per-step LLM computation increase from speech. 
> ii) As the speech embeddings of the whole utterance are treated as prompt and always prepended beforehand, it disables many streaming applications in speech, e.g. streaming ASR and simultaneous speech translation (SST). 

> In this work, we propose an alternative {\em modular} and {\em multitask} SpeechLLM design that is both {\em streamable} and {\em efficient}. 

> Our main contributions are summarized as follows:

> \item To the best of our knowledge, this is the first open SpeechLLM solution that enables {\em streaming} and  {\em multitask at scale} (beyond ASR) at the same time. Moreover, the solution is end-to-end optimizable and allows LLM knowledge transfer to speech.
> \item Propose a different backbone architecture from the popular Speech-LLaMA variants that is based on cross-attention and read-write policy. The novel backbone unifies the offline and streaming modes and achieves state-of-the-art on several large-scale and multitask  speech-to-text benchmarks (ASR, AST, SQA, DynamicSuperb), with {\em lower training/inference cost}.

## 2.Related Works: 相关工作

### 2.1.Speech Foundational Model

> Motivated by the success of foundation models in NLP~\cite{brown2020gpt,team2023gemini}, recent speech foundational model research has been shifted towards developing universal pretrained models capable of handling multilingual speech and audio tasks. Recent advances include but are not limited to: 
> i) Large scale multilingual self-supervised learning and semi-supervised learning  to leverage unlabled speech, e.g. XLSR~\cite{conneau2020unsupervised} and USM~\cite{zhang2023google}. 
> ii) Large scale multitask supervised training, e.g. Whisper variants~\cite{radford2022robust,peng2024owsm} and SeamlessM4T~\cite{barrault2023seamless}. 
> iii) More powerful and  efficient speech encoder design, e.g. Conformer and variants~\cite{gulati2020conformer,rekesh2023fast}. 
> iv) Multilingual multitask speech benchmarks, e.g. XTREME~\cite{conneau2022xtreme} and ML-SUPERB~\cite{shi2023ml}.

### 2.2.SpeechLLM

> Recently, researchers started to look at combining pretrained speech models with  large language models to form a new type of speech foundational model, {\em SpeechLLM}, which can be  categorized through speech embedding types. SpeechGPT~\cite{zhang2023speechgpt} and AudioPaLM~\cite{rubenstein2023audiopalm}  quantize speech into discrete tokens, so speech tokens and text tokens can be combined into a single vocabulary. 
> Nevertheless, this approach is limited by the quality and diversity of the speech quantization model especially for STT tasks.
> Another more popular method is to feed  speech into pretrained speech encoder, where the speech features are then projected into the word embedding space of the LLM~\cite{chen2024salm,kong2024audio,qwen2023,tang2023salmonn,gong2023ltu-as}. Our work belongs to this category.

> After extracting speech features, the previous architectures to provide speech information to LLMs can be further categorized into two branches: 
> i) {\em GPT-style} models~\cite{chen2024salm,tang2023salmonn,qwen2023} directly concatenate the speech prompts with text prompts and use the combined sequence as input to the LLM like a decoder-only GPT model; 
> ii) Flamingo and its extension~\cite{kong2024audio,alayrac2022flamingo,radhakrishnan2023whispering} are another branch of works, where cross-modal cross-attention is added to each layer/block of the pretrained GPT-based LLM with shared text query. %, such that the text tokens can access speech information at every layer of the LLM. 
>
> The resultant architecture is similar to the {\em T5 architecture}~\cite{2020t5}. 
>
> Some recent works look at introducing cross-attention layers into the first branch of methods, but with different motivations and resultant designs from our work. E.g. \cite{yu2023connecting,hussain2023m} introduces cross attention before the above concatenation to bridge the gap between the speech encoder and LLM, whose computation is the same or more.

### 2.3.Streaming Speech Models

> In streaming ASR, Efforts have been made to enable streaming into Transformer~\cite{zhang2020transformer,noroozi2023stateful}. These methods either use limited context with offline models  or train models in a streaming manner. 
> The methods usually build on top of Transducer~\cite{graves2012sequence} which rarely benefits from pretrained LLM.
>
> In simultaneous speech translation, researchers have been looking at  fixed and adaptive read-write policies. Wait-k~\cite{ma2020simulmt} and variants are the most longstanding fixed policy. Typical adaptive policies try to learn the policy with a bespoke model from the training data, e.g. EMMA~\cite{ma2023efficient} and ITST~\cite{zhang2022information}.

---

### Why Streaming SpeechLLM is Hard

## 3.Methodology: 方法

## 4.Experiments: 实验

## 5.Conclusions: 结论

> In this work, we propose the first open SpeechLLM solution that enables {\em Streaming} and  {\em Multitask at scale} (beyond ASR) at the same time. %Moreover, the solution is end-to-end optimizable and allows LLM knowledge transfer to speech.
> The solution is based on a different backbone architecture from the popular Speech-LLaMA variants that is based on cross-attention and read-write policy. The novel backbone unifies the offline and streaming modes and achieves state-of-the-art on several large-scale and multitask  speech-to-text benchmarks, with {\em lower training/inference cost}.
> We will release the code and checkpoints to promote next-generation SpeechLLM  using this backbone design. 
