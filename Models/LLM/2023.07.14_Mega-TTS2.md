---
PDF: 
标题: "Mega-TTS 2: Boosting Prompting Mechanisms for Zero-Shot Speech Synthesis"
作者:
  - Ziyue Jiang
  - Jinglin Liu
  - Yi Ren
  - Jinzheng He
  - Zhenhui Ye
  - Shengpeng Ji
  - Qian Yang
  - Chen Zhang
  - Pengfei Wei
  - Chunfeng Wang
  - Xiang Yin
  - Zejun Ma
  - Zhou Zhao
机构:
  - 浙江大学
  - 字节跳动
代码: 
ArXiv: https://arxiv.org/abs/2307.07218
提出时间: 2023-07-14
出版社: 
发表期刊:
  - ICLR2024
发表时间: 2024-01-16
引文数量: 21
被引次数: 9
tags:
  - 语音合成_TTS
  - 零样本_Zero-Shot
  - 大语言模型_LLM
DOI: 
aliases:
  - Mega-TTS2
ArXiv最新版本: "4"
ArXiv最新时间: 2024-04-10
PageNum: "21"
Demo: https://boostprompt.github.io/boostprompt/
---
# Mega-TTS 2: Boosting Prompting Mechanisms for Zero-Shot Speech Synthesis

## Abstract

> Zero-shot text-to-speech (TTS) aims to synthesize voices with unseen speech prompts, which significantly reduces the data and computation requirements for voice cloning by skipping the fine-tuning process.
> However, the prompting mechanisms of zero-shot TTS still face challenges in the following aspects: 
>
> 1. previous works of zero-shot TTS are typically trained with single-sentence prompts, which significantly restricts their performance when the data is relatively sufficient during the inference stage.
> 2. The prosodic information in prompts is highly coupled with timbre, making it untransferable to each other.
>
> This paper introduces Mega-TTS 2, a generic prompting mechanism for zero-shot TTS, to tackle the aforementioned challenges.
> Specifically, we design a powerful acoustic autoencoder that separately encodes the prosody and timbre information into the compressed la tent space while providing high-quality reconstructions.
> Then, we propose a multi-reference timbre encoder and a prosody latent language model (P-LLM) to extract useful information from multi-sentence prompts.
> We further leverage the probabilities derived from multiple P-LLM outputs to produce transferable and controllable prosody.
> Experimental results demonstrate that Mega-TTS 2 could not only synthesize identity-preserving speech with a short prompt of an unseen speaker from arbitrary sources but consistently outperform the fine-tuning method when the volume of data ranges from 10 seconds to 5 minutes.
> Furthermore, our method enables to transfer various speaking styles to the target timbre in a fine-grained and controlled manner.
> Audio samples can be found in https://boostprompt.github.io/boostprompt/.

## 1.Introduction

> In recent years, there has been remarkable progress in the development of text-to-speech (TTS) technology (Shen et al., 2018; Jia et al., 2018; Li et al., 2019; Kim et al., 2020; Ren et al., 2019; 2020; Kim et al., 2021; 2022a).
> Among them, adaptive TTS systems (Chen et al., 2021; Min et al., 2021; Kim et al., 2022b) are capable of cloning personalized voices given a few minutes of speech data.
> However, the performance of these systems relies heavily on the quality and quantity of the data utilized during the fine-tuning phases (Tan et al., 2021).
> Insufficient data during the fine-tuning stages can lead to diminished audio naturalness or speech intelligibility (Kang et al., 2023).
> Moreover, the computational demands also constrain its application for cloning everyone’s voice.

> To reduce such a reliance, existing works leverage generative models to perform zero-shot TTS (Cooper et al., 2020a; Casanova et al., 2022; Huang et al., 2022a; Kang et al., 2023; Kharitonov et al., 2023; Wang et al., 2023; Shen et al., 2023b; Matthew et al., 2023).
> These powerful models can effectively synthesize speech given only a single speech prompt, eliminating the need for data preparation and the computational requirements for fine-tuning methods.
> However, the prompting mechanisms of current solutions still face two primary challenges: 
>
> - Lack of multi-sentence prompting strategies.
> Previous works of zero-shot TTS typically employ single-sentence speech prompts during training (Wang et al., 2023; Shen et al., 2023b; Matthew et al., 2023).
> In inference, the information in the single-sentence speech prompt is insufficient to guide the zero-shot TTS systems to imitate the voice variability of a natural person perfectly.1From another perspective, the performance of fine-tuning methods can be further improved by increasing the amount of data, while zero-shot TTS systems lack an appropriate strategy to extract useful information from multi-sentence speech prompts.
> - Lack of specialized prompting mechanism for prosodic information.
> Current solutions for zero-shot TTS primarily concentrate on improving the similarity of timbre and prosody between the generated speech and the prompts.
> However, they neglect to express various unseen prosodic styles in a controlled manner while also preserving the unique timbre of the given one-sentence prompt.
> In order to control the prosodic styles, it is necessary to disentangle the prosody information from speech prompts.

> We address the above challenges by decomposing speech into content, timbre, and prosody.
> Intuitively, representing speeches for numerous speakers requires a substantial number of codebook entries for timbre modeling (Defossez et al., 2022; Yang et al., 2023).
> Through the decoupling of prosody information, a highly compact codebook for prosody modeling can be obtained, which enables our model to effectively handle extremely long prompts and have flexible control over prosodic styles.
> Therefore, this work proposes Mega-TTS 2, a generic framework that boosts the prompting mechanisms for zero-shot TTS systems.
> Specifically, we begin by designing an acoustic autoencoder that can effectively decompose speech into prosody and timbre representations and represent them in a compact latent space.
> Then, we design a **Multi-Reference Timbre Encoder (MRTE)** and a prosody latent language model (P-LLM) to extract useful information from multi-sentence prompts.
> In addition to the multi-sentence prompting mechanism, we propose a prosody interpolation technique to control the generation process of prosody codes by utilizing prosody prompts from multiple speakers while maintaining the target speaker’s timbre.
> By utilizing the probabilities derived from both the prosodic prompts of the target speaker and the auxiliary speaker, the prosodic styles of speech can be generated in a controlled manner.
> Experiments on LibriSpeech test-clean (Panayotov et al., 2015) and ESD (Zhou et al., 2021) datasets show that Mega-TTS 2 outperforms other state-of-the-art fine-tuning and zero-shot TTS models in terms of speaker similarity and speech naturalness.
> Notably, when the length of the prompt is further extended, our method surpasses the fine-tuning baseline model in the objective and subjective evaluations.
> The extensive studies on adaptive prosody transfer further highlight the superiority of our proposed prompting mechanisms.
> The main contributions of this work are summarized as follows: 
> - We design an acoustic autoencoder that separately compresses the prosody and timbre information into the latent space, which allows our model to process prompts of up to 300 seconds in length effectively.
> - We propose a multi-reference timbre encoder and an auto-regressive prosody language model to extract fine-grained information from multiple reference speeches, which bridges the speaker similarity gap between zero-shot methods and fine-tuning methods.
> - Experimental results also reveal that the performance of Mega-TTS 2 surpasses the powerful fine-tuning baseline when we have 10 seconds to 5 minutes of data for each unseen speaker, indicating the superiority of our proposed prompting mechanisms.
> - The proposed prosody interpolation technique ensures the controllability of prosody and is capable of transferring various speaking styles to the desired timbre.
> For instance, we can transform a voice with a sad tone into a happier one with the auxiliary prosody prompt from another speaker.

## 2.Related Works

### Adaptive TTS

> Adaptive TTS (Arik et al., 2018; Kons et al., 2019; Moss et al., 2020; Chien et al., 2021) focuses on synthesizing personalized voice for any user with few data.
> During the adaptation process, a TTS model pre-trained on a multi-speaker speech dataset is typically fine-tuned with few adaptation data for the target voice (Tan et al., 2021).
> Chen et al. (2018) design independent learned embeddings for each speaker, which requires few data at deployment time to adapt to new speakers rapidly.
> AdaSpeech (Chen et al., 2021) proposes an acoustic-condition modeling method for high-quality and efficient customization of new voices.
> There are also some works leveraging the meta-learning approach (Chen et al., 2018; Min et al., 2021; Huang et al., 2022b) and data augmentation (Cooper et al., 2020b; Yang & He, 2020) for speaker adaptation.
> However, although some works are data-efficient (Min et al., 2021; Huang et al., 2022b) and parameter-efficient (Chen et al., 2021), these systems still suffer from audio quality issues when data size is small, as well as computational cost issues due to hundreds of fine-tuning steps.

### Zero-Shot TTS

> Zero-shot adaptation (Jia et al., 2018; Arik et al., 2018; Cooper et al., 2020a; Casanova et al., 2021; Wu et al., 2022; Huang et al., 2022b;a; Casanova et al., 2022) aims to synthesize unseen voices with a speaker encoder that extracts speaker embeddings from the reference audio.
> This scenario is highly attractive because it does not require any adaptation data or parameters (Kang et al., 2022).
> The attention-based adaptation method (Choi et al., 2020; Zhou et al., 2022; Yin et al., 2022; Lin et al., 2021) utilizes attention mechanisms to extract fine-grained speech features from reference audios.
> Among them, Attentron (Choi et al., 2020) proposes to extracts useful style information from arbitrary number of reference audios.
> However, they do not separately model the timbre and prosody information, lacking controllability over timbre and prosody.
> Most recently, some works (Kharitonov et al., 2023; Zhang et al., 2023) are proposed to use in-context learning methods (Dong et al., 2022) to efficiently extract speaker information from acoustic prompts and have achieved remarkable results in zero-shot TTS.
> [VALL-E (2023)](../2023.01_VALL-E/2023.01_VALL-E.md) proposes the neural codec language model that exhibits strong in-context learning capability for zero-shot speech generation.
> NaturalSpeech 2 (Shen et al., 2023b) introduces in-context learning to latent diffusion model (Rombach et al., 2022), which is achieved by partitioning a speech clip into the prompt and target regions.
> VoiceBox (Matthew et al., 2023) solves a text-guided speech-infilling task with large-scale data to learn from context information.
> However, these methods are trained with single-sentence prompts, lacking an appropriate strategy to extract fine-grained information from multi-sentence speech prompts.

### Prosody Transfer for Speech Synthesis

> Prosody transfer (Lee & Kim, 2019; Klimkov et al., 2019; Gururani et al., 2019; Pan & He, 2021; Karlapati et al., 2022) aims to transfer the prosody from a reference utterance to the synthesized target speech, which is essential for producing natural and expressive speech in a controlled manner (Wagner & Watson, 2010).
> Skerry-Ryan et al. (2018) first integrate a prosody reference encoder into a TTS system based on Tacotron (Wang et al., 2017), which is capable of performing similar-text prosody transfer.
> Recent works try to transfer prosody in different-text and different-speaker settings (Karlapati et al., 2020; Zaıdi et al., 2021) with the bottleneck of the prosody encoder.
> Among them, Daft-Exprt (Zaıdi et al., 2021) uses a gradient reversal layer to penalize the prosody encoder if its output contains information about the speaker identity from the reference utterance, which enhances the target speaker fidelity for cross-speaker prosody transfer.
> However, as pointed out by Sigurgeirsson & King (2023), current solutions do not learn a transferable representation of prosody, but rather an utterance-level representation that is relatively dependent on both the reference speaker and reference text.

## 3.Method

> This section introduces Mega-TTS 2.
> To begin with, we provide an intuitive illustration of how Mega-TTS 2 decomposes the timbre and prosody information from speech.
> Next, we provide detailed explanations of our prompting mechanisms and the two-stage training process of the proposed model.

### 3.1.Decomposition for Prosody and Timbre

#### Problem Formulation

> Denote $H(X)$ as the Shannon entropy of $X$ and Denote $I(Y;X)$ as the mutual information. 
> We assume that the mel-spectrogram $y$ can be reconstructed through the following generative process: $y=D(z_c,z_{pd},z_t,g)$, where $z_c$ and $z_t$ denote the fine-grained content and timbre hidden states.
> $g$ denotes the global style information that contains timbre and prosody.
> We assume that $z_{pd}=(z_p,z_d)$ contains the fine-grained prosodic style information of pitch and energy $z_p$ and duration $z_d$.
> $z_d=Aligner(y)$ can be obtained by the external alignment tools (McAuliffe et al., 2017) and disentangled from $z_{pd}$.
> Denote $D$ as the mel-spectrogram decoder.
> Our goal is to construct an autoencoder-based model to disentangle speech components.

#### Decomposition via Corpus Partition

### 3.2.Compressive Acoustic Autoencoder

### 3.3.Prosody Latent Language Model

### 3.4.Prosody Interpolation

## 4.Experiments

### 4.1.Experimental Setup

### 4.2.Results of Zero-Shot Speech Synthesis

### 4.3.Results of Prosody Transfer

### 4.4.Ablation Studies

## 5.Conclusions

> In this paper, we present Mega-TTS 2, a framework that boosts the prompting mechanisms for zero shot TTS systems.
> With the proposed multi-sentence prompting strategy, our approach outperforms the fine-tuning baseline when 10 seconds to 5 minutes of adaptation data is available for each speaker.
> Furthermore, our method utilizes a prosody interpolation technique to successfully transfer various prosodic styles to the target speaker while preserving the target speaker’s timbre.
> Experimental results demonstrate that our method exhibits superior performance in terms of audio naturalness and speaker similarity.
> Due to space limitations, we include additional discussions in the appendix.
