---
PDF: 
标题: "InstructTTS: Modelling Expressive TTS in Discrete Latent Space with Natural Language Style Prompt"
作者:
  - Dongchao Yang
  - Songxiang Liu
  - Rongjie Huang
  - Chao Weng
  - Helen Meng
机构:
  - TecentAI
  - 香港中文大学
  - 浙江大学
代码: 
ArXiv: https://arxiv.org/abs/2301.13662
提出时间: 2023-01-31
出版社: 
发表期刊: 
发表时间: 
引文数量: 77
被引次数: 32
tags:
  - 语音合成_TTS
DOI: 
aliases:
  - InstructTTS
ArXiv最新版本: 2
ArXiv最新时间: 2023-06-25
PageNum: 13
Demo: https://dongchaoyang.top/InstructTTS/
---
# InstructTTS: Modelling Expressive TTS in Discrete Latent Space with Natural Language Style Prompt

## Abstract

## 摘要

Expressive <span class='aca-term'>text-to-speech (TTS)</span> aims to synthesize speech with varying speaking styles to better reflect human speech patterns. In this study, we attempt to use natural language as a style prompt to control the styles in the synthetic speech, e.g., “Sigh tone in full of sad mood with some helpless feeling”. Considering that there is no existing <span class='aca-term'>TTS</span> corpus that is suitable to benchmark this novel task, we first construct a speech corpus whose speech samples are annotated with not only content transcriptions but also style descriptions in natural language. Then we propose an expressive TTS model, named <span class='aca-algo'>InstructTTS</span>, which is novel in the sense of the following aspects:
－　(1) We fully take advantage of self-supervised learning and crossmodal metric learning and propose a novel three-stage training procedure to obtain a robust sentence embedding model that can effectively capture semantic information from the style prompts and control the speaking style in the generated speech.
－　(2) We propose to model acoustic features in discrete latent space and train a novel discrete diffusion probabilistic model to generate vector-quantized (VQ) acoustic tokens rather than the commonlyused mel spectrogram.
－　(3) We jointly apply mutual information(MI) estimation and minimization during acoustic model training to minimize style-speaker and style-content MI, avoiding possible content and speaker information leakage from the style prompt.
Extensive objective and subjective evaluation has been conducted to verify the effectiveness and expressiveness of <span class='aca-algo'>InstructTTS</span>.
Experimental results show that <span class='aca-algo'>InstructTTS</span> can synthesize highfidelity and natural speech with style prompts controlling the speaking style. Synthesized samples are available online

## 介绍

Test-To-Speech (TTS) aims to generate human-like speech from input text, which attracts broad interest in the audio and speech processing community. Nowadays,the state-of-the-art TTS systems [1]–[3] are able to produce natural and high-quality speech. However, there still exists a big gap between TTS-synthetic speech and human speech in terms of expressiveness, which limits the broad applications of current speech synthesis systems. Many researchers now focus on a more challenging task, i.e., expressive TTS, which aims to model and control the speaking style (e.g., emotion,speaking-rate and so on) in the generated speech according to human’s demands. We note that there are generally two types of methods in the literature to learn speaking style information: one type uses auxiliary categorical style labels as the condition of the framework [4], [5], the other imitates the speaking style of a reference speech [6]–[9]. However, there are limitations in the diversity of expressiveness when categorical style labels are used, as these models can only generate a few pre-defined styles from the training set. While TTS models that use a reference utterance to generate a particular speaking style can be trained in an unsupervised manner and are generalizable to out-of-domain speaking styles, the style information extracted from the reference speech may not be easily understandable or interpretable. Additionally, it can be challenging to select a reference speech sample that precisely matches a user’s requirements.

语音合成（Test-To-Speech, TTS）旨在从输入文本生成类似于人类的语音, 这在音频和语音处理领域引起了广泛的兴趣. 现在, 最先进的 TTS 系统 [01], [02], [03] 能够产生自然且高质量的语音. 然而, TTS 合成的语音在表达能力方面与人类语音之间仍存在很大差距, 这限制了当前语音合成系统的广泛应用. 许多研究人员现在专注于更具挑战性的任务, 即表现性 TTS, 其旨在根据人类需求模拟和控制生成的语音的说话风格 (例如, 情感, 语速等等).
我们注意到, 在文献中通常有两种方法来学习说话风格信息:

1. 使用辅助的分类风格标签作为框架的条件 [04], [05], 然而, 使用分类风格标签时, 表现能力的多样性受到限制, 因为这些模型只能从训练集中生成少量预定义的风格.
2. 模仿参考语音的说话风格 [06], [07], [08], [09]. 而使用参考语音生成特定说话风格的 TTS 模型可以以无监督的方式进行训练, 并且对于领域外的说话风格具有通用性, 但从参考语音中提取的风格信息可能不易理解或解释. 此外，选择与用户需求精确匹配的参考语音样本可能具有挑战性。

For the first time, we study the modelling of expressive TTS with style prompt in natural language, where we meet with the following research problems:
(1) how to train a language model that can capture semantic information from the natural language prompt and control the speaking style in the generated speech;
(2) how to design an acoustic model to effectively model the challenging one-to-many learning problem of expressive TTS. In this paper, we will address these two challenges.

我们首次研究使用自然语言中风格提示的表现性 TTS 建模, 期间遇到如下研究问题:

1. 如何训练一个语言模型能够从自然语言提示中捕获语义信息并控制生成语言中的说话风格;
2. 如何设计一个声学模型以有效地建模表现性 TTS 中具有挑战性的一对多学习问题.

在本文中, 我们将解决这两个挑战.

The main contributions of this study are summarized as follows:
(1) For the first time, we study the modelling of expressive TTS with natural language prompts, which brings us a step closer to achieving user-controllable expressive TTS.
(2) We introduce a novel three stage training strategy to obtain a robust sentence embedding model that can effectively capture semantic information from the style prompts.
(3) We propose to model acoustic features in discrete latent space and cast speech synthesis as a sequence-to-sequence language modeling task. Specifically, we train a novel discrete diffusion model to generate vector-quantized (VQ) acoustic features rather than to predict the commonly-used hand-crafted intermediate acoustic features, such as the mel-spectrogram.
(4) We explore to model two types of VQ acoustic features:mel-spectrogram based VQ features and waveform-based VQ features. We demonstrate that the two types of VQ features can be effectively modeled by our proposed novel discrete diffusion model. Our waveform-based modelling method only needs one-stage training, and it is a non-autoregressive model,which is far different from the concurrent work VALL-E [10]and MusicLM [11].
(5) We jointly apply mutual information (MI) estimation and minimization during acoustic model training to minimize style-speaker and style-content MI, avoiding possible content and speaker information leakage from the style prompt.

本研究的主要贡献总结如下:

1. 首次研究了使用自然语言提示的表现性 TTS 建模, 这使得我们更接近实现用户可控的表现性 TTS.
2. 引入了一种新颖的三阶段训练策略以获得一个鲁棒的句子嵌入模型, 以有效地从风格提示中捕获语义信息.
3. 提出在离散隐空间中建模声学特征, 并将语音合成视为一个序列到序列的语言建模任务. 具体地, 我们训练了一个新颖的离散扩散模型以生成 VQ 声学特征而不是预测常用的手工制作的中间声学特征, 如梅尔频谱图.
4. 探索了两种类型的 VQ 声学特征: 基于梅尔频谱图的 VQ 特征和基于波形的 VQ 特征. 我们证明了这两种 VQ 特征可以有效地由我们提出的新颖的离散扩散模型建模. 基于波形的建模方法只需要一阶段的训练, 且是非自回归模型, 和 VALL-E [10] 和 MusicLM [11] 不同.
5. 我们在声学模型训练过程中同时应用互信息估计和最小化, 以最小化风格说话者和风格内容互信息, 避免可能来源于风格提示中的内容和说话者信息泄露.

The rest of this paper is organized as follows: 
- In Section II,we motivate our study by introducing the background and related work. 
- In Section III, we present the details of the dataset. 
- In Section IV, we introduce the details of our proposed methods. 
- The experimental setting, evaluation metrics and results are presented from Section V to Section VII. 
- The study is concluded in Section VIII.

## 相关工作

This study is built on several previous works on crossmodal representation learning, vector quantization, diffusion probabilistic models and expressive TTS. We briefly introduce the related studies to set the stage for our research and rationalize the novelty of our contributions.

本研究建立在跨模态表示学习, 向量量化, 扩散概率模型和表达性 TTS 的等多个先前工作的基础上. 我们简要介绍相关研究为饿哦们的研究设定背景, 并解释贡献的创新之处.

### A. 跨模态表示学习

Cross-modal representation learning aims to learn a common latent space for different modal data (e.g. text and image, text and speech). In general, two different modal encoders are used to extract deep feature representation, and then a variety of supervised or unsupervised strategies are devised to align the two modal representation spaces [13], [14]. 
In our study, we expect to control the acoustic features (such as pitch,emotion and speed) by a natural language sentence. To realize this target, we turn to cross-modal representation learning. The details will be discussed later.

跨模态表示学习旨在为不同模态数据 (如文本和图像, 文本和语音) 学习一个共同的隐空间. 通常使用两种不同模态编码器用于提取深层特征表示, 然后设计各种监督或无监督策略用于对齐这两个模态表示空间 [13], [14].
在我们的研究中, 我们希望通过自然语言句子来控制声学特征 (如音调, 情感和语速). 为了实现这一目标, 我们转向了跨模态表示学习, 具体细节在后面讨论.

## B. 向量量化

Vector quantization technique has been widely used in different fields, such as image [15]–[17] and speech processing[18]–[21]. VQ-VAE [15] is one of the most successful vector quantization models. VQ-VAE uses an encoder and a quantizer to compress an image into a low-dimensional discrete space,then a decoder is used to reconstruct the image from a sequence of discrete tokens. Inspired by VQ-VAE, a series of works adopt the idea to reconstruct mel-spectrogram or linear-spectrogram [22], [23]. Recently, many works have focused on reconstruct waveform by VQ-VAE. To supplement　the information loss during the VQ process, Residual-VQ(RVQ) [21] and Group-residual-VQ (GRVQ) [24] technique are proposed, which uses multiple different codebooks to encode the audio information. Nowadays, the majority of TTS systems focus on using an acoustic model (AM) to directly predict mel-spectrogram, then uses a pre-trained vocoder to recover waveform from the predicted mel-spectrogram [2],[25], [26]. However, the mel-spectrogram is highly correlated along both time and frequency axes in a complicated way,leading to a great difficulty for the AM to predict. Furthermore,the gap between the ground-truth (GT) mel-spectrogram and the predict one from AM degrades the performance due to the vocoder is trained on GT mel-spectrogram. In this study,instead of using AM to predict mel-spectrogram, we turn to predict learnable and vector-quantized acoustic representation,which is transformed to a discrete latent space.

向量量化技术已经被广泛应用于不同领域, 如图像处理 VQ-VQE [15], [16], [17] 和语音处理 [18], [19], [20], Residual-VQ [21].
VQ-VAE [15] 是最成功的矢量量化模型之一, 它使用编码器和量化器将图像压缩为低维离散空间, 然后使用解码器从一系列离散标记中重构图像. 受到 VQ-VAE 的启发, 一系列研究采用这一思想来重构梅尔频谱或线性频谱 [22], [23].
最近许多研究专注于通过 VQ-VAE 重构波形.
为了补充 VQ 过程中的信息损失, 提出了 Residual-VQ (RVQ) [21] 和 Group-Residual-VQ (GRVQ) [24] 技术, 使用多个不同的 codebooks 以编码音频信息.
现在, 大多数 TTS 系统专注于使用声学模型 AM 直接预测梅尔频谱, 然后使用与训练的声码器从预测的梅尔频谱中恢复波形 [02], [25], [26]. 然而梅尔频谱在时间和频率轴上以复杂的形式高度相关, 导致 AM 预测的难度很大.
此外, 因为声码器是在 GT 梅尔频谱上训练的, 所以因为真实 GT 的梅尔频谱和 AM 预测的梅尔频谱之间的差距降低了性能.
本项研究中, 不使用 AM 预测梅尔频谱, 而是转向预测可学习的, 向量量化的声学表示, 将其转化为离散隐空间.

### C. 表现性文字转语音

In the field of expressive speech synthesis, several works[6], [9], [12], [27]–[30] have been done. Wang et al. [6]propose to use global style tokens to control and transfer global speaking styles. Inspired by [6], many similar works propose to learn speaking style from a reference audio, such as, Li et al. [27] use a multi-scale style encoder capturing style information from reference audio to facilitate expressive speech synthesis; Huang et al. [30] propose a multi-level style adaptor to transfer speaking style. The most related to our work are Style-Tagging-TTS (ST-TTS) [31] and PromptTTS[32]. ST-TTS proposes to use style tags to guide the speaking style of synthsized speech, where style tags denote a short phrase or word representing the style of an utterance, such as emotion, intention, and tone of voice. In this study, we attempt to use longer natural language as style descriptions to control the styles in the synthetic speech, which is more complicated due to the fact that longer natural language prompts carry out more abundant semantic information and result in more complicated acoustic characteristics. Our concurrent work PromptTTS [32] proposed a similar idea with us, using a sentence as a style prompt to control the style information in TTS systems. They define five different style factors (gender, pitch, speaking speed, volume, and emotion),and assume the prompts have obvious style factor words, such as low-pitch, high-speaking speech and so on, which means that the model can obtain style information from local-level description. Different from PromptTTS, our study does not apply constraint on the form of the style prompts and allows the user to use any free-form natural language to describe a speaking style, resulting in a much more challenging machine learning problem. Furthermore, we focus on Mandarin Chinese TTS and construct the first Mandarin Chinese speech corpus applicable for style-prompt-controllable expressive TTS.

在表现性语音合成领域, 已经有几项研究成果 [06], [09], [12], [27], [28], [29], [30].
Wang 等人 [06] 提出使用全局风格 tokens 来控制和传递全局说话风格.
受到该项研究的启发, 许多类似的工作提出从参考音频中学习说话风格, 如 Li 等人 [27] 使用多尺度风格编码器从参考语音中捕获风格信息以促进表现性语音合成; Huang 等人 [30] 提出了一个多级风格适配器以迁移说话风格.
与本工作最相关的是 Style-Tagging-TTS, ST-TTS [31] 和 PromptTTS [32]. ST-TTS 提出使用风格标签以指导合成语音的说话风格, 其中风格标签表示一个表示话语风格的短语或词语, 如情感, 意图, 语气等. 在本研究中, 我们尝试使用更长的自然语言作为风格描述来控制合成语音中的风格, 这更加复杂, 因为更长的自然语言提示携带更丰富的语义信息, 从而具有更复杂的声学特性.
我们同期的研究 PromptTTS [32] 提出了一个相似的思想, 使用句子作为风格提示来控制 TTS 系统中的风格信息. 他们定义五个不同风格因子 (性别, 音调, 语速, 音量和情感), 并假设提示中包含明显的风格因子词汇, 例如低语调, 高语速等, 意味着模型可以从局部级别描述获得风格信息. 与之不同, 我们的研究不对风格提示的形式进行限制, 允许用户使用任意自由形式的自然语言来描述说话风格, 这导致一个更具挑战性的机器学习问题.
此外, 我们专注于汉语 TTS 并构建了第一个适用于风格提示可控表达性 TTS 的汉语语音语料库.

### D. 扩散概率模型

Diffusion models have been demonstrated as a strong generative model for image generation [33]–[36], text generation[37], [38] and audio generation [23], [39]–[41]. Roughly speaking, Diffusion models can be divided into two types according to whether the latent spaces are continuous or discrete.
Nowadays, most diffusion models focus on continuous latent space, and the Latent Diffusion Model [42] is one of the representative works. Diffusion models with discrete state spaces are first investigated by [43]–[45]. These works try to define a structured categorical corruption process to corrupt the forward process, then train a model to learn the reverse process. Many works have successfully applied discrete diffusion models in image or sound generation, e.g., D3PMs [45]VQ-Diffusion [34], DiffSound [23]. However, no one attempts to apply the discrete diffusion model to synthesize expressive human speech. 
In the following, we briefly review background knowledge of diffusion models.

1) Vanilla Diffusion Model: Diffusion models define a Markov chain q(x1:T |x0) =QT t=1 q(xt|xt−1) that gradually destroys data x0 by adding noise over a fixed number of steps T, so that xT belongs to special noise distribution (e.g.
Gaussian distribution). The reverse process using a generative model that gradually denoises towards the original data distribution p(x0).
2) Discrete Diffusion Model: Discrete diffusion models constrain variable xt in the state space so that xt is a discrete random variable falling into one of K categories. For discrete diffusion, a transition probability matrix is defined to indicate how x0 transits to xt. The forward process can be represented as categorical distributions q(xt|xt−1) = Cat(xt; p =xt−1Qt) where Qt denotes the probabilities transition matrix.
The more details about discrete diffusion models, please refer to [23], [34], [45].

扩散模型已经被证明是最强大的生成模型, 可用于图像生成 [33], [34], [35], [36], 文本生成 [37], [38] 和音频生成 [23], [39], [40], [41]. 粗略地说, 扩散模型可以根据隐空间是连续的还是离散的分为两类.
目前大多数扩散模型集中在连续隐空间上, 其中 Latent Diffusion Model [42] 是代表性工作之一. 离散状态空间上的扩散模型最早由 [43], [44], [45] 提出. 这些工作尝试定义一个结构化的分类破坏过程以破坏正向过程, 然后训练模型学习逆向过程. 许多工作成功应用离散扩散模型于图像生成或声音生成, 如 D3PMs [45], VQ-Diffusion [34], DiffSound [23].
然而还没有人尝试将离散扩散模型应用于合成富有表现力的人类语音.

接下来, 我们简要回顾扩散模型的背景知识.

## 数据集

We use an internally collected Mandarin Chinese speech corpus named NLSpeech to conduct experimental evaluation since there is no openly available Mandarin Chinese speech corpus with rich style prompts. The corpus contains 44 hours of speech data (in total 32k utterances) from 7 speakers(5 female and 2 male). Audio waveform has a sampling rate of 24kHz. We randomly spare 0.1 hours of data as the validation set, another 0.1 hours of data as the test set and the remaining data as the training set. Each utterance has 5style prompts labeled by different annotators. To obtain highquality annotations, we ask annotators to follow three steps of annotation strategy:• Step-1: The annotators first use one word to describe the overall perceived emotion of an utterance;• Step-2: The annotators then listen to the utterance carefully and describe the emotion level of the utterance with one word;• Step-3: The annotators write a complete sentence in natural language to describe the style of the utterance.
Note that we ask annotators to not care about the speech content, which may influence the perception of emotion and style. Table I shows example style prompts in our dataset, and we also compare NLSpeech with other existing related corpora,including the FSNR0 corpus [31] and the PromptSpeech corpus [32]. We note that the style prompts in NLSpeech are in free-form natural language sentences which are more consistent with those used in our daily life, while those in the FSNR0 and the PromptSpeech corpora are in controlled format. Meanwhile, this also brings us a challenging TTS problem since natural language sentences allow for expressing virtually any concepts. Compact and informative representation of style prompt is therefore paramount to achieve effective style controlling during speech synthesis.

我们使用一个内部收集的中文普通话语音语料库，名为NLSpeech，进行实验评估，因为目前没有公开可用的中文普通话语音语料库带有丰富的样式提示。该语料库包含来自7名说话者（5名女性和2名男性）的44小时语音数据（总共32,000个话语）。音频波形的采样率为24kHz。我们随机选取0.1小时的数据作为验证集，另外0.1小时的数据作为测试集，其余数据作为训练集。每个话语都有5个样式提示，由不同的标注员标注。为了获得高质量的注释，我们要求标注员按照三个步骤的注释策略进行注释：• 第一步：标注员首先使用一个词来描述话语的整体感知情绪；• 第二步：标注员仔细聆听话语，用一个词描述话语的情绪水平；• 第三步：标注员用自然语言写一句完整的句子来描述话语的样式。

需要注意的是，我们要求标注员不关心话语的内容，这可能会影响情绪和样式的感知。表格I显示了我们数据集中的样式提示示例，并且我们还将NLSpeech与其他现有的相关语料库进行了比较，包括FSNR0语料库[31]和PromptSpeech语料库[32]。我们注意到NLSpeech中的样式提示是以自由形式的自然语言句子呈现的，更符合我们日常生活中使用的样式提示，而FSNR0和PromptSpeech语料库中的样式提示则以受控的格式呈现。同时，这也给我们带来了一个具有挑战性的TTS问题，因为自然语言句子可以表达几乎任何概念。因此，简洁而信息丰富的样式提示表示对于在语音合成过程中实现有效的样式控制至关重要。

## 方法论

The overall architecture of the proposed InstructTTS framework is demonstrated in Figure 1, which consists of five parts,including a content encoder, a style encoder, a speaker embedding module, a style-adaptive layer normalization (SALN)adaptor and a discrete diffusion decoder. The detailed design of each part will be introduced in this section.

A. Content Encoder 

The content encoder aims to extract content representation from the content prompts. We follow the architecture of FastSpeech2 [2], which consists of 4 feed-forward transformer.
The hidden size, number of attention heads, kernel size and filter size of the one-dimensional convolution in the FFT block are set as 256, 2, 9 and 1024, respectively. After that, a variance adaptor is used to predict information such as duration and pitch that is closely related to the style of synthetic speech.

B. Style Prompt Embedding Model 

To extract style representation from the style prompts,we adopt the RoBERTa model [46] as prompt embedding model. Assuming we have a style prompt sequence S =[S1, S2, ..., SM], where M denotes the sequence length. We add a [CLS] token to the start of the prompt sequence and then feed it into the prompt embedding model. After that, we take the representation of the [CLS] token as the style representation of this sentence. In order to stably control the style of the output of TTS through natural language description, the quality of prompt embedding is of great importance, which should satisfy two conditions: (1) the learned style prompt space must be able to contain important semantic information; (2) the distribution of prompt embedding space should be relatively uniform and smooth, and the model can be generalized to the style description not seen in the training. To realize this target,we propose a novel three-stage training-fine-tuning strategy.
The details are shown as follows.1) Training a base language model for Chinese: Given that most open-source pre-trained language models are trained on English data, we first train a RoBERTa model on Chinese data.2) Fine-tuning the pre-trained language model on labeled data: We use a small amount of Chinese natural language inference (NLI) to fine-tune the model parameters in a supervised way to achieve a better semantic representation of the model. Specifically, we follow the training strategy proposed in SimCSE [47], which using an InfoNCE loss [48] objective to fine-tune our pre-trained RoBERTa model.3) Cross-modal representation learning between style prompts and speech: We hope that the prompt embedding vector from the style prompt sentence and the style representation vector from the speech can be mapped to the shared semantic space so that we can control the style in the TTS output through the style description in the inference stage. Thus, we propose a cross-modal representation learning process based on metric learning, as Figure III shows. Specifically, we build an audio-text retrieval task based on the style-prompt and audio pair in our NLSpeech dataset. For any style prompt, we randomly choose N−1 negative audio samples, combined with one positive audio sample to build a training batch. Similarly,for one audio sample, we can also build a training batch that includes one positive style prompt and N − 1 negative style prompts. Inspired by previous audio-text retrieval works [14],[49], we try to use contrastive ranking loss [50] and InfoNCE loss [48] as the training objective. Experiments results show that InfoNCE loss brings better retrieval performance. The details will be introduced in Experiments part.

C. Style encoder 

The style encoder module, as shown in Fig. 1 (b), includes three parts: A pre-trained robust style prompt embedding model (i.e., the prompt encoder), an adaptor layer to map the style embedding extracted from the prompt encoder into a new latent space, an audio encoder that extracts style information from the reference mel-spectrogram. Note that our pre-trained robust prompt embedding model is fixed when we train the remaining parts in the TTS model. In the training stage, one of the training targets is to minimize the distance between style prompt embedding and audio embedding. We note that the audio encoder may encode speaker and content information.
To make sure the audio encoder only encodes style-related information, during training, we jointly minimize the stylespeaker mutual information (i.e., I(ze; zsid)) and style-content mutual information (i.e., I(ze; c)). Mutual information (MI) is a key measurement of correlation between random variables.
However, the MI of high-dimensional random variables with an unknown distribution is intractable to compute. Previous works focused on either estimating the MI lower bound or the MI upper bound. MINE [51] and InfoNCE [15] compute a lower bound as the MI estimators while CLUB [52] computes an upper bound as the MI estimator. In this work, we use the CLUB method to minimize style-speaker and style-content MI to avoid content and speaker information leakage from the mel-spectrogram during training.

D. Modelling Mel-spectrograms in Discrete Latent Space

In this part, we introduce our hypothesis: modelling melspectrograms in discrete latent space is a suitable way for expressive TTS. Then we introduce how to utilize VQ-VAE as an intermediate representation to help model the mel-spectrogram. Lastly, we introduce our proposed non-autoregressive mel-spectrogram token generation model,which is based on discrete diffusion models.
Most text-to-speech (TTS) methods [2], [40], [53] directly learn the mapping from text to mel-spectrogram in continuous space. Then they use a pre-trained vocoder to decode the predicted mel-spectrogram into waveform. However, frequency bins in a mel-spectrogram are highly correlated along both time and frequency axes in a complicated way, especially
when the speech sample conveys highly expressive emotions and speaking styles, leading to a challenging modeling problem.
Furthermore, the gap between the ground-truth melspectrogram and the predicted one also influences the synthesis performance [54]. In this study, we propose to model the mel-spectrogram in discrete latent space, but still use a HiFi-GAN vocoder [54] to recover the waveform from the melspectrogram.
Specifically, we first pre-train a VQ-VAE with a large-scale speech dataset so that the pre-trained Mel-VQVAE encodes all of the linguistic, pitch, energy, and emotion information into the latent codes. Then we regard the vector quantized latent codes as the predicting targets and hence model the mel-spectrogram in the discrete latent space. A similar idea modeling mel-spectrogram in discrete latent space is applied in VQ-TTS [55], which utilizes self-supervised VQ acoustic feature (vq-wav2vec [18]) rather than traditional mel-spectrogram as intermediate prediction target. VQ-TTS builds an autoregressive classification model for prosody labels and VQ acoustic features. Different from VQ-TTS, we still use the mel-spectrogram as intermediate acoustic feature and use a Mel-VQ-VAE model to transform the mel-spectrogram into a latent discrete space for reducing the time-frequency correlations. As Figure 3 shows, a mel-spectrogram can be represented by a sequence of mel-spectrogram tokens. Thus,the mel-spectrogram synthesis problem transfers to predicting a sequence of discrete tokens, which can be seen as a language modeling problem. In the following, we will introduce the details of Mel-VQ-VAE, and then we will introduce our proposed Mel-VQ-Diffusion decoder.
