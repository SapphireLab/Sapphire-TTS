# DiTTo-TTS

<details>
<summary>基本信息</summary>

- 标题: DiTTo-TTS: Efficient and Scalable Zero-Shot Text-to-Speech with Diffusion Transformer
- 作者:
  - 01 [Keon Lee](../../Authors/Keon_Lee.md)
  - 02 [Dong Won Kim](../../Authors/Dong_Won_Kim.md)
  - 03 [Jaehyeon Kim](../../Authors/Jaehyeon_Kim.md)
  - 04 [Jaewoong Cho](../../Authors/Jaewoong_Cho.md)
- 机构:
  - [KRAFTON.AI](../../Institutions/KRAFTON.AI.md)
- 时间:
  - 预印时间: 2024.06.17 ArXiv v1
  - 更新笔记: 2024.06.18
- 发表:
  - 期刊/会议 
- 链接:
  - [ArXiv](https://arxiv.org/abs/2406.11427)
  <!-- - [DOI]()
  - [Github]() -->
  - [Demo](https://ditto-tts.github.io/)
  - [Scholar](https://scholar.google.com/scholar?cluster=)
- 标签:
  - [语音合成](../../Tags/SpeechSynthesis.md)
- 页数: 21
- 引用: 80
- 被引: 0
- 数据:

</details>

## Abstract: 摘要

> Large-scale diffusion models have shown outstanding generative abilities across multiple modalities including images, videos, and audio.
> However, text-to-speech (TTS) systems typically involve domain-specific modeling factors (e.g., phonemes and phoneme-level durations) to ensure precise temporal alignments between text and speech, which hinders the efficiency and scalability of diffusion models for TTS.
> In this work, we present an efficient and scalable Diffusion Transformer (DiT) that utilizes off-the-shelf pre-trained text and speech encoders.
> Our approach addresses the challenge of text-speech alignment via cross-attention mechanisms with the prediction of the total length of speech representations.
> To achieve this, we enhance the DiT architecture to suit TTS and improve the alignment by incorporating semantic guidance into the latent space of speech.
> We scale the training dataset and the model size to 82K hours and 790M parameters, respectively.
> Our extensive experiments demonstrate that the large-scale diffusion model for TTS without domain-specific modeling not only simplifies the training pipeline but also yields superior or comparable zero-shot performance to state-of-the-art TTS models in terms of naturalness, intelligibility, and speaker similarity.
> Our speech samples are available at https://ditto-tts.github.io.

## 1.Introduction: 引言

> Large-scale diffusion models have demonstrated impressive generative abilities in a wide range of fields including images [1, 2], videos [3, 4], and audio [5, 6].
> The integration of latent diffusion models (LDMs) [7] further amplifies their popularity [8, 9, 10], as these models significantly enhance computational efficiency.
> This efficiency is achieved through the reduction of input dimensionality using autoencoders, which also enables the diffusion models to focus on the most critical features of data [7].
> However, applying LDMs to text-to-speech (TTS) presents unique challenges because TTS requires precise alignment between text and generated speech over time.
> Hence, their application to TTS often requires a complex pipeline, incorporating speech domain-specific modeling such as phoneme and duration [11, 12].
> Without these components, generation performance tends to be suboptimal [13, 14], while their inclusion hinders the model efficiency and scalability.

> In this work, we present a latent diffusion model for TTS that is integrated with off-the-shelf pre-trained text and speech encoders without relying on the speech domain-specific modeling.
> Our method addresses the challenge of aligning text and speech solely through cross-attention mechanisms.
> Furthermore, we introduce a module that predicts the total duration of the generated speech from a given text and speech prompt, rather than determining the duration of each individual input token.
> To accomplish this, we conduct a network architecture ablation to identify a model specifically suited for TTS applications.
> Consequently, we adopt the Diffusion Transformer (DiT) [10] to TTS, naming our method DiTTo-TTS (or simply DiTTo).
> We also explore the significance of leveraging aligned text and speech representations, showing that performance can be enhanced by either using a text encoder jointly trained with speech data or a speech autoencoder with an auxiliary language modeling objective alongside the reconstruction objective.

> Our comprehensive experiments on English-only and multilingual evaluation demonstrate that our model not only simplifies the training process but also achieves superior or comparable zero-shot performance to state-of-the-art models in terms of naturalness, intelligibility, and speaker similarity.
> The base-sized DiTTo surpasses a state-of-the-art autoregressive model [15], offering an inference speed 4.6 times faster and a model size 3.84 times smaller.
> Additionally, we demonstrate that our model scales effectively with increases in both data and model sizes.

## 2.Related Works: 相关工作

> ### Large-scale TTS 
> Recently, large-scale TTS research progresses actively in two main directions: LLM-based autoregressive (AR) TTS and non-autoregressive (Non-AR) TTS.
> A prominent feature of LLMs is the scalability [16, 17] and their proficiency in zero-shot learning tasks, demonstrating significant capabilities without prior specific training on those tasks [18, 19, 20, 21].
> Efforts to replicate LLM’s capability in different modalities have shown progress, including vision [22, 23, 24] and audio [25, 26, 27, 28].
> VALL-E [25] employs EnCodec [29] for speech-to-token mapping, posing TTS tasks as AR language modeling tasks, thus enabling zero-shot capabilities in the speech domain.
> CLaM-TTS [15] introduces Mel-VAE to achieve superior token length compression, and enables a language model to predict multiple tokens simultaneously.
> Although this approach removes the need for cascaded modeling [25] to manage the number of token streams, their resource-intensive inference processes limit their applications [30].
> On the other hand, Non-AR generative models are employed to enhance the efficiency of TTS systems.
> Voicebox [31] utilizes a flow matching [32] to generate speech, effectively casting the TTS task into a speech infilling task.
> NaturalSpeech series [12, 33], building upon recent advances in the Latent Diffusion Model (LDM) [7], incorporate auxiliary modules for controllability of various speech attribute such as content, prosody, and timbre.
> However, requiring supplementary data beyond speech-transcription pairs hinders scalability.
> Simple-TTS [13] simplifies the data preparation and training process by removing auxiliary modules and the need for phoneme-level durations.
> Similarly, E3-TTS [14] follows this approach but does not utilize a pre-trained latent autoencoder.
> However, both models have limitations in audio quality and impose a fixed length on the target audio, which is a significant constraint for speech generation.

> ### Latent Diffusion Model (LDM) 
> LDM [7] improves modeling efficiency of the diffusion process model [1, 34] by operating in a latent space, achieving remarkable performance in generating realistic samples.
> Initially applied in image generation, their success is attributed to the reduced dimensionality of the latent space, facilitating efficient training and sampling [7].
> Notably, guided diffusion [35, 36] has been expanded to various applications of LDMs, such as image editing [37] and image retrieval [38].
> In the field of audio signals, techniques such as style transfer, inpainting, and super-resolution have been explored, along with text-guided audio and speech generation [39, 40].
> In the context of TTS, however, applying LDMs to TTS [12, 33] necessitates domain-specific elements such as phonemes, phoneme-level durations, and pitch.
> This is primarily due to the need for precise temporal alignment between text and speech, as well as the higher fidelity requirements inherent in audio data.

> ### Neural Audio Codec 
> Neural audio codecs, which effectively compress various types of audio using neural networks, are used as part of many TTS systems [12, 15, 25].
> Recent advancements employ an encoder-decoder architecture coupled with Residual Vector Quantization (RVQ) [41, 42, 43] to transform raw audio waves into discretized tokens.
> For example, EnCodec [29] converts 24,000 Hz mono waveforms into 75 Hz latents.
> With a similar architecture, by focusing the compression specifically on speech rather than general audio signals, Mel-VAE [15] achieves approximately 10.76 Hz latents by compressing the mel-spectrogram.
> This reduction significantly lowers the computational cost of the speech generation module.
> Another research direction of improving neural audio codecs for TTS systems is injecting semantic information using large language models (LLMs) [44].

## 3.Methodology: 方法

> We present a latent diffusion model (LDM) for TTS that enables the use of off-the-shelf pre-trained text and speech encoders without relying on speech domain-specific modeling, such as phoneme and duration.
> Toward this goal, we employ the following two approaches: 
> (1) introducing a speech length predictor that predicts the total length of the generated speech without relying on phoneme-level durations or requiring a fixed speech length; and 
> (2) fine-tuning the pre-trained neural audio codec using a pre-trained language model to enhance the alignment between text and speech embeddings.

### 3.1.Preliminary

> Diffusion models [1, 34] are a class of generative models that iteratively transform a simple noise distribution into a complex data distribution through a stochastic denoising process.
> They define a forward process that progressively adds Gaussian noise to the input data as time step increases.
> The reverse generative process then estimates the added noise to reconstruct the original data.
> Conditional diffusion models enhance this framework by incorporating additional information, such as text descriptions in text-to-image generation [35, 45] or phonemes and their durations in TTS [11, 46].
> While diffusion models can operate directly on real-world data, many of them are applied in the latent space [7, 8, 10, 47].
> Thanks to the reduced dimensionality, this approach improves computational efficiency and output quality by allowing diffusion models to focus on the semantic information of the data while the autoencoder handles the high-frequency details that are less perceptible [7].

> In our setting, a conditional LDM can be formulated as follows.
> Given speech audio, an autoencoder produces its latent representation zspeech, and the diffusion model is trained to predict zspeech at each diffusion step t ∈ [1, T ] conditioned on a text token sequence x.
> Specifically, the noised latent z(t) is expressed as αtzspeech + σtϵ, where ϵ is sampled from the standard normal distribution and αt and σt are defined by a noise schedule.
> Note that z(1) is zspeech and z(T ) follows the standard normal distribution.
> We use v-prediction [48] as our model output vθ(z(t), x, t), which predicts v(t) := αtϵ − σtz(t).
> This setup provides a mean squared error objective as the training loss:

$$
$$

> To enrich the contextual information and facilitate zero-shot audio prompting, we incorporate a random span masking into the model training following [31, 49].
> We input

### 3.2.Model & Training

> An overview of our proposed method is presented in Figure 1.

> #### Text Encoder
> We employ a text encoder from a pre-trained large language model pϕ, which is parameterized by ϕ.
> The model was pre-trained to maximize the log-likelihood of the text token sequence log pϕ(x).
> The parameters of the model are kept frozen while training the diffusion model for TTS.
> We denote the output of the text encoder by ztext.

> #### Neural Audio Codec
> A neural audio codec, which is parameterized by ψ, comprises of three components: 1) an encoder that maps a speech into a sequence of latent representations zspeech; 2) a vector quantizer converting the latent vector into the discrete code representation; and 3) a decoder that reconstructs the speech from a sequence of the quantized latent representations ˆzspeech.
> To enhance alignment between text and speech embeddings, we fine-tune the neural audio codec using the pre-trained language model.
> We introduce a learnable linear projection f (·) to match the dimension of the latent representation zspeech to the language model’s hidden space.
> Subsequently, we use this projected embedding in place of the embedding from the pre-trained text encoder within the cross-attention operation of the language model’s decoder.
> The neural audio codec is fine-tuned with auxiliary loss that infuse semantic content into the generated representations:
>
> where LN AC(ψ) indicate the loss function which is used when the neural audio codec is pre-trained [15]. λ controls the contribution of LLM , with λ = 0 indicating the pre-training phase.
> When λ > 0, a pre-trained language decoder performs causal language modeling on text token sequences x, based on the speech latent vector zspeech.
> While the parameters of language model decoder are fixed, gradients are backpropagated to adjust the linear mappings f (zspeech).
> This training strategy aligns the speech latents with the linguistic latents of the pretrained language model during autoencoding.
>
> #### Diffusion Model
> We are given text embedding ztext and speech embedding zspeech.
> We train the diffusion model vθ(·) using the objective in Eq. (1), replacing x with ztext:
> where z(t) mask = m ⊙ z(t) + (1 − m) ⊙ zspeech is the masked input latent, m is the binary span masking, and t is the diffusion time step.
> We apply classifier-free guidance (CFG) [36] and adopt the diffusion noise schedule from [13].

> #### Speech Length Predictor
> We introduce a model designed to predict the total length of a generated speech for a given text rather than to estimate each phoneme’s duration, and the input noise of diffusion model is set by the length from the speech length predictor at inference time.
> As shown in Figure 1, we employ an encoder-decoder transformer for the speech length predictor.
> The encoder processes text input bidirectionally to capture comprehensive textual context, while the decoder, equipped with causal masking to prevent future lookahead, receives an audio token sequence from the encoder of the neural codec for speech prompting at inference time.
> We use cross-attention mechanisms to integrate text features from the encoder.
> We use the softmax activation in the final layer to predict the number of tokens to be generated within the given maximum length N .
> Specifically, the ground truth label for the remaining audio length decreases by one at each subsequent time step.
> The model is trained separate from the diffusion model, using the cross-entropy loss function.


### 3.3.Model Architecture

> We conduct a comprehensive model architecture search to identify the most suitable diffusion-based model for TTS, resulting in the adoption of the Diffusion Transformer (DiT) [10] model (see Section 6.1).
> We adopt the DiT model in TTS while incorporating recent architectural advancements
> for transformer variants, such as the gated linear unit with GELU activation [50], rotary position embeddings [51], and cross-attention with global adaptive layer normalization (AdaLN) [52].
> For the latent space, we employ Mel-VAE introduced in [15] which is able to compress audio sequences approximately seven times more than EnCodec [29], yet maintaining superior quality.
> Due to space limitations, additional details regarding model configurations are provided in Appendix 8.4.
> We also detail down our noise scheduler and CFG in Appendix 8.6.

---

<details>
<summary>实验占位</summary>

## 4.Experiments: 实验

### Dataset

### Training

### Inference

### Metrics

### Baselines

### Tasks

## 5.Results: 结果

### 5.1.Comparison with Baselines

### 5.2.Scaling Model Size

### 5.3.Aligned Text-Speech Embeddings Improve Performances

## 6.Ablation Study: 消融研究

</details>

---

## 7.Conclusions: 结论

> We presented DiTTo-TTS, a latent diffusion model for text-to-speech (TTS) that leverages cross-attention and the prediction of the total length of latent speech representations to achieve text-speech alignment.
> We demonstrated that the proposed method shows exceptional zero-shot performance in naturalness, intelligibility, and speaker similarity, all without relying on domain-specific elements such as phonemes and durations, while simplifying the training process as well.
> In the process of obtaining this result, we also found that fine-tuning a speech autoencoder with an auxiliary language modeling objective can significantly enhance the text-speech alignment.
> Moreover, DiTTo-TTS shows effective scalability with respect to data and model sizes.
> Our future work includes: 
> (1) exploring various noise schedules to improve output quality and distillation methods to reduce inference times; 
> (2) enhancing pronunciation accuracy by improving character input normalization; 
> (3) enabling DiTTo to understand and learn from natural language instructions.