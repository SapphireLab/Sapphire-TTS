# Flow Matching for Generative Modeling

## 预备知识: 连续标准化流

设 $\mathbb{R}^d$ 为数据空间, 数据点 $x=(x^1,\cdots,x^d)\in\mathbb{R}^d$.

通篇需要两个重要的概念:
1. **概率密度路径 (Probability Density Path)** $p=p_t(x): [0,1]\times \mathbb{R}^d\to \mathbb{R}_{>0}$, 是依赖时间的概率密度函数, 即 $\int p_t(x)\text{d}x = 1$;
2. 依赖时间的矢量场 (Time-Dependent Vector Field) $v=v_t: [0,1]\times \mathbb{R}^d\to \mathbb{R}^d$. 

矢量场 $v_t$ 可以用于构造依赖时间的**微分同胚映射 (Diffeomprphic Map)**, 称为**流 (Flow)** $\phi: [0,1]\times \mathbb{R}^d\to \mathbb{R}^d$, 通过常微分方程 ODE 定义如下:

$$
    \dfrac{\text{d}}{\text{d} t}\phi_t(x) = v_t(\phi_t(x)),\quad \phi_0(x)=x.
$$

在 NeurIPS 2018 发表的论文 "Neural Ordinary Differential Equations" 中, 使用了神经网络来建模矢量场 $v_t(x;\theta)\approx v_t$, 其中 $\theta\in \mathbb{R}^p$ 是可学习的参数, 这反而得到了一种流 $\phi_t$ 的深度参数模型, 称为**连续标准化流 (Continuous Normalizing Flow, CNF)**. **CNF** 用于将一个简单的先验分布密度 $p_0$ (如纯噪声) 通过**前推方程 (Push-Forward Equation)** 重塑为一个更复杂的分布密度 $p_1$.

$$
    p_t = [\phi_t]_{*}p_0
$$

其中前推或变量替换算子 $*$ 定义为

$$
    [\phi_t]_{*}p_0(x) = p_0(\phi^{-1}_t(x))\det \left[ \dfrac{\partial \phi_t^{-1}}{\partial x}(x)\right].
$$

当流 $\phi_t$ 满足**前推方程**时, 则称一个矢量场 $v_t$ 生成一个概率密度路径 $p_t$.

一个测试矢量场是否生成概率路径的实用方法是使用**连续性方程 (Continuity Equation)**, 这也是证明过程中的一个关键部分.

**连续性方程**是一个偏微分方程, 是矢量场 $v_t$ 生成概率路径 $p_t$ 的充分必要条件,

$$
    \dfrac{\text{d}}{\text{d} t}p_t(x)+\text{div}(p_t(x)v_t(x)) = 0.
$$

此外, 对 CNF 的一些信息翻新, 特别是如何计算在任一点 $x\in\mathbb{R}^d$ 处的概率 $p_1(x)$.

## Flow Matching

随机变量 $x_1$ 服从某些未知数据分布 $q(x_1)$. 我们只能使用从 $q(x_1)$ 中采样得到的数据样本, 而不能直接使用密度函数本身.
我们设 $p_t$ 为概率路径, 其初值 $p_0=p$ 是一个简单分布, 例如标准正态分布 $p(x)=\mathcal{N}(x|0,I)$, 而 $p_1$ 近似等价于分布 $q$.
后续将讨论如何构造这样的路径.

Flow Matching 的目标是匹配这样的目标概率路径, 使得我们能够从 $p_0$ 流到 $p_1$.

给定一个目标概率密度路径 $p_t(x)$ 和生成该路径 $p_t(x)$ 的相应矢量场 $u_t(x)$.
我们定义 Flow Matching 目标函数为:

$$
    \mathcal{L}_{FM}(\theta) = \mathbb{E}_{t,p_t(x)}\| v_t(x)-u_t(x) \|^2.
$$

其中 $\theta$ 是 CNF 矢量场 $v_t$ 的可学习参数, $t\sim \mathcal{U}[0,1]$, $x\sim p_t(x)$.

该损失函数使用神经网络 $v_t$ 对矢量场 $u_t$ 进行回归. 当损失值达到零, 学习到的 CNF 模型将能够生成 $p_t(x)$.

虽然 Flow Matching 是一种简单且有吸引力的目标函数, 但是实际上难以处理. 这是因为我们缺乏 $p_t$ 和 $u_t$ 的先验知识, 且可能存在很多种可能的概率路径满足 $p_1(x)\approx q(x)$. 更重要的是, 我们通常无法获得能够生成我们想要的 $p_t$ 的 $u_t$ 的具体形式.

解决方案是我们可以使用仅在每个样本上定义的概率路径和矢量场, 再结合合适的聚合方法来构造我们想要的 $p_t$ 和 $u_t$.
除此之外, 这一构造允许我们构建出更好处理的 Flow Matching 目标函数.

### 条件矢量场与边际矢量场

构造目标概率路径的一种简单方式是通过更简单的概率路径的混合:

给定一个特定点 $x_1$, 我们定义**条件概率路径 (Conditional Probability Path)** $p_t(x|x_1)$ 使得 $t=0$ 时 $p_0(x|x_1)=p(x)$, 在 $t=1$ 时 $p_1(x|x_1)$ 为以 $x=x_1$ 为中心的分布, 如正态分布 $p_1(x|x_1)=\mathcal{N}(x|x_1,\sigma^2I)$.

在 $q(x_1)$ 上对条件概率路径边际化得到**边际概率路径 (Marginal Probability Path)** $p_t(x)$, 即

$$
    p_t(x) = \int p_t(x|x_1)q(x_1)\text{d}x_1.
$$

当 $t=1$ 时, 边际分布 $p_1$ 是个近似数据分布 $q$ 的混合分布,

$$
    p_1(x) = \int p_1(x|x_1)q(x_1)\text{d}x_1 \approx q(x)
$$

类似地, 可以定义**边际矢量场 (Marginal Vector Field)**, 通过在如下意义下对条件矢量场进行边际化:

$$
    u_t(x) = \int u_t(x|x_1) \dfrac{p_t(x|x_1)q(x_1)}{p_t(x)}\text{d}x_1.
$$

其中 $u_t(\cdot|x_1): \mathbb{R}^d\to \mathbb{R}^d$ 是条件矢量场, 能生成条件概率路径 $p_t(\cdot|x_1)$.
这可能不明显, 但这种聚合条件矢量场的方式切实地生成了正确的矢量场, 用于建模边际概率路径.

一个关键的观察是: **边际矢量场生成边际概率路径**.

这一观察表明了条件矢量场 (可生成条件概率路径) 和边际矢量场 (可生成边际概率路径) 之间存在的联系.
这一连续允许我们将未知且难以处理的边际矢量场拆分为更简单的条件矢量场, 这些条件矢量场仅依赖于单个数据样本进行定义.

下面给出更正式的定理:

**定理 1.** 给定能生成条件概率路径 $p_t(x|x_1)$ 的矢量场 $u_t(x|x_1)$, 对于任意分布 $q(x_1)$, 边际矢量场 $u_t$ 可以生成边际概率路径 $p_t$, 即 $u_t$ 和 $p_t$ 满足连续性方程.

这一定理还可以由 Peluchetti (2021) 中的**扩散混合表示定理 (Diffusion Mixture Representation Theorem)** 得到, 提供了在扩散随机微分方程中的边际漂移系数和扩散系数.

**证明**: #TODO

### 条件流匹配