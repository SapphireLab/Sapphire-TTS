# BigVGAN: A Universal Neural Vocoder with Large-Scale Training

<details>
<summary>基本信息</summary>

- 标题:
- 作者:
  - 01 [Sang-gil Lee](../../Authors/Sang-gil_Lee.md)
  - 02 [Wei Ping](../../Authors/Wei_Ping.md)
  - 03 [Boris Ginsburg](../../Authors/Boris_Ginsburg.md)
  - 04 [Bryan Catanzaro](../../Authors/Bryan_Catanzaro.md)
  - 05 [Sungroh Yoon](../../Authors/Sungroh_Yoon.md)
- 机构:
  - [Nvidia](../../Institutions/Nvidia.md)
- 时间:
  - 预印时间: 2022.06.09 ArXiv v1
  - 预印时间: 2023.02.16 ArXiv v2
  - 更新笔记: 2024.08.02
- 发表:
  - 期刊/会议 
- 链接:
  - [ArXiv](https://arxiv.org/abs/2206.04658)
  - [DOI]()
  - [Github]()
  - [Demo](https://bigvgan-demo.github.io/)
  - [Scholar](https://scholar.google.com/scholar?cluster=)
- 标签:
  - ?
- 页数: ?
- 引用: ?
- 被引: ?
- 数据:
  - ? 
- 对比:
  - ?
- 复现:
  - ?

</details>

## Abstract: 摘要

> Despite recent progress in generative adversarial network~(GAN)-based vocoders, where the model generates raw waveform conditioned on acoustic features, it is challenging to synthesize high-fidelity audio for numerous speakers across various recording environments. 
> In this work, we present ***BigVGAN***, a universal vocoder that generalizes well for various out-of-distribution scenarios without fine-tuning.
> We introduce periodic activation function and anti-aliased representation into the GAN generator, which brings the desired inductive bias for audio synthesis and significantly improves audio quality.
> In addition, we train our GAN vocoder at the largest scale up to 112M parameters, which is unprecedented in the literature.
> We identify and address the failure modes in large-scale GAN training for audio, while maintaining high-fidelity output without over-regularization. 
> Our ***BigVGAN***, trained only on clean speech~(LibriTTS), achieves the state-of-the-art performance for various zero-shot~(out-of-distribution) conditions, including unseen speakers, languages, recording environments, singing voices, music, and instrumental audio.~\footnote{Listen to audio samples from ***BigVGAN*** at: {\url{https://bigvgan-demo.github.io/}}. }
> We release our code and model at: {\small \url{https://github.com/NVIDIA/BigVGAN}}.

## 1.Introduction: 引言

> Deep generative models have demonstrated noticeable successes for modeling raw audio.
> The successful methods include, autoregressive models~\citep{oord2016wavenet, mehri2016samplernn, kalchbrenner2018efficient}, flow-based models~\citep{oord2017parallel,ping2018clarinet, prenger2019waveglow, kim2018flowavenet, ping2019waveflow, lee2020nanoflow}, GAN-based models~\citep{donahue2018adversarial, kumar2019melgan,binkowski2020high, yamamoto2020parallel, kong2020hifi}, and diffusion models~\citep{kong2020diffwave, chen2020wavegrad, lee2022priorgrad}.
>
> Among these methods, GAN-based vocoders~\citep[e.g.,][]{kong2020hifi} can generate high-fidelity raw audio conditioned on mel spectrogram, while synthesizing hundreds of times faster than real-time on a single GPU. 
> However, existing GAN vocoders are confined to the settings with a moderate number of voices recorded in clean environment due to the limited model capacity.
> The audio quality can heavily degrade when the models are conditioned on mel spectrogram from unseen speakers in different recording environments. 
> In practice, a \emph{universal vocoder}, that can do zero-shot generation for out-of-distribution samples, is very valuable in many real-world applications, including text-to-speech with numerous speakers~\citep{ping2017deep}, neural voice cloning~\citep{arik2018neural, jia2018transfer}, voice conversion~\citep{liu2018wavenet}, speech-to-speech translation~\citep{jia2019direct}, and neural audio codec~\citep{zeghidour2021soundstream}.
> In these applications, the neural vocoder also needs to generalize well for audio recorded at various conditions.
>
> Scaling up the model size for zero-shot performance is a noticeable trend in text generation~\citep[e.g.,][]{brown2020language} and image synthesis~\citep[e.g.,][]{ramesh2021zero}, but has not been explored in audio synthesis.
> Although likelihood-based models are found to be easier for scaling among others because of their simple training objective and stable optimization, we build our universal vocoder with large-scale GAN training, because GAN vocoder has the following advantages:
> 1. In contrast to autoregressive or diffusion models, it is fully parallel and requires only one forward pass to generate high-dimensional waveform.
> 2. In contrast to flow-based models~\citep{prenger2019waveglow}, it does not enforce any architectural constraints~(e.g., affine coupling layer) that maintain the bijection between latent and data. 
> Such architectural constraints can limit model capacity given the same number of parameters~\citep{ping2019waveflow}.
>
> In this work, we present ***BigVGAN***, a **Big** **V**ocoding **GAN** that enables high-fidelity out-of-distribution~(OOD) generation without fine-tuning. 
> Specifically, we make the following contributions:
> - We introduce periodic activations into the generator, which provide the desired inductive bias for audio synthesis. 
> Inspired by the methods proposed for other domains~\citep{liu2020neural, sitzmann2020implicit}, we demonstrate the noticeable success of periodic activations in audio synthesis.
> - We propose anti-aliased multi-periodicity composition~(AMP) module for modeling complex audio waveform.
> AMP composes multiple signal components with learnable periodicities and uses low-pass filter to reduce the high-frequency artifacts.
> - We successfully scale ***BigVGAN*** up to 112M parameters by fixing the failure modes of large-scale GAN training without regularizing both generator and discriminator.
> The empirical insights are different from \citet{brock2018large} in image domain.
> For example, regularization methods~\citep[e.g.,][]{miyato2018spectral} introduce phase mismatch artifacts in audio synthesis. 
> - We demonstrate that ***BigVGAN***-base with 14M parameters outperforms the state-of-the-art neural vocoders with comparable size for both in-distribution and out-of-distribution samples.
> In particular, ***BigVGAN*** with 112M parameters outperforms the state-of-the-art models by a large margin for zero-shot generation at various OOD scenarios, including unseen speakers, novel languages, singing voices, music and instrumental audio in varied unseen recording environments. % It synthesizes 24 kHz high-fidelity speech 44.72~$\times$ faster than real-time on a V100 GPU.
>
> We organize the rest of the paper as follows.
> We discuss related work in \S~\ref{sec:related_work} and present ***BigVGAN*** in \S~\ref{sec:method}.
> We report empirical results in \S~\ref{sec:results} and conclude the paper in \S~\ref{sec:conclusion}.

## 2.Related Works: 相关工作

> Our work builds upon the state-of-the-art of GANs for image and audio synthesis. 
>
> GAN was first proposed for image synthesis~\citep{goodfellow2014generative}.
> Since then, impressive results have been obtained through optimized architectures~\citep[e.g.,][]{radford2015unsupervised, karras2021alias} or large scale training~\citep[e.g.,][]{brock2018large}.
>
> In audio synthesis, previous works focus on improving the discriminator architectures or adding new auxiliary training losses.
> MelGAN~\citep{kumar2019melgan} introduces the multi-scale discriminator~(MSD) that uses average pooling to downsample the raw waveform at multiple scales and applies window-based discriminators at each scale separately.
> It also enforces the mapping between input mel spectrogram and generated waveform via an $\ell_1$ feature matching loss from discriminator.
> In contrast, GAN-TTS~\citep{binkowski2020high} uses an ensemble of discriminators which operate on random windows of different sizes, and enforces the mapping between the conditioner and waveform adversarially using conditional discriminators.
> Parallel WaveGAN~\citep{yamamoto2020parallel} extends the single short-time Fourier transform (STFT) loss~\citep{ping2018clarinet} to multi-resolution, and adds it as an auxiliary loss for GAN training.
> \cite{yang2021multi} and \cite{mustafa2021stylemelgan} further improve MelGAN by incorporating the multi-resolution STFT loss.
>
> HiFi-GAN~\citep{kong2020hifi} reuses the MSD from MelGAN, and introduces the multi-period discriminator~(MPD) for high-fidelity synthesis. 
> UnivNet~\citep{jang2020universal, jang2021univnet} uses the multi-resolution discriminator~(MRD) that takes the multi-resolution spectrograms as the input and can sharpen the spectral structure of synthesized waveform.
> In contrast, CARGAN~\citep{morrison2021chunked} incorporates the partial autoregression~\citep{ping2019waveflow} into generator to improve the pitch and periodicity accuracy.
>
> In this work, we focus on improving and scaling up the generator.
> We introduce the periodic inductive bias for audio synthesis and address the feature aliasing issues within the non-autoregressive generator architecture.
> Our architectural design has a connection with the latest results in time-series prediction~\citep{liu2020neural}, {implicit neural representations~\citep{sitzmann2020implicit}}, and image synthesis~\citep{karras2021alias}.
> Note that, \citet{you2021gan} argues that different generator architectures can perform equally well for single-speaker neural vocoding.
> We demonstrate that improving generator architecture is crucial for universal neural vocoding in challenging conditions.
>
> There are limited successes for universal neural vocoding due to the noticeable challenges.
> In previous work, WaveRNN has been applied for universal vocoding task~\citep{lorenzo2018towards, paul2020speaker}. \citet{jiao2021universal} builds the universal vocoder with flow-based model.
> GAN vocoder is found to be a good candidate recently~\citep{jang2021univnet}.

## 3.Methodology: 方法

> In this section, we introduce the preliminaries for GAN vocoder, then present the ***BigVGAN***.
> See Figure~\ref{fig_model_diagram} for an illustration and refer to the Appendix \ref{appendix:arch_detail} for a detailed description of the architecture.

## Preliminaries of GAN Vocoder

> The audio waveform is known to exhibit high periodicity and can be naturally represented as the composition of primitive periodic components {(i.e., Fourier series under Dirichlet conditions)}.
> This suggests that we need to provide the desired inductive bias to the generator architecture. 
> However, the current non-autoregressive GAN vocoders~\citep[e.g.,][]{kong2020hifi} solely rely on layers of dilated convolutions to learn the required periodic components at different frequencies.
> Their activation functions~(e.g., {Leaky ReLU}) can produce new details with necessary nonlinearities, but do not provide any periodic inductive bias.
> Furthermore, we identified that Leaky ReLU behaves poorly for \textit{extrapolation} in waveform domain: although the model can generate high-quality speech signal in a seen recording environment at training, the performance degrades significantly for out-of-distribution scenarios such as unseen recording environments, non-speech vocalizations, and instrumental audio.
>
> We introduce a proper inductive bias of periodicity to the generator by applying a recently proposed periodic activation called Snake function~\citep{liu2020neural}, defined as $f_{\alpha}(x) = x + \frac{1}{\alpha} \sin^2(\alpha x)$, where $\alpha$ is a trainable parameter that controls the frequency of the periodic component of the signal and larger $\alpha$ gives higher frequency.
> The use of $\sin^2(x)$ ensures monotonicity and renders it amenable to easy optimization. 
> \citet{liu2020neural} demonstrates this periodic activation exhibits an improved extrapolation capability for temperature and financial data prediction.
>
> In ***BigVGAN***, we use Snake activations $f_{\vv\alpha}(x)$ with channel-wise trainable parameters $\vv\alpha\in\mathbb{R}^h$ that define the periodic frequencies for each 1-D convolution channels.
> Taking this periodic functional form with learned frequency control, the convolutional module can naturally fit the raw waveform with multi-periodic components.
> We demonstrate that the proposed Snake-based generator is more robust for out-of-distribution audio samples unseen during training, indicating strong extrapolation capabilities in universal vocoding task.
> See Figure~\ref{fig_visualization_main_text} and Appendix~\ref{appendix:visualization} for illustrative examples; ***BigVGAN***-base w/o filter using snake activations is closer to ground-truth sample than HiFi-GAN.

### Anti-aliased Representation

> The Snake activations provide the required periodic inductive bias for modeling raw waveform, but it can produce arbitrary high frequency details for continuous-time signals that can not be represented by the discrete-time output of the network,~\footnote{One can think of the neural vocoder as a discrete-time function on the sampled continuous-time signals.} which can lead to aliasing artifacts.
> This side effect can be suppressed by applying a low-pass filter~\citep[e.g.,][]{karras2021alias}.
> The anti-aliased nonlinearity operates by upsampling the signal $2\times$ along time dimension, applying the Snake activation, then downsampling the signal by $2\times$, which is a common practice inspired by the Nyquist-Shannon sampling theorem~\citep{shannon1949communication}.
> Each upsampling and downsampling operation is accompanied by the low-pass filter using a windowed \texttt{sinc} filter with a Kaiser window \citep{dsp}.
> Refer to the Appendix \ref{appendix:arch_detail} for details.
>
> We apply this filtered Snake nonlinearity in every residual dilated convolution layers within the generator to obtain the anti-aliased representation of the discrete-time 1-D signals.
> The module is named as \emph{anti-aliased multi-periodicity composition}~(AMP).
> See Figure~\ref{fig_model_diagram} for an illustration. 
> We find that incorporating the filtered activation can reduce the high-frequency artifacts in the synthesized waveform; see ***BigVGAN***-base w/o filter vs.
> ***BigVGAN***-base~(with filter) in Figure~\ref{fig_visualization_main_text} as an illustration.
> We will demonstrate that it provides significant improvements in various objective and subjective evaluations.
>
> Note that we also explored anti-aliased upsampling layers, but this results in significant training instabilities and lead to early collapse for large models.
> See Appendix~\ref{appendix:practical} for more details.

### BigVGAN with Large Scale Training

> In this subsection, we explore the limits of universal vocoding by scaling up the generator's model size to 112M parameters while maintaining the stability of GAN training and practical usability as a high-speed neural vocoder.
> We start with our improved generator using the comparable HiFi-GAN V1 configuration with 14M parameters~\citep{kong2020hifi}, which is denoted as ***BigVGAN***-base.
>
> We grow ***BigVGAN***-base by increasing the number of upsampling blocks and convolution channels for each block.
> The ***BigVGAN***-base upsamples the signal by $256\times$ using 4 upsampling blocks with the ratio of $[8, 8, 2, 2]$.
> Each upsampling block is accompanied by multiple residual layers with dilated convolutions, i.e., the AMP module. 
> We further divides the $256\times$ upsampling into 6 blocks $[4, 4, 2, 2, 2, 2]$ for more fine-grained feature refinement. 
> In addition, we increase the number of channels of AMP module (analogous to MRF in HiFi-GAN) from $512$ to $1536$.
> We denote the model with $1536$ channels and 112M parameters as ***BigVGAN***.
>
> We found that the default learning rate of $2\times10^{-4}$ used in HiFi-GAN causes an early training collapse for ***BigVGAN*** training, where the losses from the discriminator submodules immediately converge to zero after several thousands of iterations.
> Halving the learning rate to $1\times10^{-4}$ was able to reduce such failures. 
> We also found that large batch size is helpful to reduce mode collapse at training~\citep{brock2018large}.
> We only double the batch size from the usual 16 to 32 for a good trade-off between training efficiency and stability, as neural vocoders can require millions of steps to converge.
> Note that this recommended batch size is still much smaller than the one for image synthesis~(e.g., 2048)~\citep{brock2018large}, because neural vocoding has strong conditional information.
>
> Even with the aforementioned changes, the large ***BigVGAN*** can still be prone to collapse early in training.
> We track the gradient norm of each modules during training and identify that the anti-aliased nonlinearity significantly amplified the gradient norm of MPD.
> Consequently, ***BigVGAN*** generator receives a diverging gradient early in training, leading to instabilities and potential collapse.
> We visualize the norm of gradient for each modules in Figure~\ref{fig_grad_norm} at Appendix~\ref{appendix:practical}.
> We alleviate the issue by clipping the global norm of the gradient to $10^{3}$, which is close to the average gradient norm of the 112M ***BigVGAN*** generator.
> This gradient clipping prevents the early training collapse of the generator.
> Note that, gradient clipping was found ineffective to alleviate training instability for image synthesis~(see Appendix H in \citet{brock2018large}), but it is very effective in our endeavors.
>
> In addition to above efforts, we have explored other directions, including various ways to improve the model architecture, spectral normalization~\citep{miyato2018spectral} to stabilize GAN training, which is crucial for large-scale GAN training in image domain, and data augmentation to improve model generalization.
> Unfortunately, all these trials resulted in worse perceptual quality in our study.
> The details can be found in the Appendix~\ref{appendix:practical}.
> We hope these practical lessons that we have learned would be useful to future research endeavors.

## 4.Experiments: 实验

> We conduct a comprehensive evaluation of BigVGAN for both in-distribution and out-of-distribution scenarios.
> We train BigVGAN and all baseline models on the full LibriTTS dataset.

### 4.1.Training data

> We use LibriTTS \citep{zen2019libritts} dataset with the original sampling rate of 24 kHz for training. Unlike previous studies which only adopted a subset (\texttt{train-clean-100} or \texttt{train-clean-360}) recorded in a clean environment \citep{jang2020universal, jang2021univnet, albadawy2021vocbench}, we use all training data including the subset from diverse recording environments (\texttt{train-full} = \texttt{train-clean-100 + train-clean-360 + train-other-500}), which is unprecedented in the literature. We find that the diversity of the training data is important to achieve the goal towards universal neural vocoding using BigVGAN.~\footnote{The ablation results on training data diversity can be found in Table~\ref{tab:data-ablation}.}
> For OOD experiments, we resample the audio to 24 kHz if necessary using \texttt{kaiser-best} algorithm provided by \texttt{librosa} package.
>
> Conventional STFT parameters are engineered to have a limited frequency band [0, 8] kHz by cutting off the high frequency details for easier modeling. On a contrary, we train all models~(including the baselines) using a frequency range [0, 12] kHz and a 100-band log-mel spectrogram, which is also used in a recent study towards universal vocoding~\citep{jang2021univnet}. We set other STFT parameters as in previous work \citep{kong2020hifi}, with 1024 FFT size, 1024 Hann window, and 256 hop size.

### 4.2.Models

> We train all BigVGAN models including the ablation models and the baseline HiFi-GAN using our training configuration for 1M steps. We use the batch size of 32, a segment size of 8,192, and a initial learning rate of $1\times10^{-4}$. All other configurations including optimizer, learning rate scheduler, and scalar weights of the loss terms follow the official open-source implementation of HiFi-GAN \citep{kong2020hifi} without modification, with an exception that BigVGAN replaces MSD by MRD for the discriminator. All models are trained using NVIDIA DGX-1 with 8 V100 GPUs. Refer to Table \ref{hyperparams} in the Appendix \ref{appendix:arch_detail} for detailed hyperparameters.
>
> We include a comparison with SC-WaveRNN~\citep{paul2020speaker}, a state-of-the-art autoregressive universal neural vocoder based on WaveRNN~\citep{kalchbrenner2018efficient}, using the official implementation. We also include two popular flow-based models: WaveGlow~\citep{prenger2019waveglow} and WaveFlow~\citep{ping2019waveflow}, using their official implementation. 
> For out-of-distribution test, we include the unofficial open-source implementation of UnivNet-c32~\citep{jang2021univnet}, \footnote{\url{https://github.com/mindslab-ai/univnet}. Note there is no official open-source code.}  which uses \texttt{train-clean-360} subset for training and is reported to outperform HiFi-GAN under the same training configurations. See appendix~\ref{appendix:more-ablations-ljspeech-vctk} for more details.
>
> Table~\ref{tab:model_comparison} summarizes the synthesis speed of flow-based and GAN vocoders for generating 24 kHz audio. We omit SC-WaveRNN as it is much slower. 
> BigVGAN-base with 14M parameters can synthesize the audio 70.18$\times$ faster than real time, which is relatively slower than HiFi-GAN as the filtered \emph{Snake} function requires more computation.
> HiFi-GAN and BigVGAN are faster than flow-based models, because they are fully parallel (WaveFlow has partial autoregression) and have much fewer layers (WaveGlow has 96 layers).
> Our BigVGAN with 112M parameters can synthesize the audio 44.72~$\times$ faster than real-time and keeps the promise as a high-speed neural vocoder.

### 4.3.Evaluation Metrics

> The objective metrics we collected are designed to measure varied types of distance between the ground-truth audio and the generated sample. We provide 5 different metrics: 
> 1. Multi-resolution STFT (M-STFT)~\citep{yamamoto2020parallel} which measures the spectral distance across multiple resolutions.~\footnote{We used an open-source implementation from \texttt{Auraloss} \citep{steinmetz2020auraloss}.}
> 2. Perceptual evaluation of speech quality (PESQ)~\citep{rix2001perceptual}, a widely adopted automated assessment of voice quality.~\footnote{We used a 16,000Hz wide-band version from \url{https://github.com/ludlows/python-pesq}.}
> 3. Mel-cepstral distortion~(MCD)~\citep{kubichek1993mel} with dynamic time warping  which measures the difference between mel cepstra.~\footnote{We used an open-source implementation from \url{https://github.com/ttslr/python-MCD}.}
> 4. Periodicity error, and 
> 5. F1 score of voiced/unvoiced classification (V/UV F1) which are considered as major artifacts from non-autoregressive GAN vocoders~\citep{morrison2021chunked}.~\footnote{We used the periodicity error and V/UV F1 score code provided by CARGAN~\citep{morrison2021chunked}.}
>
> The conventional 5-scale mean opinion score (MOS) is insufficient for the subjective evaluation of universal vocoder, because the metric needs to differentiate the utterances from diverse speaker identities recorded in various environments. For example, the model may always output some very natural ``average'' voices, which is not preferred but can still be highly rated by human workers in MOS evaluation.
> As a result, we also perform the 5-scale similarity mean opinion score (SMOS) evaluation, where the participant is asked to give the score of similarity for the pair of audio after listening to ground-truth audio and the sample from the model side-by-side. SMOS provides an improved way of assessing how close the given sample is to the ground-truth, where the ground-truth recordings can have diverse speaker identities, contains unseen languages for the listeners, and be recorded in various acoustic environments. SMOS is also directly applicable to non-speech samples, e.g., music. 
> We did MOS and SMOS evaluation on Mechanical Turk. More details can be found in Appendix~\ref{appendix:AMT}.

## 5.Results: 结果

### 5.1.LibriTTS Results

> We report the performance of BigVGAN and the baseline models evaluated on LibriTTS using above objective and subjective metrics.
> We perform objective evaluations on \texttt{dev-clean} and \texttt{dev-other} altogether, and conduct subjective evaluations on the combined \texttt{test-clean} and \texttt{test-other}. The \texttt{dev} and \texttt{test} splits of LibriTTS contains unseen speakers during training, but the recording environments are covered in the train split.
>
> Table \ref{tab:libritts-all} shows the in-distribution test results on LibriTTS. Baseline models other than HiFi-GAN performs significantly worse. This indicates that GAN vocoder is the state-of-the-art for universal neural vocoding. BigVGAN significantly improves all objective metrics. In particular, BigVGAN-base exhibits consistently improved objective scores over HiFi-GAN~(V1) with the same amount of paramters, suggesting that it has better periodic inductive bias for waveform data.
>
> HiFi-GAN (V1), BigVGAN-base, and BigVGAN perform comparably well in terms of MOS without listening to the ground-truth audio side-by-side. 
> When the listeners can compare the model sample with ground truth audio side-by-side, BigVGAN-base measurably outperforms HiFi-GAN (V1) in terms of SMOS ($+0.05$), and the 112M BigVGAN outperforms HiFi-GAN by a clear margin in terms of SMOS ($+0.11$) because it has high model capacity to further leverage the diverse training data for better quality.
>
> In Appendix~\ref{appendix:more-ablations-ljspeech-vctk}, we also report the additional results including UnivNet~\citep{jang2021univnet} and the ablation models of BigVGAN-base on LibriTTS \texttt{dev} sets, unseen \texttt{VCTK}~\citep{yamagishi2019cstr}, and \texttt{LJSpeech}~\citep{Ito2017ljspeech} data.

### 5.2.Unseen Languages & Varied Recording Environments

> In this subsection, we assess the universal vocoding capability of BigVGAN by measuring its zero-shot performance for various unseen languages with varied types of the recording environments in the unseen dataset.
> Based on the results in Table \ref{tab:libritts-all}, we only include GAN-based vocoders as the state-of-the-art baseline.
> We gather three classes of a publicly available multi-language dataset categorized by the type of noise from the recording environment.
>
> - A collection of under-resourced languages recorded in a noiseless studio environment \citep{sodimana2018step}: Javanese, Khmer, Nepali, and Sundanese. We use randomly selected 50 audio clips with equal balance across languages from the combined dataset.
> - The Multilingual TEDx Corpus \citep{salesky2021multilingual}: contains a collection of TEDx talks in Spanish, French, Italian, and Portuguese. We use randomly selected 50 audio clips with equal balance across languages from the IWSLT'21 test set. We simulate the unseen recording environment setup by adding a random environmental noise from MS-SNSD \citep{reddy2019scalable}, such as airport, cafe, babble, etc.
> - Deeply Korean read speech corpus \citep{deeply_corpus_kor}: contains short speech audio clips in Korean, recorded in three types of recording environments (anechoic chamber, studio apartment, and dance studio) using a smartphone. 
> We use randomly selected 50 audio clips where 25 clips are from the studio apartment, and the remaining 25 clips are from the dance studio. The collected audio clips contain a significant amount of noise and artifacts from real-world recording environments, such as reverb, echo, and static background noise.
>
> Table \ref{tab:multi-language} summarizes the SMOS results from three different types of unseen dataset. We only did SMOS evaluations, because the datasets have unseen languages for human listeners and it is hard to determine the quality without side-by-side comparison with the ground-truth recordings. 
> For clean under-resourced language dataset, the performance gap between models is not substantially large. This indicates that the universal vocoder trained on the entire LibriTTS training set is robust to unseen languages under clean recording environments.
> For both types of unseen recording environment (simulated or real-world), BigVGAN outperforms the baseline models by a large margin. The small capacity BigVGAN-base also shows improvements compared to the baseline with statistical significance (p-value $<$ 0.05 from the Wilcoxon signed-rank test). This suggests that BigVGAN is significantly more robust to the unseen recording environments thanks to the improved generator design with the AMP module.
> In Appendix~\ref{appendix:asr}, we further demonstrate that BigVGAN is the most linguistically accurate universal vocoder in terms of character error rate (CER) on multiple languages.
>
> We test the open-source implementation of UnivNet~\citep{jang2021univnet} with the pretrained checkpoint which is trained on \texttt{train-clean-360} subset. 
> Contrary to the report from \citet{jang2021univnet} that UnivNet-c32 outperformed HiFi-GAN \citep{kong2020hifi}, we find that the unmodified HiFi-GAN trained on the entire LibriTTS dataset is able to match or outperform UnivNet-c32. 
> We also train UnivNet-c32 on LibriTTS \texttt{train-full} and find that it is not benefited from larger training data. See Appendix~\ref{appendix:more-ablations-ljspeech-vctk} for detailed analysis. 

### 5.3.Out-of-Distribution Robustness

> In this subsection, we test BigVGAN's robustness and extrapolation capability by measuring zero-shot performance on out-of-distribution data. We conduct the SMOS experiment using MUSDB18-HQ~\citep{musdb18-hq}, a multi-track music audio dataset which contains vocal, drums, bass, other instruments, and the original mixture. The test set contains 50 songs with 5 tracks. We gather the mid-song clip with the duration of 10 seconds for each track and song.
>
> Table~\ref{tab:musdb-hq} shows the SMOS results from the 5 tracks and their average from the MUSDB18-HQ test set. BigVGAN models demonstrate a substantially improved zero-shot generation performance with wider frequency band coverage, whereas baseline models fail to generate audio outside the limited frequency range and suffer from severe distortion. The improvements are most profound for singing voice~(vocal), instrumental audio~(others) and the full mixture of the song~(mixture), whereas the improvements from drums and bass are less significant.
>
> We also experiment with audios obtained from YouTube videos from real-world recording environments. BigVGAN also exhibits robustness to various types of out-of-distribution signals such as laughter. We provide audio samples to our demo page.~\footnote{\small\url{https://bigvgan-demo.github.io}}

### 5.4.Ablation Study

#### Model architecture

> To measure the effectiveness of the BigVGAN generator, we include SMOS test for the ablation models of BigVGAN-base on MUSDB18-HQ data. Table~\ref{tab:musdb-hq} shows that the ablation models exhibit clear degradation on various  scenarios such as instrumental audio (others, mixture). From the average SMOS ratings, 1) disabling the anti-aliasing filter for \emph{Snake} activation performs worse than BigVGAN-base and 2) removing both the filter and \emph{Snake} activation (i.e., vanilla HiFi-GAN trained with MRD replacing MSD) is even worse than the \emph{Snake}-only ablation model, both with statistical significance (p-value $<$ 0.01 from the Wilcoxon signed-rank test). This indicates that Leaky ReLU is not robust enough to extrapolate beyond the learned frequency range and the aliasing artifacts degrade the audio quality in challenging setups. The results show that BigVGAN generator demonstrates strong robustness and extrapolation capability to out-of-distribution scenarios because of the seamless integration of periodic inductive bias and anti-aliased feature representation. See Appendix~\ref{appendix:visualization} for the visualization of anti-aliasing effect in BigVGAN.

#### Big model

> We compare HiFi-GAN and BigVGAN both with the largest 112M parameters. We train the 112M HiFi-GAN with the same training setting as BigVGAN. We conduct a pairwise test between the two models on the \texttt{mixture} test set of MUSDB18-HQ which is challenging out-of-distribution data. We ask the participants to select a better sounding audio between the samples from the two models. The test shows that 58 $\%$ of the ratings voted to BigVGAN over the large HiFi-GAN and the quality of BigVGAN is greater than the large HiFi-GAN with statistical significance (p-value $<$ 0.01 from the Wilcoxon signed-rank test). The results further validate the architectural advantage of BigVGAN in large-scale setting. 

#### Large Training data

> To verify the importance of using large-scale training data, we trained our BigVGAN using less diverse, clean speech-only dataset with the same training configuration for 1M steps: 1) \texttt{train-clean-360} subset of LibriTTS, or 2) VCTK dataset. Table~\ref{tab:data-ablation} shows that training BigVGAN on less diverse data shows degradation in both objective metrics and the subjective SMOS on the LibriTTS evaluation sets. The result verifies the importance of using diverse training data and demonstrates the effectiveness of BigVGAN on large-scale datasets.

## 6.Conclusions: 结论

> This study explores the limits of universal neural vocoding with an unprecedented scale of the data, model, and evaluations.
> We analyze the performance with various automatic and human evaluations across diverse scenarios including unseen speakers, languages, recording environments and out-of-distribution data.
> We present ***BigVGAN*** with an improved generator architecture by introducing anti-aliased periodic activation function with learned frequency control, which injects the desired inductive bias for waveform generation.
> Based on the improved generator, we demonstrate the largest GAN vocoder with strong zero-shot performance under various OOD conditions, including unseen recording environments, singing voice, and instrumental audio.
> We believe that ***BigVGAN***, combined with practical lessons learned from the large scale training, will inspire future endeavors for universal vocoding and improve the state-of-the-art results for real-world applications, including voice cloning, voice conversion, speech translation, and audio codec. 
