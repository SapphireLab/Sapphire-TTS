# BigVGAN: A Universal Neural Vocoder with Large-Scale Training

<details>
<summary>基本信息</summary>

- 标题:
- 作者:
  - 01 [Sang-gil Lee](../../Authors/Sang-gil_Lee.md)
  - 02 [Wei Ping](../../Authors/Wei_Ping.md)
  - 03 [Boris Ginsburg](../../Authors/Boris_Ginsburg.md)
  - 04 [Bryan Catanzaro](../../Authors/Bryan_Catanzaro.md)
  - 05 [Sungroh Yoon](../../Authors/Sungroh_Yoon.md)
- 机构:
  - [Nvidia](../../Institutions/Nvidia.md)
- 时间:
  - 预印时间: 2022.06.09 ArXiv v1
  - 预印时间: 2023.02.16 ArXiv v2
  - 更新笔记: 2024.08.02
- 发表:
  - 期刊/会议 
- 链接:
  - [ArXiv](https://arxiv.org/abs/2206.04658)
  - [DOI]()
  - [Github]()
  - [Demo](https://bigvgan-demo.github.io/)
  - [Scholar](https://scholar.google.com/scholar?cluster=)
- 标签:
  - ?
- 页数: ?
- 引用: ?
- 被引: ?
- 数据:
  - ? 
- 对比:
  - ?
- 复现:
  - ?

</details>

## Abstract: 摘要

> Despite recent progress in generative adversarial network~(GAN)-based vocoders, where the model generates raw waveform conditioned on acoustic features, it is challenging to synthesize high-fidelity audio for numerous speakers across various recording environments.  
> In this work, we present BigVGAN, a universal vocoder that generalizes well for various out-of-distribution scenarios without fine-tuning.
> We introduce periodic activation function and anti-aliased representation into the GAN generator, which brings the desired inductive bias for audio synthesis and significantly improves audio quality.
> In addition, we train our GAN vocoder at the largest scale up to 112M parameters, which is unprecedented in the literature.
> We identify and address the failure modes in large-scale GAN training for audio, while maintaining high-fidelity output without over-regularization. 
> Our BigVGAN, trained only on clean speech~(LibriTTS), achieves the state-of-the-art performance for various zero-shot~(out-of-distribution) conditions, including unseen speakers, languages, recording environments, singing voices, music,  and instrumental audio.~\footnote{Listen to audio samples from BigVGAN at: {\url{https://bigvgan-demo.github.io/}}. }
> We release our code and model at:  {\small \url{https://github.com/NVIDIA/BigVGAN}}.

## 1.Introduction: 引言

> Deep generative models have demonstrated noticeable successes for modeling raw audio.
> The successful methods include, autoregressive models~\citep{oord2016wavenet, mehri2016samplernn, kalchbrenner2018efficient}, flow-based models~\citep{oord2017parallel,ping2018clarinet, prenger2019waveglow, kim2018flowavenet, ping2019waveflow, lee2020nanoflow}, GAN-based models~\citep{donahue2018adversarial, kumar2019melgan,binkowski2020high, yamamoto2020parallel, kong2020hifi}, and diffusion models~\citep{kong2020diffwave, chen2020wavegrad, lee2022priorgrad}.
>
> Among these methods, GAN-based vocoders~\citep[e.g.,][]{kong2020hifi} can generate high-fidelity raw audio conditioned on mel spectrogram, while synthesizing hundreds of times faster than real-time on a single GPU. 
> However, existing GAN vocoders are confined to the settings with a moderate number of voices recorded in clean environment due to the limited model capacity.
> The audio quality can heavily degrade when the models are conditioned on mel spectrogram from unseen speakers in different recording environments. 
> In practice, a \emph{universal vocoder}, that can do zero-shot generation for out-of-distribution samples, is very valuable in many real-world applications, including text-to-speech with numerous speakers~\citep{ping2017deep}, neural voice cloning~\citep{arik2018neural, jia2018transfer}, voice conversion~\citep{liu2018wavenet}, speech-to-speech translation~\citep{jia2019direct}, and neural audio codec~\citep{zeghidour2021soundstream}.
> In these applications, the neural vocoder also needs to generalize well for audio recorded at various conditions.
>
> Scaling up the model size for zero-shot performance is a noticeable trend in text generation~\citep[e.g.,][]{brown2020language} and image synthesis~\citep[e.g.,][]{ramesh2021zero}, but has not been explored in audio synthesis.
> Although likelihood-based models are found to be easier for scaling among others because of their simple training objective and stable optimization, we build our universal vocoder with large-scale GAN training, because GAN vocoder has the following advantages:
> 1. In contrast to autoregressive or diffusion models, it is fully parallel and requires only one forward pass to generate high-dimensional waveform.
> 2. In contrast to flow-based models~\citep{prenger2019waveglow}, it does not enforce any architectural constraints~(e.g., affine coupling layer) that maintain the bijection between latent and data. 
> Such architectural constraints can limit model capacity given the same number of parameters~\citep{ping2019waveflow}.
>
> In this work, we present ***BigVGAN***, a **Big** **V**ocoding **GAN** that enables high-fidelity out-of-distribution~(OOD) generation without fine-tuning. 
> Specifically, we make the following contributions:
> - We introduce periodic activations into the generator, which provide the desired inductive bias for audio synthesis. 
> Inspired by the methods proposed for other domains~\citep{liu2020neural, sitzmann2020implicit}, we demonstrate the noticeable success of periodic activations in audio synthesis.
> - We propose anti-aliased multi-periodicity composition~(AMP) module for modeling complex audio waveform.
> AMP composes multiple signal components with learnable periodicities and uses low-pass filter to reduce the high-frequency artifacts.
> - We successfully scale BigVGAN up to 112M parameters by fixing the failure modes of large-scale GAN training without regularizing both generator and discriminator.
> The empirical insights are different from \citet{brock2018large} in image domain.
> For example, regularization methods~\citep[e.g.,][]{miyato2018spectral} introduce phase mismatch artifacts in audio synthesis. 
> - We demonstrate that BigVGAN-base with 14M parameters outperforms the state-of-the-art neural vocoders with comparable size for both in-distribution and out-of-distribution samples.
> In particular, BigVGAN with 112M parameters outperforms the state-of-the-art models by a large margin for zero-shot generation at various OOD scenarios, including unseen speakers, novel languages,  singing voices, music and instrumental audio in varied unseen recording environments. % It synthesizes 24 kHz high-fidelity speech 44.72~$\times$ faster than real-time on a V100 GPU.
>
> We organize the rest of the paper as follows.
> We discuss related work in \S~\ref{sec:related_work} and present BigVGAN in \S~\ref{sec:method}.
> We report empirical results in \S~\ref{sec:results} and conclude the paper in \S~\ref{sec:conclusion}.

## 2.Related Works: 相关工作

> Our work builds upon the state-of-the-art of GANs for image and audio synthesis.  
>
> GAN was first proposed for image synthesis~\citep{goodfellow2014generative}.
> Since then, impressive results have been obtained through optimized architectures~\citep[e.g.,][]{radford2015unsupervised, karras2021alias} or large scale training~\citep[e.g.,][]{brock2018large}.
>
> In audio synthesis, previous works focus on improving the discriminator architectures or adding new auxiliary training losses.
> MelGAN~\citep{kumar2019melgan} introduces the multi-scale discriminator~(MSD) that uses average pooling to downsample the raw waveform at multiple scales and applies window-based discriminators at each scale separately.
> It also enforces the mapping between input mel spectrogram and generated waveform via an $\ell_1$ feature matching loss from discriminator.
> In contrast, GAN-TTS~\citep{binkowski2020high} uses an ensemble of discriminators which operate on random windows of different sizes, and enforces the mapping between the conditioner and waveform adversarially using conditional discriminators.
> Parallel WaveGAN~\citep{yamamoto2020parallel} extends the single short-time Fourier transform (STFT) loss~\citep{ping2018clarinet} to multi-resolution, and adds it as an auxiliary loss for GAN training.
> \cite{yang2021multi} and \cite{mustafa2021stylemelgan} further improve MelGAN by incorporating the multi-resolution STFT loss.
>
> HiFi-GAN~\citep{kong2020hifi} reuses the MSD from MelGAN, and introduces the multi-period discriminator~(MPD) for high-fidelity synthesis. 
> UnivNet~\citep{jang2020universal, jang2021univnet} uses the multi-resolution discriminator~(MRD) that takes the multi-resolution spectrograms as the input and can sharpen the spectral structure of synthesized waveform.
> In contrast, CARGAN~\citep{morrison2021chunked} incorporates the partial autoregression~\citep{ping2019waveflow} into generator to improve the pitch and periodicity accuracy.
>
> In this work, we focus on improving and scaling up the generator.
> We introduce the periodic inductive bias for audio synthesis and address the feature aliasing issues within the non-autoregressive generator architecture.
> Our architectural design  has a connection with the latest results in time-series prediction~\citep{liu2020neural}, {implicit neural representations~\citep{sitzmann2020implicit}}, and image synthesis~\citep{karras2021alias}.
> Note that, \citet{you2021gan} argues that different generator architectures can perform equally well for single-speaker neural vocoding.
> We demonstrate that improving generator architecture is  crucial for universal neural vocoding in challenging conditions.
>
> There are limited successes for universal neural vocoding due to the noticeable challenges.
> In previous work, WaveRNN has been applied for universal vocoding task~\citep{lorenzo2018towards, paul2020speaker}. \citet{jiao2021universal} builds the  universal vocoder with flow-based model.
> GAN vocoder is found to be a good candidate recently~\citep{jang2021univnet}.

## 3.Methodology: 方法

> In this section, we introduce the preliminaries for GAN vocoder, then present the BigVGAN.
> See Figure~\ref{fig_model_diagram} for an illustration and refer to the Appendix \ref{appendix:arch_detail} for a detailed description of the architecture.

## Preliminaries of GAN Vocoder

> The audio waveform is known to exhibit high periodicity and can be naturally represented as the composition of primitive periodic components {(i.e., Fourier series under Dirichlet conditions)}.
> This suggests that we need to provide the desired inductive bias to the generator architecture. 
> However, the current non-autoregressive GAN vocoders~\citep[e.g.,][]{kong2020hifi} solely rely on layers of dilated convolutions to learn the required periodic components at different frequencies.
> Their activation functions~(e.g., {Leaky ReLU}) can produce new details with necessary nonlinearities, but do not provide any periodic inductive bias.
> Furthermore, we identified that Leaky ReLU behaves poorly for \textit{extrapolation} in waveform domain: although the model can generate high-quality speech signal in a seen recording environment at training, the performance degrades significantly for out-of-distribution scenarios such as unseen recording environments, non-speech vocalizations, and instrumental audio.
>
> We introduce a proper inductive bias of periodicity to the generator by applying a recently proposed periodic activation called Snake function~\citep{liu2020neural}, defined as $f_{\alpha}(x) = x + \frac{1}{\alpha} \sin^2(\alpha x)$, where $\alpha$ is a trainable parameter that controls the frequency of the periodic component of the signal and larger $\alpha$ gives higher frequency.
> The use of $\sin^2(x)$ ensures monotonicity and renders it amenable to easy optimization. 
> \citet{liu2020neural} demonstrates this periodic activation exhibits an improved extrapolation capability for temperature and financial data prediction.
>
> In BigVGAN, we use Snake activations $f_{\vv\alpha}(x)$ with channel-wise trainable parameters $\vv\alpha\in\mathbb{R}^h$ that define the periodic frequencies for each 1-D convolution channels.
> Taking this periodic functional form with learned frequency control, the convolutional module can naturally fit the raw waveform with multi-periodic components.
> We demonstrate that the proposed Snake-based generator is more robust for out-of-distribution audio samples unseen during training, indicating strong extrapolation capabilities in universal vocoding task.
> See Figure~\ref{fig_visualization_main_text} and Appendix~\ref{appendix:visualization} for illustrative examples; BigVGAN-base w/o filter using snake activations is closer to ground-truth sample than HiFi-GAN.

### Anti-aliased Representation

> The Snake activations provide the required periodic inductive bias for modeling raw waveform, but it can produce arbitrary high frequency details for continuous-time signals that can not be represented by the discrete-time output of the network,~\footnote{One can think of the neural vocoder as a discrete-time function on the sampled continuous-time signals.} which can lead to aliasing artifacts.
> This side effect can be suppressed by applying a low-pass filter~\citep[e.g.,][]{karras2021alias}.
> The anti-aliased nonlinearity operates by upsampling the signal $2\times$ along time dimension, applying the Snake activation, then downsampling the signal by $2\times$, which is a common practice inspired by the Nyquist-Shannon sampling theorem~\citep{shannon1949communication}.
> Each upsampling and downsampling operation is accompanied by the low-pass filter using a windowed \texttt{sinc} filter with a Kaiser window \citep{dsp}.
> Refer to the Appendix \ref{appendix:arch_detail} for details.
>
> We apply this filtered Snake nonlinearity in every residual dilated convolution layers within the generator to obtain the anti-aliased representation of the discrete-time 1-D signals.
> The module is named as \emph{anti-aliased multi-periodicity composition}~(AMP).
> See Figure~\ref{fig_model_diagram} for an illustration. 
> We find that incorporating the filtered activation can reduce the high-frequency artifacts in the synthesized waveform; see BigVGAN-base w/o filter vs.
> BigVGAN-base~(with filter) in Figure~\ref{fig_visualization_main_text} as an illustration.
> We will demonstrate that it provides significant improvements in various objective and subjective evaluations.
>
> Note that we also explored anti-aliased upsampling layers, but this results in significant training instabilities and lead to early collapse for large models.
> See Appendix~\ref{appendix:practical} for more details.

### BigVGAN with Large Scale Training

> In this subsection, we explore the limits of universal vocoding by scaling up the generator's model size to 112M parameters while maintaining the stability of GAN training and practical usability as a high-speed neural vocoder.
> We start with our improved generator using the comparable HiFi-GAN V1 configuration with 14M parameters~\citep{kong2020hifi}, which is denoted as BigVGAN-base.
>
> We grow BigVGAN-base by increasing the number of upsampling blocks and convolution channels for each block.
> The BigVGAN-base upsamples the signal by $256\times$ using 4 upsampling blocks with the ratio of $[8, 8, 2, 2]$.
> Each upsampling block is accompanied by  multiple residual layers with dilated convolutions, i.e., the AMP module. 
> We further divides the $256\times$ upsampling into 6 blocks $[4, 4, 2, 2, 2, 2]$ for more fine-grained feature refinement. 
> In addition, we increase the number of channels of AMP module (analogous to MRF in HiFi-GAN) from $512$ to $1536$.
> We denote the model with $1536$ channels and 112M parameters as BigVGAN.
>
> We found that the default learning rate of $2\times10^{-4}$ used in HiFi-GAN causes an early training collapse for BigVGAN training, where the losses from the discriminator submodules immediately converge to zero after several thousands of iterations.
> Halving the learning rate to $1\times10^{-4}$ was able to reduce such failures. 
> We also found that large batch size is helpful to reduce mode collapse at training~\citep{brock2018large}.
> We only double the batch size from the usual 16 to 32 for a good trade-off between training efficiency and stability, as neural vocoders can require millions of steps to converge.
> Note that this recommended batch size is still much smaller than the one for image synthesis~(e.g., 2048)~\citep{brock2018large}, because neural vocoding has strong conditional information.
>
> Even with the aforementioned changes, the large BigVGAN can still be prone to collapse early in training.
> We track the gradient norm of each modules during training and identify that the anti-aliased nonlinearity significantly amplified the gradient norm of MPD.
> Consequently, BigVGAN generator receives a diverging gradient early in training, leading to instabilities and potential collapse.
> We visualize the norm of gradient for each modules in Figure~\ref{fig_grad_norm} at Appendix~\ref{appendix:practical}.
> We alleviate the issue by clipping the global norm of the gradient to $10^{3}$, which is close to the average gradient norm of the 112M BigVGAN generator.
> This gradient clipping prevents the early training collapse of the generator.
> Note that, gradient clipping was found ineffective to alleviate training instability for image synthesis~(see Appendix H in \citet{brock2018large}), but it is very effective in our endeavors.
>
> In addition to above efforts, we have explored other directions, including various ways to improve the model architecture,  spectral normalization~\citep{miyato2018spectral} to stabilize GAN training, which is crucial for large-scale GAN training in image domain, and data augmentation to improve model generalization.
> Unfortunately, all these trials resulted in worse perceptual quality in our study.
> The details can be found in the Appendix~\ref{appendix:practical}.
> We hope these practical lessons that we have learned would be useful to future research endeavors.

## 4.Experiments: 实验

## 5.Results: 结果

## 6.Conclusions: 结论

> This study explores the limits of universal neural vocoding with an unprecedented scale of the data, model, and evaluations.
> We analyze the performance with various automatic and human evaluations across diverse scenarios including unseen speakers, languages, recording environments and out-of-distribution data.
> We present BigVGAN with an improved generator architecture by introducing anti-aliased periodic activation function with learned frequency control, which injects the desired inductive bias for waveform generation.
> Based on the improved generator, we demonstrate the largest GAN vocoder with strong zero-shot performance under various OOD conditions, including unseen recording environments, singing voice, and instrumental audio.
> We believe that BigVGAN, combined with practical lessons learned from the large scale training, will inspire future endeavors for universal vocoding and improve the state-of-the-art results for real-world applications, including voice cloning, voice conversion, speech translation, and audio codec. 
