# 变分自编码器

## 01.原来是这么一回事
<a id="01"></a>

原文: <https://kexue.fm/archives/5253>
时间: 2018-03-18

### 分布变换

通常我们会拿***变分自编码器 (Variational Auto-Encoder, VAE)*** 与 GAN 比较, 的确, 它们两个的目标基本是一致的: 希望构建一个从隐变量 $Z$ 生成目标数据 $X$ 的模型, 但是实现上有所不同.
更准确地讲, 它们是假设了隐变量 $Z$ 服从某些常见的分布 (如正态分布/均匀分布), 然后希望训练一个模型 $X=g(Z)$, 这个模型能够将隐变量概率分布映射到训练集的概率分布, 即它们的目的都是**进行分布之间的变换**.

那么现在假设 $Z$ 服从标准正态分布, 从中采样获得若干个隐变量 $Z_1,\cdots,Z_n$, 然后对它做变换得到 $\hat{X}_1=g(Z_1),\cdots,\hat{X}_n=g(Z_n)$.
那么**如何判断这个通过 $g$ 构造出来的数据集, 其分布和目标数据集的分布是否一致?**

用 KL 散度 (KL divergence) 衡量两分布之间的差异? 不行, 因为 KL 散度是根据两个概率分布的表达式来计算分布之间的相似度的, 而现在我们只有一批从构造的分布中采样而来的数据 $\{\hat{X}_1, \cdots, \hat{X}_n\}$, 和从真实的分布中采样而来的数据 $\{X_1, \cdots, X_n\}$, 只有样本本身, 没有分布表达式, 也就没有办法计算 KL 散度.

GAN 的思路: 没有合适的度量, 直接用神经网络来训练这样的度量. WGAN -> "[互怼的艺术: 从零直达 WGAN-GP]()" #TODO 补充原文.
VAE 的思路: 采用精致迂回的技巧.

### 经典回顾

下面回顾一下一般的教程是如何介绍 VAE 的.
我们有一批数据样本 $X=\{X_1, \cdots, X_n\}$, 

首先如果能够直接得到 $X$ 的分布 $p(X)$, 那么只需要根据 $p(X)$ 采样就能获得所有可能的 $X$, 这是终极理想的模型.
但这样的模型很难实现, 退而求其次, 我们修改分布为求和形式 (积分类似) 

<a id="Eq01-01"></a>

$$
  p(X) = \sum_Z p(X|Z) p(Z)\tag{01-01}
$$

此时 $p(X|Z)$ 描述了一个由 $Z$ 来生成 $X$ 的模型, 假设 $Z$ 服从标准正态分布, 即 $p(Z)\sim\mathcal{N}(0,I)$.

如果这一想法能够实现, 则可以从标准正态分布中采样一个 $Z$, 然后根据 $Z$ 计算一个 $X$, 这也是一个很好的模型.

然后结合自编码器来保证重构, 使得有效信息没有丢失, 再加上一系列的推导, 最后实现模型.

观察上面这一过程, 我们其实完全不清楚, 经过重新采样出来的 $Z_k$ 是不是还对应原来的 $X_k$,
如果直接最小化原始样本和重构样本之间的距离度量 $\mathcal{D}(\hat{X}_k,X_k)$ 是不合理的, 代码也不是如此实现的.

### VAE 初现

其实在整个 VAE 模型中, 并没有使用**隐变量的分布 $p(Z)$ 是正态分布的假设**, 而是使用了**后验分布 $p(Z|X)$ 是正态分布的假设**.

具体来说给定一个真实样本 `X_k`, 假设存在一个专属于 $X_k$ 的后验分布 $p(Z|X_k)$, 并进一步假设这个分布是独立的多元正态分布. 
注: 为什么要强调专属呢, 因为后面需要训练生成器 $X=g(Z)$, 希望能够把从分布 $p(Z|X_k)$ 中采样出来的 $Z_k$ 还原为 $X_k$. 如果假设 $p(Z)$ 是正态分布, 从 $p(Z)$ 采样的 $Z$ 无法知道对应哪一个真实的 $Z$. 专属化后就有理由说从这个分布采样出来的 $Z$ 应该要还原到 $X_k$.

事实上, 在论文 "Auto-Encoding Variational Bayes" 的应用部分, 也特别强调了这一点.
> In this case, we can let the variational approximate posterior be a multivariate Gaussian with a diagonal covariance structure:

$$
    \log q_{\phi}(z|x^{(i)}) = \log \mathcal{N}(z; \mu^{(i)}, \sigma^{2(i)}I) \tag{9}
$$

这一式子是实现整个模型的关键.
尽管论文也提到 $p(Z)$ 是标准正态分布, 但那其实并不是本质重要的.

现在为每一个 $X_k$ 都配上了一个专属的正态分布, 才方便后面的生成器做还原.
但此时有多少个数据 $X$ 就有多少个正态分布, 那么如何找到专属于 $X_k$ 的正态分布 $p(Z|X_k)$ 的均值和方差呢?
没有直接的思路, 那就使用神经网络来拟合出来.

于是构建两个神经网络 $\mu_k = f_1(X_k)$ 和 $\log \sigma_k^2 = f_2(X_k)$ 来拟合均值和方差.
注: 拟合 $\log \sigma_k^2$ 而不是直接拟合 $\sigma_k^2$ 的原因是因为后者总是非负的, 需要加激活函数处理, 而前者无需增加激活函数.

现在通过神经网络获得了专属于 $X_k$ 的均值和方差, 也就能够得到相应的正态分布了, 然后从这一分布采样一个 $Z_k$, 通过一个生成器得到 $\hat{X}_k = g(Z_k)$, 然后最小化 $\mathcal{D}(\hat{X}_k,X_k)$ 即可.

### 分布标准化

- Q: 那么根据上述的训练过程, 最终会得到什么结果?
  首先, 我们希望重构 $X$, 即最小化 $\mathcal{D}(\hat{X}_k,X_k)$, 但这一重构受到噪声的影响, 因为 $Z_k$ 是重采样得到的, 而不是直接由编码器直接计算出来的. 那么噪声会增加重构的难度.
  不过好在这个噪声强度 (方差) 是由神经网络算出来的, 所以最终模型为了重构得更好, 会尽力让方差为 0.
  但方差为 0 就失去了随机性, 会导致采样总是得到确定的结果, 即均值, 只你和一个当然比拟合多个要容易, 而均值是另一个神经网络计算得到的.
  
  总的来说, **模型会慢慢退化为普通的自编码器, 噪声将不再起作用**.

所以 VAE 还让所有的 $p(Z|X)$ 都向标准正态分布看齐, 从而防止噪声为零, 同时保证模型具有生成能力.

- Q: 如何理解生成能力这一词?
  若所有的 $p(Z|X)$ 都接近标准正态分布 $\mathcal{N}(0,I)$, 那么根据前面的[公式 (01-01)](#Eq01-01), 有
  <a id="Eq01-02"></a>
  $$
  \begin{aligned}
    p(Z) &= \sum_X p(Z|X) p(X) \\
    &\approx \sum_X \mathcal{N}(0,I) p(X) = \mathcal{N}(0,I)\sum_X p(X) = \mathcal{N}(0,I) \tag{01-02}
    \end{aligned}
  $$

    此时, 就达到了先验假设: $p(Z)$ 是标准正态分布, 然后就可以直接从 $\mathcal{N}(0,I)$ 中采样 $Z$ 以生成图像了.

- Q: 那么如何让所有的 $p(Z|X)$ 都向 $\mathcal{N}(0,I)$ 看齐呢?
  如果没有外部知识, 最直接的方法是在重构损失的基础上加入额外的损失:

  <a id="Eq01-03"></a>
  $$
  \mathcal{L}_{\mu}=\|f_1(X_k)\|^2, \quad \mathcal{L}_{\sigma}=\|f_2(X_k)\|^2 \tag{01-03}
  $$
  分别代表了均值和方差对数, 要使得分布接近标准正态分布, 则这两个损失将尽量接近 0.
- Q: 这两个损失的比例如何选取?
  选取得不好, 则生成的图像会比较模糊.
  所以, 原论文直接计算了一般 (各个分量独立) 正态分布与标准正态分布的 KL 散度 $KL(\mathcal{N}(\mu,\sigma^2)\|\mathcal{N}(0,I))$ 作为额外的损失, 即
  <a id="Eq01-04"></a>
  $$
  \mathcal{L}_{\mu\sigma}=\dfrac{1}{2}\sum_{i=1}^d (\mu^2_{(i)}+\sigma_{(i)}^2-\log \sigma_{(i)}^2-1) \tag{01-04}
  $$
  
  此处的 $d$ 是隐变量 $Z$ 的维度, 而 $\mu_{(i)}$ 和 $\sigma_{(i)}$ 是一般正态分布的均值向量和方差向量的第 $i$ 个分量.
  直接用这个损失作为补充, 就不用考虑均值损失和方差损失的相对比例问题了.


### 重参数技巧



## 02.从贝叶斯观点出发
<a id="02"></a>

原文: <https://kexue.fm/archives/5343>
时间: 2018-03-28

已知概率密度函数 $p(x)$, 那么 $x$ 的期望定义为
$$
  \mathbb{E}[x] = \int x p(x) \text{d}x
$$

对其进行数值计算, 那么可以选择若干个具有代表性的点 $x_0<x_1<\cdots<x_n$, 然后得到数值积分结果:
$$
  \mathbb{E}[x] \approx \sum_{i=1}^{n} x_i p(x_i) (x_{i}-x_{i-1})
$$

如果从 $p(x)$ 中采样若干个点 $x_1,\cdots,x_n$, 那么有
$$
  \mathbb{E}[x] \approx \frac{1}{n} \sum_{i=1}^{n} x_i,\quad x_i\sim p(x)
$$

这两个等式的区别是后者的 $x_i$ 是从 $p(x)$ 按概率采样出来的, 所以无需再乘以 $p(x_i)$.

更一般地可以写出

$$
  \mathbb{E}_{x\sim p(x)}[f(x)] = \int f(x) p(x) \text{d}x\approx \frac{1}{n} \sum_{i=1}^{n} f(x_i), x_i\sim p(x)
$$

这就是蒙特卡洛模拟的基础.

### KL 散度

通常使用 KL 散度来衡量两个概率分布 $p(x)$ 和 $q(x)$ 之间的差异, 定义为

$$
  D_{KL} [p(x)\| q(x)] = \int p(x)\log \dfrac{p(x)}{q(x)} \text{d}x = \mathbb{E}_{x\sim p(x)} [\log \dfrac{p(x)}{q(x)}]
$$

KL 散度的主要性质是非负性.
若固定 $p(x)$, 那么 $D_{KL}[p(x)\|q(x)] = 0 \lrArr p(x)=q(x)$;
若固定 $q(x)$, 那么 $D_{KL}[p(x)\|q(x)] = 0 \lrArr p(x)=q(x)$.

也就是不管固定哪一个, 最小化 KL 散度的结果都是两者尽可能相等.

这一点的严格证明需要用到变分法.

KL 散度有一个比较明显的问题: $q(x)$ 在某个区域等于 0, 而 $p(x)$ 在该区域不为 0, 那么 KL 散度就会出现无穷大.
这是 KL 散度的固有问题, 只能想办法规避.
例如隐变量的先验分布是高斯分布而不是均匀分布.

除了 KL 散度, 还有其他各种分布距离定义, Wiki 百科的 Statistical Distance.
例如巴氏距离 (Bhattacharyya Distance) 定义为

$$
  D_{B}(p(x), q(x)) = -\ln \int \sqrt{p(x)q(x)}\text{d}x
$$

这个距离不仅对称, 还没有 KL 散度的无穷大的问题.
然而还是选用 KL 散度, 因为不仅理论上要优美, 还需要实践上的可行.
KL 散度可以写成期望的形式, 允许进行采样计算.
相反, 巴氏距离就没那么容易了.

### 框架

首先有一批数据样本 $\{x_1, \cdots, x_n\}$, 希望借助隐变量 $z$ 描述 $x$ 的分布 $p(x)$:

$$
  q(x) = \int q(x,z)\text{d}z = \int q(x|z) q(z)\text{d}z
$$

这里的 $q(z)$ 是先验分布 (标准正态分布).

目的是希望 $q(x)$ 能逼近 $p(x)$. 这样既描述了 $p(x)$ 又得到了生成模型 $q(x|z)$.

直接对 $p(x,z)$ 进行近似是最干脆的.
具体来说, 定义 $p(x,z) = p(x) p(z|x)$, 我们假设用一个联合概率分布 $q(x,z)$ 来逼近 $p(x,z)$, 那么相应的 KL 散度为
$$
  D_{KL}[p(x,z)\| q(x,z)] = \iint p(x,z) \log \dfrac{p(x,z)}{q(x,z)} \text{d}z\text{d}x
$$

然后拆分

$$
\begin{aligned}
  D_{KL}[p(x,z)\| q(x,z)]
  &= \iint p(x,z) \log \dfrac{p(x,z)}{q(x,z)} \text{d}z\text{d}x\\
  &= \iint p(x)p(z|x) \log \dfrac{p(x)p(z|x)}{q(x,z)}\text{d}z\text{d}x\\
  &= \int p(x) \left[\int p(z|x) \log \dfrac{p(x)p(z|x)}{q(x,z)}\text{d}z\right]\text{d}x\\
  &= \mathbb{E}_{x\sim p(x)}\left[\int p(z|x) \log \dfrac{p(x)p(z|x)}{q(x,z)}\text{d}z\right]\\
  &= \mathbb{E}_{x\sim p(x)}\left[\int p(z|x) \log p(x)+ p(z|x) \log \dfrac{p(z|x)}{q(x,z)}\text{d}z\right]\\
  &= \mathbb{E}_{x\sim p(x)}\left[\int p(z|x) \log p(x)\text{d}z\right] + \mathbb{E}_{x\sim p(x)}\left[\int p(z|x) \log \dfrac{p(z|x)}{q(x,z)}\text{d}z\right]\\
  &= \mathbb{E}_{x\sim p(x)}\left[\log p(x) \int p(z|x)\text{d}z\right] + \mathbb{E}_{x\sim p(x)}\left[\int p(z|x) \log \dfrac{p(z|x)}{q(x,z)}\text{d}z\right]\\
  &= \mathbb{E}_{x\sim p(x)}\left[\log p(x)\right] + \mathbb{E}_{x\sim p(x)}\left[\int p(z|x) \log \dfrac{p(z|x)}{q(x,z)}\text{d}z\right]\\
  &= C + \mathbb{E}_{x\sim p(x)}\left[\int p(z|x) \log \dfrac{p(z|x)}{q(x,z)}\text{d}z\right]\\
\end{aligned}
$$

此时最小化 $D_{KL}[p(x,z)\| q(x,z)]$ 等价于最小化 $D_{KL}[p(x)\| q(x|z)] - C$.

为了得到生成模型, 那么 $q(x,z)=q(x|z)q(z)$, 那么

$$
\begin{aligned}
  Loss &= \mathbb{E}_{x\sim p(x)}\left[\int p(z|x) \log \dfrac{p(z|x)}{q(x,z)}\text{d}z\right]\\
  &= \mathbb{E}_{x\sim p(x)}\left[\int p(z|x) \log \dfrac{p(z|x)}{q(z)q(x|z)}\text{d}z\right]\\
  &= \mathbb{E}_{x\sim p(x)}\left[\int p(z|x) \log \dfrac{p(z|x)}{q(z)}\text{d}z-\int p(z|x) \log q(x|z)\text{d}z\right]\\
  &= \mathbb{E}_{x\sim p(x)}\left[\mathbb{E}_{z\sim p(z|x)}[\log \dfrac{p(z|x)}{q(z)}] + \mathbb{E}_{z\sim p(z|x)} [-\log q(x|z)] \right]\\
  &= \mathbb{E}_{x\sim p(x)}\left[D_{KL}[p(z|x)\|q(z)] + \mathbb{E}_{z\sim p(z|x)} [-\log q(x|z)] \right]
\end{aligned}
$$

括号内就是 VAE 的损失函数.

上式不能分成两部分看, 并认为是两个损失的最小化.

若 $D_{KL}[p(z|x)\|q(z)] = 0$, 就等价于 $p(z|x) = q(z)$, 也就是 $p(z|x)$ 和 $x$ 无关, 导致 $z$ 没有辨识度, 所以 $-\log q(x|z)$ 不可能小 (因为预测不准);
若 $-\log q(x|z)$ 小, 那么 $q(x|z)$ 大, 预测准确, 那么此时 $p(z|x)$ 不会太随机, 那么 $D_{KL}[p(z|x)\|q(z)]$ 不会小.

所以这两部分损失是互相拮抗的.
从整体来看, 整体损失越小那么模型就越接近收敛.

### 实验

现在 $q(z), q(x|z), p(z|x)$ 都是未知的.
那么为了实验也需要把损失函数的每一项都明确写出来.

首先为了便于采样, 假设 $z\sim \mathcal{N}(0,I)$, 即多元标准正态分布.
而 $q(x|z)$ 和 $p(z|x)$ 则使用神经网络拟合.

具体来说, 假设 $p(z|x)$ 也是正态分布, 其均值和方差由 $x$ 决定:

$$
  p(z|x) = \dfrac{1}{\prod_{k=1}^{d}\sqrt{2\pi\textcolor{cyan}{\sigma_{(k)}^2(x)}}}\exp(-\dfrac{1}{2}\left\|\dfrac{z-\textcolor{cyan}{\mu(x)}}{\textcolor{cyan}{\sigma(x)}}\right\|^2)
$$

其中 $\mu(x),\sigma(x)$ 是输入为 $x$, 输出分别为均值和方差的神经网络.

那么 KL 散度部分就可以先计算出来

$$
  D_{KL}[p(z|x)\| q(z)] =\dfrac{1}{2} \sum_{k=1}^{d} (\mu_{(k)}^{2}(x) + \sigma_{(k)}^{2}(x) - \log \sigma_{(k)}^{2}(x) - 1)
$$

剩下的 $q(x|z)$ 要选择什么分布呢?
Auto-Encoding Variational Bayes 给出了两种候选方案: 伯努利分布或正态分布.

1. 伯努利分布
   $$
    q(x|z) = \prod_{k=1}^{D} (\rho_{(k)}(z))^{x_{(k)}}(1-\rho_{(k)}(z))^{1-x_{(k)}}
   $$
   对应的损失为
   $$
    -\log q(x|z) = \sum_{k=1}^{D} -x_{(k)} \log \rho_{(k)}(z) - (1-x_{(k)}) \log (1-\rho_{(k)}(z))
   $$
   这表明 $\rho(z)$ 要压缩到 0~1 之间, 然后用交叉熵作为损失函数.
2. 正态分布
  $$
    -\log q(x|z) = \dfrac{1}{2}\| \dfrac{x-\mu(z)}{\sigma(z)}\|^2 + \dfrac{D}{2} \log 2\pi + \dfrac{1}{2}\sum_{k=1}^{D} \log \sigma_{(k)}^2(z)
  $$
  很多时候会固定方差为常数, 那么就出现了 MSE 损失函数.

所以, 二值数据可以对 Decoder 用 Sigmoid 函数激活, 然后用交叉熵作为损失函数, 对应于 $q(x|z)$ 为伯努利分布;
对于一般数据, 使用 MSE 作为损失函数, 对应于 $q(x|z)$ 为固定方差的正态分布.

至于对 $p(z|x)$ 的期望, VAE 只进行了一个采样.
