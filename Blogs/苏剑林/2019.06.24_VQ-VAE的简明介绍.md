# VQ-VAE 的简明介绍: 量子化自编码器

- 作者: 苏剑林
- 链接: [原文](https://kexue.fm/archives/6760)

有两件事情引起了笔者对 VQ-VAE 的兴趣:
1. DeepMind 提出的 VQ-VAE-2 实现了能匹配 BigGAN 的生成效果, by 机器之心 [超越BigGAN，DeepMind提出「史上最强非GAN生成器」VQ-VAE-2](https://mp.weixin.qq.com/s/GJr-YtV84eV1KbkyVSkcBA);
2. [Unsupervised Paraphrasing without Translation](https://papers.cool/arxiv/1905.12752) 里也使用了 VQ-VAE;

这表明 VQ-VAE 应该是一个颇为通用且有意思的模型.

## 模型综述

**VQ-VAE (Vector Quantized-Variational AutoEncoder)** 首先出现在论文 [Neural Discrete Representation Learning](https://papers.cool/arxiv/1711.00937), 和 VQ-VAE-2 一样, 都是 Google 团队的大作.

作为一个自编码器, VQ-VAE 的一个明显特征是它编码出来的编码向量是离散的. 
换句话说, 它最后得到的编码向量的每个元素都是一个整数, 这也就是 Quantized 的含义, 可以称为量子化 (和量子力学的量子一样, 都包含离散化的意思).

明明整个模型都是连续可导的, 但最终得到的编码向量却是离散的, 并且重构效果看起来还很清晰, 这至少意味着 VQ-VAE 会包含一些有意思, 有价值的技巧.
但实际读完有一种 "故弄玄虚" 的感觉.

首先读完整篇论文就会明白, VQ-VAE 其实是一个自编码器而不是变分自编码器, 明显加大了读懂这篇论文的难度.
其次 VQ-VAE 的核心步骤之一是 **Straight-Through Estimator**, 这是将隐变量离散化后的优化技巧, 在原文中没有稍微详细的介绍, 以至于必须看源码才能更好地理解.
最后论文的核心思想也没有很好地交代清除, 给人的感觉是纯粹在介绍模型本身而没有介绍模型思想.

### PixelCNN

要追溯 VQ-VAE 的思想就不得不谈到自回归模型.
可以说 VQ-VAE 做生成模型的思路, 源于 PixelRNN, PixelCNN 之类的自回归模型, 这类模型留意到要生成的图像实际上是离散的而不是连续的.
以 CIFAR-10 的图像为例, 它是 32×32 大小的三通道图像, 即 32×32×3 的矩阵, 矩阵的每个元素是 0~255 的任意一个整数, 这样一来, 就可以视为长度为 32×32×3 = 3072 的句子, 而词表的大小是 256, 从而可以使用语言模型的方法, 来逐像素, 递归地生成一张图片 (用前面的所有像素来预测下一个像素), 即自回归方法:

$$
    p(x) = p(x_1)p(x_2|x_1)\cdots p(x_{3n^2}|x_1,\cdots,x_{3n^2-1})\tag{01}
$$

其中 $p(x_1), p(x_{2}|x_{1})\cdots, p(x_{3n^2}|x_1,\cdots,x_{3n^2-1})$ 均是类别为 256 的分类问题, 只不过所依赖的条件不同.

自回归模型的研究主要集中在两个方面:
1. 如何设计递归顺序, 使得模型可以更好地生成采样, 因为图像序列不是简单的一维序列, 它至少是二维的, 更多情况是三维的, 这种情况下的递归顺序很大程度上影响生成效果;
2. 如何加速采样过程, 截至当时比较新的成果是 ICLR2019 的 [Generating High Fidelity Images with Subscale Pixel Networks and Multidimensional Upscaling](https://papers.cool/arxiv/1812.01608).

自回归的方法很稳妥, 也能有效地做概率估计, 但它有一个最致命的缺点: 慢.
因为它是逐像素地生成, 所以要每个像素地进行随机采样.
上面提及的 CIFAR-10 数据集是小图像尺寸地, 目前做图像生成至少也要 128×128×3 才有说服力, 总像素接近 5 万个, 此时逐像素生成会非常耗时.
而且这么长的序列, 不管是 RNN 还是 CNN 模型都无法很好地捕捉这么长的依赖.

原始的自回归还有一个问题, 就是割裂了类别之间的联系.
虽然说每个像素都是离散的, 所以视为 256 分类问题也无妨, 但事实上连续像素之间的差别是很小的, 纯粹地分类问题捕捉不到这种联系.

更数字化地说, 就是目标函数交叉熵是 $-\log p_t$, 假如目标像素是 100, 如果预测成 99, 因为类别不同, 那么 $p_t$ 就接近于 0, $-\log p_t$ 就很大, 从而带来一个很大的损失.
但从视觉上看, 像素值是 100 还是 99 的差别并不大, 不应该有这么大的损失.

## VQ-VAE

针对自回归模型的固有问题, VQ-VAE 提出的解决方案是: 先降维, 再对编码向量用 PixelCNN 建模.

### 降维离散化

看上去这个方案很自然, 似乎没什么特别的, 但事实上一点都不自然.

因为 PixelCNN 生成的是离散序列, 想用 PixelCNN 建模编码向量, 那意味着编码向量也是离散的才行.
而常见的降维手段, 如自编码器, 生成编码向量都是连续性变量, 无法直接生成离散变量.
同时, 生成离散型变量往往还意味着存在梯度消失的问题.
还有, 降维重构的过程, 如何保证重构后出现的图像不失真?
如果失真太严重, 甚至还比不上普通的 VAE 的话, 那 VQ-VAE 也没有什么价值了.

幸运的是, VQ-VAE 确实提供了有效的训练策略解决这两个问题.

### 最近邻重构

在 VQ-VAE 中一张 $n\times n\times 3$ 的图片 $x$ 先传入一个编码器中, 得到连续的编码向量 $z$:

$$
    z = encoder(x)
$$

这里的 $z$ 是一个大小为 $d$ 的向量.

另外, VQ-VAE 还维护一个 Embedding 层, 也可称为编码表, 记为

$$
    E = [e_1, e_2, \cdots, e_{K}]
$$

这里每个 $e_i$ 都是一个大小为 $d$ 的向量.

接着, VQ-VAE 经过最近邻搜索, 将 $z$ 映射为这 $K$ 个向量之一:

$$
    z\to e_{k}, k=\arg\min_{j} \|z-e_j\|_2
$$

可以将 $z$ 对应的编码表向量记为 $z_{q}$, 然后认为 $z_{q}$ 才是最后的编码结果.
最后将 $z_{q}$ 传入一个解码器, 希望重构原图 $\hat{x}=decoder{z_{q}}$.

所以整个流程是: $x$ → 编码器 → $z$ → 最近邻搜索 → $z_{q}$ → 解码器 → $\hat{x}$.

这样一来因为 $z_q$ 是编码表 $E$ 中的向量之一, 所以实际上就等价于 $1,\cdots K$ 这 $K$ 个整数之一, 因此这整个流程相当于将整张图片编码成了一个整数.

当然上述过程是比较简化的, 如果只编码为一个向量, 重构时难免失真, 而且泛化性难以得到保证.
所以实际编码时直接用多层卷积将 $x$ 编码为 $m\times m$ 个大小为 $d$ 的向量:

$$
    z = \begin{pmatrix}z_{11} & z_{12} & \cdots & z_{1m}\\z_{21} & z_{22} & \cdots & z_{2m}\\\vdots & \vdots & \ddots & \vdots\\z_{m1} & z_{m2} & \cdots & z_{mm}\end{pmatrix}
$$

也就是 $z$ 的总大小为 $m\times m\times d$, 它依然保留着位置结构, 然后每个向量都用前述方法映射为编码表中的一个, 就得到一个同样大小的 $z_{q}$, 然后用它来重构.
这样 $z_{q}$ 也等价于一个 $m\times m$ 的整数矩阵, 这就实现了离散型编码.

### 自行设计梯度

如果是普通的自编码器, 只需要优化下述损失:
$$
    \|x-decoder(z)\|_2^2
$$

但是 VQ-VAE 中用于重构的是 $z_q$, 那么似乎应该用:
$$
    \|x-decoder(z_q)\|_2^2
$$

但问题在于 $z_q$ 的构建过程包含了 $\arg\min$, 这一操作是没有梯度的, 所以直接用第二个损失是无法优化编码器的.

换言之, 目标是使得第二个损失最小, 但是却不好优化, 第一个损失容易优化, 但却不是所需的优化目标.
当然一个粗暴的方法是两个损失都用, 直接相加, 但这样并不好, 因为最小化第一个损失不是优化目标, 会带来额外的约束.

VQ-VAE 使用了一个很精巧且直接的方法, 称为 Straight-Through Estimator, 可以称为直通估计, 最早源于 Benjio 的论文 [Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation](https://papers.cool/arxiv/1308.3432).
在 VQ-VAE 原文中没有进行讲解, 但直接读这篇引用论文是一个很不友好的选择, 不如直接读源代码.

事实上 Straight-Through Estimator 的思想很简单, 就是前向传播的时候可以用想要的变量 (哪怕不可导), 而反向传播时, 用自己为它设计的梯度.
根据这一思想, 设计的目标函数是:

$$
    \| x-decoder(z+sg[z_q-z])\|_2^2
$$

其中 $sg$ 表示 Stop Gradient, 即不需要它的梯度.

这样一来, 前向传播计算损失时, 就等价于第二个损失函数;
反向传播计算梯度时, 由于 $z_q-z$ 不提供梯度, 所以等价于第一个损失的优化.

基于这个思想可以为很多函数自定义梯度, 例如 $x+sg[relu(x)-x]$ 就将 $relu(x)$ 的梯度定义恒为 1, 但误差计算和 $relu(x)$ 本身等价.
至于实用价值, 就要具体任务具体分析了.

### 维护编码表

要注意, 根据 VQ-VAE 最近邻搜索的设计, 应该期望 $z$ 和 $z_q$ 很接近, (事实上编码表 $E$ 中每个向量类似于各个 $z$ 的聚类中心出现.)
但事实上却未必如此, 即使上面两个损失都很小, 也不意味着 $z_q$ 和 $z$ 差别很小.
所以为了让两者更接近, 直接将两者的差加入到损失中:

$$
    \| x-decoder(z+sg[z_q-z])\|_2^2+\beta \|z-z_q\|_2^2
$$

除此之外还可以做的更仔细些.
由于编码表相对比较自由的, 而 $z$ 要尽力保证重构效果, 所以应当让 $z_q$ 去接近 $z$ 而不是让 $z$ 去靠近 $z_q$;
而因为额外损失项的梯度等价于对 $z_q$ 的梯度加上对 $z$ 的梯度, 所以可以等价地分解为

$$
    \|sg[z]-z_q\|_2^2+ \|z-sg[z_q]\|_2^2
$$

第一项等价于固定 $z$, 让 $z_q$ 靠近 $z$;
第二项等价于固定 $z_q$, 让 $z$ 靠近 $z_q$.
注意这个等价是对于反向传播来说的, 前向传播则是原来的两倍.

根据上述讨论, 希望让 $z_q$ 去靠近 $z$ 多于让 $z$ 去靠近 $z_q$, 所以可以调整损失函数的比例.

$$
    \| x-decoder(z+sg[z_q-z])\|_2^2+\beta \|sg[z]-z_q\|_2^2+\gamma \|z-sg[z_q]\|_2^2
$$

其中 $\gamma < \beta$, 原文使用的是 $\gamma=0.25\beta$.

注: 还可以使用滑动平均的方式更新编码表, 详情请看原文.

### 拟合编码分布

经过上述设计后, 终于将图片编码为 $m\times m$ 的整数矩阵了, 由于这 $m\times m$ 的矩阵一定程度上也保留了原来输入图片的位置信息, 所以可以用自回归模型来对编码矩阵进行拟合 (即建模先验分布).
通过如 PixelCNN 得到编码分布后, 就可以随机生成一个新的编码矩阵, 然后通过编码表 $E$ 映射为三维的实数矩阵 $z_q$, 最后经过解码器得到一张图片.

一般来说现在的 $m\times m$ 要比 $n\times n\times 3$ 小得多, 比如使用 CelebA 数据做实验时, 原来的 $128\times 128\times 3$ 的图像可以编码为 $32×32$ 的编码而基本不失真, 所以用自回归模型对编码矩阵进行建模比直接对原始图像建模要容易得多.

## 个人复现

笔者使用 Keras 复现的 VQ-VAE (Python2.7 Tensorflow1.8 Keras2.2.4), 链接为 [Github](https://github.com/bojone/vae/blob/master/vq_vae_keras.py), 模型部分参考了 [Github](https://github.com/nadavbh12/VQ-VAE)

这个脚本的正文部分只包含 VQ-VAE 的编码和重构, 没有包含用 PixelCNN 建模先验分布.
不过最后的注释那里包含了一个用 Attention 来建模先验分布的例子, 用 Attention 建模先验分布后的效果一定程度上表明这样的随机采样是可行的, 但效果还不能说是很好.

个人复现不够好不意味着这套方法不够好, 可能是没调好, 也可能是网络不够深.

## 总结

纵观全文, 其实没有任何 VAE 的味道, 因此笔者认为这其实就是一个编码为离散型向量的自编码器.
能够重构出比较清晰的图像是因为它编码时保留了足够大的特征图.

如果弄懂了 VQ-VAE, 那么 VQ-VAE-2 也没有什么难理解的, 因为几乎没有本质上的技术更新, 不过是把编码和解码分成了两层进行, 一层整体, 一层局部, 从而使得生成图像的模糊感更少.
