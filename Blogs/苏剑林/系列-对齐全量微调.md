# 对齐全量微调! 最精彩的 LoRA 改进

注: 基于原博客进行内容重排.

## 前言

众所周知, LoRA 是一种常见的参数高效的微调方法, 在文章 "梯度视角下的 LoRA: 简介, 分析, 猜测及推广" 中做过简单介绍.
LoRA 使用低秩分解来降低微调参数量, 节省微调显存, 同时训练好的权重可以合并到原始权重上, 推理架构不需要作出改变, 是一种训练和推理都比较友好的微调方案.

此外, 在文章 "配置不同的学习率, LoRA 还能再涨一点?" 中讨论了 LoRA 的不对称性, 指出给矩阵 $A$, $B$ 设置不同的学习率能取得更好的效果, 该结论被称为 LoRA+.

为了进一步提升效果, 研究人员还提出了不少其他 LoRA 变体: AdaLoRA, rsLoRA, DoRA, PiSSA 等, 这些改动都有一定道理, 但没有特别让人印象深刻的地方.

## LoRA

假设预训练参数为 $W_0\in \mathbb{R}^{n\times m}$, 那么**全量微调**时的更新量 $\delta W$ 自然也是一个 $n\times m$ 矩阵.
**LoRA** 将更新量约束为低秩矩阵来降低训练时的参数量, 即设 $\delta W= AB, W = W_0 + AB$, 其中 $A\in \mathbb{R}^{n\times r}, B\in \mathbb{R}^{r\times m}, r<<\min(n,m)$. 实际上会用新的参数 $W$ 替换模型原参数, 然后固定 $W_0$ 不变, 只训练 $A,B$.

为了使得 LoRA 的初始状态和预训练模型一致, 通常会将 $A$ 和 $B$ 其中之一进行全零初始化, 这样可以得到 $A_0B_0 = \mathbf{0}$, 从而初始的 $W$ 就是 $W_0$. 
注意这不是必须的, 如果 $A,B$ 都是非零初始化, 那么只需要设 $W=(W_0-A_0B_0)+AB$ 即可, 同样可以满足初始 $W$ 等于 $W_0$.

**需要指出的是, LoRA 往往只是显存不足的无奈之选, 因为一般情况下全量微调的效果都会优于 LoRA.**
所以若算力足够并且要追求最佳效果, 应优先选择全量微调. 
这一点也是 LoRA-GA 的假设之一, 因为它的改进方向就是向全量微调对齐.

**使用 LoRA 的另一个场景是有大量微型定制化需求**, 需要存下非常多的微调结果, 此时使用 LoRA 能够减少储存成本.

## LoRA-GA

于 2024 年 07 月 06 日提出的 [LoRA-GA: Low-Rank Adaptation with Gradient Approximation](../../Modules/LoRA/2024.07.06_LoRA-GA.md) 让笔者眼前一亮, 仅仅看了摘要就有种必然有效的感觉, 仔细阅读后更觉得是至今最精彩的 LoRA 改进.

LoRA-GA 提出了一个非常深刻的优化点: 通过 $W=(W_0-A_0B_0)+AB$ 可以保证 $W$ 的初始值等于 $W_0$. 那么能否**进一步调整 $A_0$ 和 $B_0$ 使得 LoRA 和全量微调在后续训练中也尽可能地近似**? 比如让经过第一步优化后的 $W_1$ 尽可能相等.

仔细思考越觉得这个优化角度直击本质, LoRA 的目标就是以小博大, 希望接近全量微调的效果. 那么尽可能对齐全量微调的后续更新结果就是最正确的改进方向.
从逼近的角度看, $W$ 的初始值等于 $W_0$ 相当于全量微调的零阶近似, 保持后面的 $W_1, W_2, \cdots$ 接近则相当于更高阶的近似, 是合情合理的选择.

具体来说, 假设优化器是 SGD, 那么对于全量微调有:
$$
    W_1 = W_0 - \eta \dfrac{\partial \mathcal{L}}{\partial W_0}.
$$

其中 $\mathcal{L}$ 是损失函数, $\eta$ 是学习率.

如果是 LoRA 则有:

$$
    A_1 = A_0 - \eta\dfrac{\partial \mathcal{L}}{\partial A_0} = A_0 - \eta\dfrac{\partial \mathcal{L}}{\partial W_0}B_0^{\mathsf{T}},
$$

$$
    B_1 = B_0 - \eta\dfrac{\partial \mathcal{L}}{\partial B_0} = B_0 - \eta A_0^{\mathsf{T}} \dfrac{\partial \mathcal{L}}{\partial W_0},
$$

$$
    W_1 = W_0 - A_0B_0 + A_1B_1\approx W_0 - \eta (A_0A_0^{\mathsf{T}}\dfrac{\partial \mathcal{L}}{\partial W_0} + \dfrac{\partial \mathcal{L}}{\partial W_0} B_0^{\mathsf{T}}B_0).
$$

最后这步近似省略了 $\eta$ 的二阶项.

现在两个 $W_1$ 具有相似的形式, 为了让它们尽可能近似, 可以考虑最小化

$$
    \arg\min_{A_0,B_0}\left\| A_0 A_0^{\mathsf{T}} \dfrac{\partial \mathcal{L}}{\partial W_0} + \dfrac{\partial \mathcal{L}}{\partial W_0} B_0^{\mathsf{T}}B_0 - \dfrac{\partial \mathcal{L}}{\partial W_0} \right\|_F^2.
$$

其中 $\| \cdot \|_F^2$ 表示 Frobenius 范数的平方, 即矩阵每个元素的平方和.

下面用 $G_0$ 代替 $\dfrac{\partial \mathcal{L}}{\partial W_0}$, 则目标函数可以写为:

$$
    \arg\min_{A_0,B_0}\left\| A_0 A_0^{\mathsf{T}} G_0 + G_0 B_0^{\mathsf{T}}B_0 - G_0 \right\|_F^2.
$$

注意到 $A_0A_0^{\mathsf{T}}G_0$, $G_0B_0^{\mathsf{T}}B_0$ 的秩顶多为 $r$, 相加之后的秩顶多为 $2r$.
下面假设 $2r < \min(n,m)$, 所以上述目标相当于寻找 $G_0$ 的一个秩不超过 $2r$ 的最优近似.

先考虑 $G_0$ 是非负对角矩阵的情形, 并且对角线元素已经按照从大到小的顺序排列.
这个例子很简单, 它的秩不超过 $2r$ 的最优近似就是只保留对角线前 $2r$ 个元素的新对角矩阵, 这一结论叫做 **Eckart-Young 定理**, 而能让 $A_0A_0^{\mathsf{T}}G_0 + G_0B_0^{\mathsf{T}}B_0$ 只保留 $G_0$ 的前 $2r$ 个对角线元素的 $A_0, B_0$ 可以是分块矩阵: $A_0= (I_n)_{[:,r]}$, $B_0=(I_m)_{[r:2r,:]}$, 下标 $[:,r]$ 表示前 $r$ 列, $[r:2r,:]$ 表示第 $r+1$ 到第 $2r$ 列.

注意这里是 "可以是", 即解不唯一. 也就是把 $G_0$ 的前 $2r$ 个对角元挑出来, $A_0A_0^{\mathsf{T}}G_0$ 和 $G_0B_0^{\mathsf{T}}B_0$ 各挑一半, 至于如何分配就无所谓了.
上面给出的解对应的是 $A_0A_0^{\mathsf{T}}G_0$ 取前 $r$ 个, $G_0B_0^{\mathsf{T}}B_0$ 取 $r+1~2r$ 个.

