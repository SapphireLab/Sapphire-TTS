# 别慌! 一文教你看懂 GPT-4o 背后的语音技术

- 作者：林唯秀
- 链接: [知乎](https://zhuanlan.zhihu.com/p/698725358)
- 日期：2024.05.27

## 导语

05 月 14 日凌晨, OpenAI 推出了最新的生成模型 GPT-4o, 带来了一系列震撼的功能, 用技术彻底颠覆了产品形态.
产品最大的亮点在于：以近乎完美的交互方式, 为每位用户带来GPT-4级别的智能体验.
在语音方面, GPT-4o做到了实时低延迟, 平均响应时间与人类反应速度相当, 输出的语音能够理解极度贴合对话上下文, 能够理解人类的情感情绪, 听觉质量上佳, 与真人无异.


OpenAI的生肉博客：https://openai.com/index/hello-gpt-4o/

GPT-4o 是一个 any2any 的多模态模型, 能够接受文本, 音频, 图像, 视频等多模态输入, 也能够生成包含文本, 语音, 图像和视频等混合内容的多模态输出.
**限于篇幅, 本文主要谈谈语音多模态的实现, 并分享一些对于语音研究未来发展的看法**.


当我们主要关注文本和语音模态时, GPT-4o 其实就是一个**语音语言模型 (Speech Language Model, SLM)**.
该 SLM 同时具备语音理解能力和语音合成能力, 输入端和输出端均支持文本和语音的混合多模态.
那么, 这一 SLM 应该如何实现呢? 
在**大语言模型 (Large Language Model, LLM)** 滥觞的今日, 不难想到这样一种方法：将连续的语音数据离散化成如同单词 (或者称 Token, 词元) 一样的表示, 并入到 LLM 的词表中, 再走一遍训练 LLM 的老路.

基于上述思想来构建 SLM, 需要解决以下几个问题：
- 语音如何离散化? 
- 如何让 LLM 理解语音的 token? 加入语音 token 之后, LLM 在语音数据的理解上是否具有涌现性? 
- LLM 如何合成/解码语音? 

接下来, 我们按图索骥, 分别看看上述三个问题应该如何解决.

## 语音的离散化：向 LLM 看齐! 

在谈及语音离散化之前, 我们先来看看语音和文本作为两种不同的模态, 有什么区别, 有什么联系.
这直接关系到后文建模方法的选择以及离散化特征的关注点.

语音和文本的差别主要体现在：
- 文本离散, 序列短, 信息密度高 (几乎每个词都包含语义)；
- 语音连续, 序列长, 信息密度低.

语音序列长, 信息密度低的特点, 意味着语音数据有很大的压缩空间, 这一点和图像非常类似.
因此, 一些用于图像的离散化压缩方法也可以用在语音上.

除了差异, 语音和文本也有一定的联系：
语音是文本的超集, 既包含文本内容(说话人说了什么, 也就是语义信息), 也包含语音特有的音色, 韵律, 语速等声学信息(也叫做副语言).
既然语音包含文本, 那么在 NLP 中预训练语言模型也可以用来建模语音中的上下文依赖关系, 从而得到语音的离散化 token.

基于这些方法得到的 token 主要包含语音的**语义信息**.

花开两朵, 各表一枝.
我们先来看看语音的语义 token 如何获取.

语音的语义 token: 用 MLM 建模语音的上下文依赖
语音的语义建模方法, 最常用到的就是 BERT 的 MLM 方法, 比较经典的工作有三个：
- wav2vec 2.0[1], 
- HuBERT[2]
- w2v-BERT[3].

## Wav2Vec 2.0

类似于 BERT, wav2vec 2.0 在隐空间 (latent space) 随机 mask 了一定比例的语音输入, 然后用基于对比学习的训练目标学习帧的表征.
值得注意的一点是, 对比学习中目标帧的离散化处理是一个非常巧妙的操作, 它将无限的连续特征空间坍缩为有限的离散空间, 让帧特征的鲁棒性更强了.
这在语音领域上非常有用的技巧, 允许模型接受带有噪声的语音作为输入.

wav2vec 2.0 的模型架构

## HuBERT 

wav2vec 2.0 只是借用了 BERT 中 mask 的操作, 训练目标大体上是基于对比学习的范式.
那么, 能直接用 BERT 的 MLM 建模目标来得到高质量的语音表征吗? 
其后的HuBERT[2]做的就是这个事情.

HuBERT[2] 的核心点在于使用简单的 KMeans 聚类方法为语音数据抽取离散化的分类标签, 也就是文中所说的 hidden unit/acoustic unit.
有了分类标签, 然后就是用 BERT 的 MLM loss 来学习语音数据中内在的上下文依赖关系.
对于 KMeans 聚类对初始值和K值高灵敏的特点, 作者设计了 ensemble 和 iterative refinement 方法予以解决.
前者就是多个聚类模型 ensemble, 后者就是先在基于 MFCC 的聚类标签上进行学习, 学习到一定程度时, 在模型学习到的表征重新聚类, 再做一次 BERT 的学习.

HuBERT的模型架构
既然对比学习可以学习语音的语义表征, BERT的MLM也可以, 那将二者结合起来, 会不会有互补的效果呢? w2v-BERT[3]做的就是这个事情.
注意到：HuBERT中语音的离散token不是端到端获得的, 需要用KMeans算法对特征进行离线聚类, 而wav2vec 2.0又正好提供了音频帧的量化离散表征, HuBERT和wav2vec 2.0很容易就能缝合在一起.
缝合的方法也是显然的：前面若干层做类似wav2vec 2.0的对比学习, 学习出HuBERT要用的离散表征, 然后在后面若干层做类似HuBERT的MLM训练.

w2v-BERT的模型架构
语音的声学token
上一部分介绍的预训练模型做的是上下文关系的预训练, 学习到的表征主要包含与上下文相关的语义信息.



要想将语音的token还原成为真正具有真人表现力的信号, 还需要有包含音色, 韵律, 语速等副语言信息的声学特征.
声学特征的学习在很大程度上参考了图像领域的工作, 用到的主要是类似于VQVAE[4], VQGAN等的离散化压缩方法, 并针对语音数据的特性做了优化.
这一部分比较经典的工作就是SoundStream[5]和Encodec[6], 二者的工作高度类似, 我们放在一起来看.


说到压缩, 最先想到的模型当然就是AutoEncoder(自编码器).
为提升压缩效率, 有利于数字传输和存储, 以及离散化建模的要求, 压缩模型中还需要包含量化(quantization), 将连续的音频信号转换为离散的数值.
基于上述考虑, 模型大体上应该是VQVAE[4]的结构.
为了平衡VQ(Vector Quantization, 向量量化)与音频实时高保真传输的矛盾, 通常采用多个残差连接的codebook来进行量化, 这个就是所谓的RVQ(具体过程可以参见知乎文章).
采用RVQ的好处主要有两个：其一, 区分不同quantization block的分工, 第一个block包含最重要的语义信息, 后续的block包含还原语音的副语言信息；第二, 模型训练时可随机采样前面若干个block来训练, 保持一定精度, 实现对比特率的动态适应.


总而言之, SoundStream[5]/Encodec[6]其实就是一个RVQ-VAE, 它们所建模的语音离散化token包含了层次化的语义信息和声学信息.



Encodec的模型结构
语音的统一表征? 
不难发现, 虽然说SoundStream[5]和Encodec[6]这样的基于RVQ-VAE的压缩建模方法包含了语音的声学特征, 但其中也不可避免地带入了语义特征.
二者提取的实际上更像是一种语义特征和声学特征的混合体.
基于此, SpeechTokenizer[7]在二者的基础上, 引入了语义引导信息来解耦语义特征和声学特征.
语义特征和声学特征的解耦对于最终的语音合成有着相当的重要性.
SpeechTokenizer的具体做法是：使用HuBERT[2]的特征对RVQ1的特征做语义蒸馏, 其余部分保留声学信息.



SpeechTokenizer的模型架构
语音的其他表征
上述的语音离散表征, 不管是基于HuBERT[2]的语义token, 还是基于Encodec[6]的声学token, 它们都是直接基于原始的音频波形抽取的.
除此之外, 也可以基于语音的中间表征来抽取.
最典型的语音中间表征就是梅尔谱(MEL spectrogram, 下文简称MEL).
梅尔谱本身就对语音进行了压缩, 将梅尔谱类比于图像, 使用单码本的VQ也可以达到与SoundStream和Encodec那样类似的压缩程度.
这种MEL+VQ的做法在各种语音合成模型中也相当常见.


让LLM理解语音token! 
有了上面所说的语义token和声学token之后, 其实就可以利用它们来构建语音层面的语言模型了.
比较经典的工作有：谷歌的AudioLM[8]和AudioPaLM[9], 字节的SALMONN[10], 复旦的SpeechGPT[11]/SpeechGPT-Gen[12]/SpeechAlign[13], 阿里的LauraGPT[14]和新加坡国立大学的NextGPT[15].
它们的做法其实都大差不差, 我们看几个就知道是怎么回事了.


AudioLM：最初的SLM
见名知义, AudioLM[8]构建的是语音层面的语言模型——给定一段语音, 模型预测后续的语音.
输入侧和输出侧都只有语音模态.
这个任务形式和GPT-4o非常类似, 不会经历ASR->LM->TTS的过程, 直接从语音上下文中推理语义信息, 再结合声学信息合成贴合上下文的高表现力语音.
而上文所述的语义token和声学token正好就能满足这个任务的要求.


AudioLM的具体做法是：用SoundStream[5]提取声学token, 用w2v-BERT[3]提取语义token, 模型主体就是一个常规的GPT, 词表包含所有的声学token和语义token.
它的建模过程也相当有意思, 有很大的参考意义：先做最重要的语义建模, 然后先预测SoundStream的前若干层特征, 建模粗糙的声学特征, 在预测SoundStream的剩余层特征, 建模声音的细节信息, 最后基于所有的声学token还原为语音.
这种层次化的建模在诸如VALL-E[16]这样的语音合成模型中也非常常见.



AudioLM的tokenizer

AudioLM的建模过程
当然, AudioLM[8]仅仅关注语音模态, LM也很常规, 不具备如同GPT-4o一样强悍的指令遵循能力和对话能力, 语音对话的连贯性和表现力都相当弱.
但这一工作仍然具有相当的启发性和开拓性, 证明了：即使是常规的LM, 照样也能理解语音token.


AudioPaLM[9]：整合LLM
这个就是AudioLM的后续了, 谷歌将常规的LM替换成已经训练好的, 具有强大文本理解能力和生成能力的大语言模型——PaLM-2[17], 既继承了AudioLM保留副语言的能力, 又融合了PaLM-2强大的语义理解能力和推理能力.
而且, 该模型的词表同时包含大语言模型的token和语音token, 可以同时做语音理解任务和合成生成任务, 第一将这些任务整合在一个模型中进行解决.


不过, 需要指出地是, 文中的语音token embedding是直接输入到Transformer中的, 并没有使用音频编码器做一次转换.
而且, AudioPaLM的训练更加接近文本多任务的T5, 并未用到复杂的, 丰富多样的指令来表达任务的意图, 还不能算是真正严格的instruction fine-tuning.



AudioPaLM的模型架构
SALMONN[10]：让LLM理解语音
这是字节跳动和清华大学电子系(也是我们实验室)的合作成果.
虽然这个工作的目的是让LLM能够理解语音, 还不能生成语音, 但它的训练方法和LLM比较接近, 而且在诸多语音相关的任务上都显示出了涌现性, 可以用作universal的特征提取器, 这对于构建高质量的, 包含语音-文本多模态的指令微调数据集具有相当大的意义.



SALMONN的模型架构
SpeechGPT/SpeechGPT-Gen/SpeechAlign：向LLM的训练方法看齐
这算是复旦大学邱锡鹏组在这个领域一个成系列的工作, 我们一个一个来看.


SpeechGPT[11]做的也是兼具语音理解能力和语音生成能力的多模态模型.
在模型的训练上, SpeechGPT大幅度向LLM看齐, 使用了三段式的训练方法：第一阶段先做模态适应的预训练, 其实就是拿ASR的语音数据来做预训练；第二阶段和第三阶段都是指令微调, 不过根据指令模态的不同, 细分为了跨模态的指令微调和模态链指令微调.
指令微调的数据集都是来自ASR数据集.
描述任务需求的指令由GPT-4生成.


在我看来, 这个工作还是相当偏学术化的作品, 文中有不少点都有值得商榷的地方：第一, 语音的离散化仅仅用了HuBERT[2], 模型只能看到语音的语义特征, 这对模型合成语音的音质和表现力有非常大的影响, demo的语音也验证了我的判断；第二, 指令微调数据集的构造上有问题.
他们用的是ASR数据集, 其实更好的选择应该是TTS数据集, 可惜高质量的TTS数据集实在是太少了.
ASR数据集中的文本和语音可能并不是严格对齐的, GPT-4产生的meta-prompt和语音本身的特征也有可能是对不上的, 比如prompt要求大声朗读, 但语音本身可能是特定低沉的.
meta-prompt本身就无法做到足够复杂丰富, 不能描述到语音的一些细粒度信息.
这一部分, 最好要有像诸如SALMONN[10]这样的多模态语音理解模型的介入, 像DALLE3一样丰富指令的多样性.
至于语音方面, 可以考虑引入zero-shot的语音合成模型或者变声模型来做合成数据.
第三, 文中的训练方法也没有与人类偏好做对齐.



SpeechGPT的模型架构
对于上面的第一个问题, 作者在其后的SpeechGPT-Gen[12]中做了解决.
解决思路的核心点就是：让模型不仅看到语音的语义token, 也要看到语音的声学token.
具体做法是：SpeechGPT的HuBERT特征替换成了SpeechTokenizer[7]中的语义特征, 用SpeechGPT这一LLM来自回归地建模语义特征, 有了语义特征之后, 再使用Flow-Matching这样的扩散模型来建模声学特征.
这里选用Flow-Matching扩散模型, 可能是受了SD3和Voicebox/Audiobox的影响.
为了增强两阶段建模的依赖关系, 作者将语义特征的先验信息注入到第二阶段扩散模型的先验分布中.
可以看到, 这里语音的解码其实也是一种层次化渐进式解码.



SpeechGPT-Gen的模型架构
SpeechAlign[13]做的则是SLM与人类偏好的对齐, 彻底地向LLM的训练方法看齐.
该工作构建了对比gold token和合成token的encodec数据集, 然后进行偏好优化来进行改进.
使用的偏好优化方法包括RLHF和Chain of Hindsight.



SpeechAlign的流程图
简单总结一下上面这些工作中值得关注的点：

要想让LLM输出上下文连贯的高表现力语音, 必须要让LLM看到语义token和声学token, 只有语义token, 那语音就会显得呆板机械, 只有声学token, 那语音就不知所云；
LLM的指令微调同样可以迁移到语音-文本多模态领域中, LLM的指令微调同样可以带来如同NLP一样的涌现性；
高质量指令微调数据集的构建应该是最大的瓶颈! 一下子让LLM同时做语音理解和语音生成, 难度非常大.
不如分步进行.

如果要分步进行的话, 要先实现一个类似于SALMONN[10]那样的多模态理解模型和一个强大的Zero-shot TTS模型.
前者用于给语音数据打上丰富的标签, 可以是情感情绪, 韵律, 音高, 语速, 也可以是口音, 意图和说话环境；后者则用于生成高质量的语音数据.
毕竟, 高质量的, 文本和语音严格对齐的TTS数据实在是太少了, 尤其是中文领域.
有了这两个模型的加持, 我们其实就能够构造出高质量的指令微调数据集.
我不知道OpenAI是否有SALMONN这样的模型, 但OpenAI的OpenVoice模型应该足够为其提供高质量的语音数据了.

既然我们在上面的篇幅中论述了语音理解多模态模型的构建, 那我们在下一部分就重点关注zero-shot TTS模型, 它对高质量指令微调数据集的构建同样至关重要.
同时, LLM解码语音的方法也能从zero-shot TTS方案中得到不少的启发.


LLM如何合成语音：Zero-shot TTS
前面说到, SLM词表中包含了语音的语义token和声学token.
语义token保证生成语音与对话上下文的连贯性, 声学token保证了合成语音的质量和表现力.
要想做到合成上下文连贯的高自然度语音, 有两个问题必须要解决：

语音既有语义token, 又有声学token, 应该要如何解码成语音? 
SLM在合成语音的过程中是否能够遵循多轮对话中的文本指令和语音指令? 这个很重要! 这允许模型根据用户的即时要求来生成语音回复.
比如说, OpenAI演示视频中出现的：“将语速提高两倍”, “采用更加机械化的语气”这样的要求.

对于第一个问题, 以VALL-E[16]为代表的诸多zero-shot TTS模型给出了不同的解决方案, 这些方案虽有不同, 但也有不可忽视的共同点；对于第二个问题, 以VoiceLDM[18]和ParlerTTS[19]为代表的text/prompt-guided zero-shot TTS工作给出了肯定的答案.
简单解释一下text/prompt-guided zero-shot TTS是怎么回事, 通常的语音合成就是将文本(transcription)转换成声音, 该任务在transcription之外, 又增加了description的输入, 来描述合成语音的情感情绪, 口音, 语气, 语速, 音高, 说话环境, 氛围等等信息.
我们逐个来看这些工作.


Zero-shot TTS
2023年以来, 学术界和工业界出了不少具备in-context learning(zero-shot/few-shot)能力的TTS模型.
这些TTS模型通常会将低信息密度, 长序列的连续语音数据压缩为高信息密度的tokens或者latents(其实就是码本中具体的token embedding).
这些模型本质上做的事情就是：如何高效实现语音tokens/latents到音频波形的映射.


这些模型给出的解决方案基本上都遵循一个准则：语义token和声学token层次化解码, 先语义后声学, 或者先解码成MEL再后接声码器, 并且非必要不做自回归(毕竟自回归上线虽高, 但太吃数据了)! 我们一个个来看.


基于声学token或语义token的工作

先是微软的VALL-E[16].
这是zero-shot TTS的开山之作, 首次在TTS任务上采用了上万小时的数据.
它采用Encodec将语音转换为离散的token, 然后用GPT在token上做语言模型的任务.
但是, 语音毕竟不是文本, 如果直接在语音的所有特征上都做自回归的话, 那训练的成本会相当高.
考虑到Encodec RVQ特征的层次性, 低层特征表示语义内容这样的重要特征, 高层特征则表征声学细节.
前者具有比较强的上下文依赖关系, 适合用自回归来建模, 后者诸如音色这样的特征, 具有全局性, 用非自回归特征也可以搞定, 所以就有了VALLE中自回归+非自回归的层次建模方式.



VALL-E的模型架构
尽管VALL-E[16]在用GPT建模token的上下文关系的时候, 基于token的层次化特性做了分治处理, 可能是限于当前语音数据集的规模(几万小时可能不够), 这种GPT自回归的难度还是相当大的, 解码过程存在常见的错误传播现象, 鲁棒性非常差, 极其不稳定.
根据Ilya Sutskever此前对于自回归的论述, GPT自回归相比于BERT这种双向结构是非常data-hungry的, 万小时的数据可能不够.
根据本人以及一些同行的经验, VALL-E模型这一类的自回归模型, 也包括tortoise-tts[20]和xtts v2, 要想显出威力, 至少要有十几万小时的数据才行.


既然GPT自回归的难度这么大, 就有不少人想方设法地来降低GPT学习的难度了.
他们的解决方案也非常类似：给GPT提供额外的条件信息不就行了.
比较典型的工作就是微软的RALL-E[21]和吉利的HAM-TTS[22].
RALL-E先生成了时长信息和音高信息, 作为GPT自回归的先验, 之所以会补充时长和音高, 这大概是受到FastSpeech2[23]这样的非自回归模型的启发, 这两个指标的引入, 有助于提升合成的鲁棒性；HAM-TTS则是补充了基于HuBERT的语义信息.
值得注意地是, HAM-TTS将模型的训练数据扩充到了65万小时, 其中有50万小时的数据是合成数据.
合成数据也能大幅度提升合成语音的音质.



RALL-E的模型架构

HAM-TTS的模型架构
说到VALL-E的后续改进, VoiceCraft不得不提.
我愿意称之为“优雅的VALL-E”.
它的优雅主要体现在两个方面：casual masking和delayed stacking.
所谓的causal masking, 是为了用自回归GPT架构来做语音编辑任务, 就是把被mask的部分移动到序列末尾去预测, 一套架构同时做合成和编辑任务；所谓的delay stacking, 是为了适配自回归和RVQ, 通过delay错位让当前码本的token预测正好可以利用前面那些token的预测结果, 比起VALL-E那样自回归和非自回归缝合在一起的结构要优雅不少.



VoiceCraft的建模流程
我们通常所说的语音token是离散的.
如果使用对应码本中的embedding来表示语音的话, 它也可以是连续的低维度的latent变量.
既然是低维度的连续latent变量, 那图像合成领域中大火的LDM(latent diffusion model, 其实就是stable diffsion 1&2采用的模型)模型[]自然也可以用到语音的合成上.
这方面的经典工作有很多, 比如说：NaturalSpeech 2&3[25, 26], AudioLDM 2[27], VoiceLDM[18].
但这里面只有NaturalSpeech2用到了语音离散化部分提及的声学/语义token, NaturalSpeech3的属性分解形式的VQ更像是另一种形式的RVQ.
我们先来看NaturalSpeech 2&3, 其他的工作后面再来看.


首先是NaturalSpeech 2[26], 它基本上就是VALL-E的连续版本.
它用的latent也是来自Encodec, 对其中不同层次的latent做了求和, 然后将其作为扩散模型的训练目标.
值得注意地是, 扩散模型和FastSpeech2一样也用了时长和音高作为合成的先验条件.
这一点也被后来的RALL-E采用.
该工作中的扩散模型采用WaveNet实现, 同时预测不加噪的latent和后验均值, 和图像合成领域的扩散模型在实现方式上还是有所不同的.



NaturalSpeech 2的模型架构
然后是NaturalSpeech 3[26], 还是非自回归的, 而且非自回归的正统性味道更加浓厚, 借用了不少FastSpeech2和megatts1&2(后面会讲)[27, 28]的设计思想.
像megatts 1&2一样, 同样采用(自)监督信号对语音token编码的内容做了限制, 而不再像是VALL-E/NaturalSpeech2那样一把抓.
相应地, 语音token化的方法也用VQ就行.
具体而言, 文章将语音信号分解为时长, 内容, 韵律和细节四个部分, 然后每个部分用离散化的扩散模型来建模.
不过, 原文使用GRL来促进语音属性的分解, 这一点的靠谱程度存疑.
我也尝试过文章的FACodec, 但效果很差.
三级扩散模型级联的结构, 预测起来似乎也非常麻烦.



NS3的模型架构
基于MEL谱+VQ的TOKEN的工作

当然, 也有不少工作用了MEL谱作为中间特征, 然后在梅尔谱的基础上, 或是用VQ提供离散token, 或是用CNN来提取连续latent.
对于MEL+VQ的工作, 有tortoise-tts[20], xtts 1&2, megatts1&2[28, 29], base TTS[30].
对于MEL+latents的工作, 有：AudioLDM 1&2[27], StyleTTS 1&2[31, 32].
我们来简单看看是它们是怎么做的.


Tortoise-tts[20].
该工作是著名的开源英文TTS模型.
其作者目前在OpenAI就职, 同时也是GPT-4o的重要Contributor(他自个儿在博客中说的).
Tortoise-tts使用MEL+VQVAE的方法得到语音的MEL token, 然后对MEL token以及text token做GPT自回归建模.
对于语音的解码, 自然也是分为两步：先是用扩散模型将MEL token转换为MEL谱, 这一步和文生图很像, 用扩散模型是很自然的选择；然后用声码器将MEL谱转换为音频波形.
tortoise-tts和VALL-E的主体都是自回归建模, 二者的不同主要在于token的不同.



tortoise-tts的模型架构
MegaTTS 1&2[28, 29].
字节跳动的MegaTTS系列对语音token编码信息做了显式的信息压缩处理, 让语音token仅编码上下文依赖强的韵律信息, 然后用GPT自回归来建模语音的韵律.
对于其他方面的信息, 模型的处理显得较为常规：音色一般具有全局性, 使用单一的音色编码器从参考音频中提取就性；对于文本语义内容的处理, 模型在很大程度上参考了非自回归的FastSpeech 2.
对于语音的解码, 也是分为两步：先通过MEL decoder还原为MEL谱, 然后通过声码器解码为音频波形.
MegaTTS 2和1总体上类似, 在音色编码(音素级编码, 多条参考音频), 语音提示长度(扩展同speaker语音上下文长度硬train, 音频prompt长度更长)和时长建模(也用GPT自回归)上做了改进, 同时堆了更大规模的数据.
剪映的后端TTS模型用的就是megatts2.
该工作在各论文的评测中表现也都不错.



megatts1的模型架构
基于MEL谱+VAE的工作

AudioLDM 1&2[27].
AudioLDM 1&2使用的语音latents是一致的, 均通过MEL+VAE获得.
既然是连续的latents, 使用扩散模型来建模也合情合理.
解码过程也相当简单：VAE decoder获得梅尔谱, 然后用声码器转换为音频波形.
该系列工作的核心创新点是利用多模态模型统一了扩散模型条件输入侧的信息：AudioLDM 1用CLAP统一了文本模态和音频模态, 用单模态的音频数据就能完成模型的训练；AudioLDM 2则包含了图像, 文本, 转录文本等更多模态, 模型泛用性也更强, 既能做语音合成, 也能做音乐生成, 音频事件生成.



AudioLDM 1的模型架构

AudioLDM 2的模型架构
StyleTTS 1&2[31, 32].
StyleTTS系列的模型一众zero-shot TTS模型显得比较老派, 整体结构基本上沿袭了非自回归的FastSpeech 2, 不同之处在于增加了基于参考音频抽取的风格信息.
说是风格, 其实跟megatts的音色很像.
StyleTTS 2的工作则将风格进一步拆分成声学风格和韵律风格.
训练时的风格信息由音频提供, 推断时的风格信息则由扩散模型提供.
StyleTTS 2通过一个扩散模型桥接了文本韵律和语音风格之间的联系, 摆脱推断时对参考音频的依赖.
不用参考音频其实对产品的意义还挺大的, 要都用现实世界中真人尤其是名人的声音作为参考音频, 那这势必会引起版权纠纷.
这种纠纷在国内国外都有相关的事件.
最近寡姐投诉OpenAI的事件就是一例.



StyleTTS 1的模型架构

StyleTTS 2的模型架构
TTS对指令的遵循
SLM不仅要合成合乎上下文语义的高表现力语音, 合成的语音还要符合用户的即时要求.
一些text-guided zero-shot TTS的工作值得参考.
这些工作一般都是在已有的zero-shot TTS模型或者text-to-audio模型上改造而来, 同时吸收transcription和description两路条件.
其中的重点还是在于数据集的构建.
这方面的工作有：PromptTTS[33], InstructTTS[34], ParlerTTS[19], VoiceLDM[18]和Audiobox[35].
我们主要谈谈ParlerTTS和VoiceLDM.


ParlerTTS[19].
VALL-E/VoiceCraft的增强版, 通过T5编码器和cross-attention旁路引入了描述性文本的信息.
该工作的目的是想使用自然语言prompt来指定说话风格和环境信息, 摆脱对参考音频的依赖.
描述性标签文本的收集过程也显得相当朴素：通过定制化的监督式模型获取语音数据的口音特征, 录音质量特征, 音高语速特征.
然后用LLM将这些特征转换为自然语言的描述.
在我看来, 这个工作有这么几点局限性吧：其一, 缺乏情绪标签；其二, 语音描述性标签的收集并不具备通用性, 较为繁琐, 远不如一个强大的多模态语音理解模型来得实在.
文章demo虽然达到了预期的效果, 但场景似乎局限在朗读的情景中.



ParlerTTS的模型架构
VoiceLDM[18].
在VoiceLDM1的基础上增加了转录文本的输入.
这个工作和AudioLDM 1很像, 同样使用CLAP注入语音的描述性信息.
不同地是, 为了做TTS任务, 该工作通过cross-attention旁路增加了transcription的信息.



VoiceLDM的模型架构
TTS总结
林林总总说了这么多zero-shot的TTS方法, 我想说明的结论有这么几点：

在LLM大行其道, scaling law大显神威的时代, TTS模型的训练数据规模已经突破了万小时, 甚至达到了数十万小时的级别.
在大数据的加持下, TTS任务上也涌现出了in-context learning能力.

语音信息的解码通常都要层次化或者多步进行, 不能一步到位.
自回归, 扩散模型和流匹配都能在TTS中发挥作用；
借鉴NLP instruction fine-tuning和文生图的经验, TTS模型同样可以遵循文本指令或者语音指令, 合成符合用户即时要求的语音, 摆脱对参考音频的依赖, 这或许也能规避一些知识产权的困扰(比如最近有名的寡姐投诉OpenAI事件).
同时, 用户也能在对话过程中随时切换语音回复的风格, 这一点在OpenAI的demo中有很明确的体现.
另外, 不知道大家有没有注意, GPT-4o合成的语音是可以是放映所处的声学环境的：有一段语音背后似乎是有钢琴声的.

text-guided zero-shot TTS在模型架构上和zero-shot TTS有非常大的相似性.
但训练数据可能较为缺乏.
先开发zero-shot TTS, 再用类似SALMONN那样的多模态理解模型来打标签(类似DALLE3的做法), 这样数据集构造方式, 可能会是更好的选择.

另外, 对于语音的解码方案, 我倾向于是这样的：

如果要做流式推理, 外接类似HIFIGAN这样的声码器的方式可能不是好的选择.
HIFIGAN并不天然支持流式解码.
相反地, 诸如SoundStream和Encodec这样的方法, 同时有流式变体和非流式变体；
先做语义token的解码, 这个解码大概率是自回归解码.
语义token毕竟是建模上下文依赖关系, 自回归方法已经在NLP上证明了这一点；
然后做声学token的解码, 扩散或者flow-matching可能是更好的选择.
扩散模型或者流匹配可以很好地修补语音的细节；
当然, 除了上面讲到的, zero-shot TTS还有很多值得研究的方法.
限于篇幅, 仅列举于此, 不再详述：HierSpeech++[36], base TTS[30], Voicebox/Audiobox[35], UniAudio[37], Make-a-Voice[38]等等.


其他问题
对于GPT-4o模型, 如果仅仅聚焦于语音多模态, 还有下面的问题值得关注：

语音交互如何做到低延迟? 大概率要求流式切片处理, 主要工作在于工程优化, 用C++重写算子.
推理框架的话, 用tensorrt, mnn这些都行.
上下文所述的音频离散化方法, 诸如SoundStream和Encodec, 其实也支持流式处理.


语音对话中的打断如何实现? 个人认为有两种可能的方案：turn-based和流式处理.
所谓的turn-based方案, 是比较工程化的, 简答概括一下就是：检测是否有停顿, 如果一段时间内没有声音, 模型就开始返回语音回复.
另一种流式方案, 则是：模型一直在接受用户的流式语音输入, 判断是否应该输出语音回复, 一个充分训练的模型应该是能够准确预测出语音词表中的[START]和[END]的.


全文总结
总结一下本文说谈的内容, 我认为GPT-4o语音多模态的实现可能是走了以下的技术路线：

audio & text tokenizer的实现应该是语音离散化部分所用的技术, 例如SoundStream, Encodec, SpeechTokenizer, 或者是MEL+VQ最后配合声码器来解码；参考zero-shot TTS, AudioLM/AudioPaLM, SpeechGPT-Gen等工作的结果, LLM中语音token的解码应该是要走层次化或者多步的方法, 先解码语义特征, 再解码声学特征, 或者是先解码MEL, 再加一个HIFIGAN这样的声码器.
另外, 如果做audio/speech/music这样的通用声合成的话, 可能也能通过prompt来控制.
AudioLDM2虽然做了这方面的工作, 但audio/music和speech的参数其实是不一样的, 说到底还不是同一个模型.

对于指令微调, 数据集的构造非常重要, 大概率要用到合成数据.
其一, 网络上高质量语音数据的量级远远不及文本, 直接拿ASR数据来做肯定会影响模型合成语音的音质；其二, 大语言模型合成的instruction往往触及不到语音的细粒度特征, 这样的instruction其实无法准确详尽地描述text和speech之间的关系.
因而, 需要引入强大的zero-shot TTS模型合成高质量语音, 然后用多模态语音理解模型来为合成语音打标签, 当然也可以评分做筛选什么的.

最后是要让大模型的输出对齐人类的偏好.
这方面的方法有很多, 有DPO, PPO什么的, 都可以用.
