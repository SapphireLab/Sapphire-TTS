# SapphireTTS

## Survey

|时间|发表|标题|
|:-:|---|---|
|2021.06|ArXiv|[A Survey on Neural Speech Synthesis]()|
|2024.02.12|(Springer) EURASIP Journal on Audio, Speech, and Music Processing|[Deep Learning-Based Expressive Speech Synthesis - A Systematic Review of Approaches, Challenges, and Resources](Survey/DL-Based_Expressive_Speech_Synthesis.md)|

## Papers

|时间|发表|标题|代码|
|:-:|---|---|:-:|
|2017.03.29(v1)<br>2017.04.06(v2)|[ArXiv](https://arxiv.org/abs/1703.10135)<br>[InterSpeech](https://doi.org/10.21437/Interspeech.2017-1452)|[Tacotron: Towards End-to-End Speech Synthesis](Papers/2017.03_Tacotron.md)
|2017.12.16(v1)<br>2017.02.16(v2)|[ArXiv](https://arxiv.org/abs/1712.05884)<br>[IEEE-ICASSP](https://doi.org/10.1109/ICASSP.2018.8461368)|[Tacotron 2: Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions](Papers/2017.12_Tacotron2.md)|
|2019.05.22(v1)<br>2019.11.20(v5)|[ArXiv]()<br>[NeurIPS](https://proceedings.neurips.cc/paper_files/paper/2019/hash/f63f65b503e22cb970527f23c9ad7db1-Abstract.html)|[FastSpeech: Fast, Robust and Controllable Text to Speech](Papers/2019.05_FastSpeech.md)|
|2020.06.08(v1)<br>2022.08.08(v8)|[ArXiv](https://arxiv.org/abs/2006.04558)|[FastSpeech2: Fast and High-Quality End-to-End Text-to-Speech](Papers/2020.06_FastSpeech2.md)|
|2021.04.01(v1)<br>2022.10.06(v2)|[ArXiv](https://arxiv.org/abs/2104.00436)|[ST-TTS: Expressive Text-to-Speech using Style Tag](Papers/2021.04_ST-TTS.md)|
|2021.06.11(v1)|[ArXiv](https://arxiv.org/abs/2106.06103)<br>[ICML](https://proceedings.mlr.press/v139/kim21f.html)|[VITS: Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech](Papers/2021.06_VITS.md)|[Official](https://github.com/jaywalnut310/vits)|
|2021.12.04(v1)<br>2023.04.30(v4)|[ArXiv](https://arxiv.org/abs/2112.02418)<br>[ICML](https://proceedings.mlr.press/v162/casanova22a.html)|[YourTTS: Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice Conversion for Everyone](Papers/2021.12_YourTTS.md)|
|2022.11.30|[MDPI-Appl. Sci.](https://www.mdpi.com/2076-3417/13/4/2225)|[Emo-VITS: An Emotion Speech Synthesis Method Based on VITS](Papers/2022.11_Emo-VITS.md)|
|2023.01.05(v1)|[ArXiv](https://arxiv.org/abs/2301.02111)|[VALL-E: Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers](Papers/2023.01_VALL-E.md)|
|2023.01.31(v1)<br>2023.06.25(v2)|[ArXiv](https://arxiv.org/abs/2301.13662)|[InstructTTS: Modelling Expressive TTS in Discrete Latent Space with Natural Language Style Prompt](Papers/2023.01_InstructTTS.md)|
|2023.03.07(v1)|[ArXiv](https://arxiv.org/abs/2303.03926)|[VALL-E X: Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling](Papers/2023.03_VALL-E_X.md)|
|2023.04.18(v1)<br>2023.05.30(v3)|[ArXiv](https://arxiv.org/abs/2304.09116)|[NaturalSpeech 2: Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers](Papers/2023.04_NaturalSpeech2.md)|
|2023.05.16(v1)|[ArXiv](https://arxiv.org/abs/2305.09636)|[SoundStorm: Efficient Parallel Audio Generation](Papers/2023.05_SoundStorm.md)|[_Unofficial](https://github.com/yangdongchao/SoundStorm/)
|2023.06.14|[Blog](https://coqui.ai/blog/tts/xtts_taking_tts_to_the_next_level)|[XTTS: Taking TTS to the Next Level](Papers/2023.06_XTTS.md)|
|2023.06.23(v1)<br>2023.10.19(v2)|[Arxiv](https://arxiv.org/abs/2306.15687)<br>[NeurIPS](https://proceedings.neurips.cc/paper_files/paper/2023/hash/2d8911db9ecedf866015091b28946e15-Abstract-Conference.html)|[VoiceBox: Text-Guided Multilingual Universal Speech Generation at Scale](Papers/2023.06_VoiceBox.md)|
|2023.07.31(v1)|[ArXiv](https://arxiv.org/abs/2307.16430)|[VITS2: Improving Quality and Efficiency of Single-Stage Text-to-Speech with Adversarial Learning and Architecture Design](Papers/2023.07_VITS2.md)|
|2023.07.31(v1)|[ArXiv](https://arxiv.org/abs/2307.16549)|[DiffProsody: Diffusion-based Latent Prosody Generation for Expressive Speech Synthesis with Prosody Conditional Adversarial Training](Papers/2023.07_DiffProsody.md)|
|2023.11.21(v1)<br>2023.11.27(v2)|[ArXiv](https://arxiv.org/abs/2311.12454)|[HierSpeech++: Bridging the Gap between Semantic and Acoustic Representation of Speech by Hierarchical Variational Inference for Zero-shot Speech Synthesis](Papers/2023.11_HierSpeechpp.md)|[Official](https://github.com/sh-lee-prml/HierSpeechpp)|
|2023.12.12(v1)<br>2024.01.02(v5)|[ArXiv](https://arxiv.org/abs/2312.01479)|[OpenVoice: Versatile Instant Voice Cloning](Papers/2023.12_OpenVoice.md)|[Official](https://github.com/myshell-ai/OpenVoice)|
|2024.02.14(v1)|[ArXiv](https://arxiv.org/abs/2402.09378)|[MobileSpeech: A Fast and High-Fidelity Framework for Mobile Zero-Shot Text-to-Speech](Papers/2024.02_MobileSpeech.md)|
|2024.03.05(v1)|[ArXiv](https://arxiv.org/abs/2403.03100)|[NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models](Papers/2024.03_NaturalSpeech3.md)|
|2024.03.25(v1)|[ArXiv](https://arxiv.org/abs/2403.16973)|[VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild](Papers/2024.03_VoiceCraft.md)|[Official](https://github.com/jasonppy/VoiceCraft)|

