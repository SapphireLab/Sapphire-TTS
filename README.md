# SapphireTTS

## Survey

|时间|发表|标题|
|:-:|---|---|
|2024.02.12|(Springer) EURASIP Journal on Audio, Speech, and Music Processing|[Deep Learning-Based Expressive Speech Synthesis - A Systematic Review of Approaches, Challenges, and Resources](Survey/DL-Based_Expressive_Speech_Synthesis.md)|

## Papers

|时间|发表|标题|代码|
|:-:|---|---|:-:|
|2017.03.29 (v1)<br>2017.04.06 (v2)|[ArXiv](https://arxiv.org/abs/1703.10135)<br>[InterSpeech](https://doi.org/10.21437/Interspeech.2017-1452)|[Tacotron: Towards End-to-End Speech Synthesis](Papers/2017.03_Tacotron.md)
|2017.12.16 (v1)<br>2017.02.16 (v2)|[ArXiv](https://arxiv.org/abs/1712.05884)<br>[IEEE-ICASSP](https://doi.org/10.1109/ICASSP.2018.8461368)|[Tacotron 2: Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions](Papers/2017.12_Tacotron2.md)|
|2019.05.22 (v1)<br>2019.11.20 (v5)|[ArXiv]()<br>[NeuIPS](https://proceedings.neurips.cc/paper_files/paper/2019/hash/f63f65b503e22cb970527f23c9ad7db1-Abstract.html)|[FastSpeech: Fast, Robust and Controllable Text to Speech](Papers/2019.05_FastSpeech.md)|
|2020.06.08 (v1)<br>2022.08.08 (v8)|[ArXiv](https://arxiv.org/abs/2006.04558)|[FastSpeech2: Fast and High-Quality End-to-End Text-to-Speech](Papers/2020.06_FastSpeech2.md)|
|2023.01.05 (v1)|[ArXiv](https://arxiv.org/abs/2301.02111)|[VALL-E: Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers](Papers/2023.01_VALL-E.md)|
|2023.01.31 (v1)<br>2023.06.25 (v2)|[ArXiv](https://arxiv.org/abs/2301.13662)|[InstructTTS: Modelling Expressive TTS in Discrete Latent Space with Natural Language Style Prompt](Papers/2023.01_InstructTTS.md)|
|2023.04.18 (v1)<br>2023.05.30 (v3)|[ArXiv](https://arxiv.org/abs/2304.09116)|[NaturalSpeech 2: Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers](Papers/2023.04_NaturalSpeech2.md)|
|2023.06.14|[Blog](https://coqui.ai/blog/tts/xtts_taking_tts_to_the_next_level)|[XTTS: Taking TTS to the Next Level](Papers/2023.06_XTTS.md)|
|2023.06.23|[NeuIPS2023](https://proceedings.neurips.cc/paper_files/paper/2023/hash/2d8911db9ecedf866015091b28946e15-Abstract-Conference.html)<br>[Arxiv](https://arxiv.org/abs/2306.15687)|[VoiceBox: Text-Guided Multilingual Universal Speech Generation at Scale]()|
|2023.11.21 (v1)<br>2023.11.27 (v2)|[ArXiv](https://arxiv.org/abs/2311.12454)|[HierSpeech++: Bridging the Gap between Semantic and Acoustic Representation of Speech by Hierarchical Variational Inference for Zero-shot Speech Synthesis](Papers/2023.11_HierSpeechpp.md)|[HierSpeechpp](https://github.com/sh-lee-prml/HierSpeechpp)|
|2023.12.12 (v1)<br>2024.01.02 (v5)|[ArXiv](https://arxiv.org/abs/2312.01479)|[OpenVoice: Versatile Instant Voice Cloning](Papers/2023.12_OpenVoice.md)|[OpenVoice](https://github.com/myshell-ai/OpenVoice)|
|2024.03.05 (v1)|[ArXiv](https://arxiv.org/abs/2403.03100)|[NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models](Papers/2024.03_NaturalSpeech3.md)|