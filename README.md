# SapphireTTS

## Books

|时间|标题|作者|
|:-:|---|---|
|2023.05|[Neural Text-to-Speech Synthesis](Books/2023.05_Neural_TTS_Synthesis/Ch00.ToC.md)|谭旭 (Microsoft Research Asia)|

## Surveys

|时间|发表|标题|
|:-:|---|---|
|2021.06|ArXiv|[A Survey on Neural Speech Synthesis]()|
|2024.02.12|(Springer) EURASIP Journal on Audio, Speech, and Music Processing|[Deep Learning-Based Expressive Speech Synthesis - A Systematic Review of Approaches, Challenges, and Resources](Survey/DL-Based_Expressive_Speech_Synthesis.md)|

## Datasets

|时间|发表|标题|链接|机构|
|:-:|---|---|:-:|:-:|
|2019.12(v1)|[ArXiv](https://arxiv.org/abs/1912.07875)<br>[ICASSP](https://doi.org/10.1109/ICASSP40776.2020.9052942)|**Libri-Light**|[Github](https://github.com/facebookresearch/libri-light)|Facebook|

## Models

|                       时间                       | 发表                                                                                                                                                                           | 标题                                                                                                                                                                                               |                                             机构                                             |                                                                  代码                                                                  |
| :--------------------------------------------: | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | :----------------------------------------------------------------------------------------: | :----------------------------------------------------------------------------------------------------------------------------------: |
|        2017.03.29(v1)<br>2017.04.06(v2)        | [ArXiv](https://arxiv.org/abs/1703.10135)<br>[InterSpeech](https://doi.org/10.21437/Interspeech.2017-1452)                                                                   | [**Tacotron**: Towards End-to-End Speech Synthesis](Papers/2017.03_Tacotron.md)                                                                                                                  |                                           Google                                           |                                                                                                                                      |
|        2017.12.16(v1)<br>2017.02.16(v2)        | [ArXiv](https://arxiv.org/abs/1712.05884)<br>[IEEE-ICASSP](https://doi.org/10.1109/ICASSP.2018.8461368)                                                                      | [**Tacotron 2**: Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions](Papers/2017.12_Tacotron2.md)                                                                      |                                           Google                                           |                                                                                                                                      |
|        2018.09.19(v1)<br>2019.01.30(v3)        | [ArXiv](https://arxiv.org/abs/1809.08895)<br>[AAAI](https://doi.org/10.1609/aaai.v33i01.33016706)                                                                            | [**Transformer TTS**: Neural Speech Synthesis with Transformer Network](Papers/2018.09_Transformer_TTS.md)                                                                                       |                                            MSRA                                            |                                                                                                                                      |
|                 2018.10.31(v1)                 | [ArXiv](https://arxiv.org/abs/1811.00002)                                                                                                                                    | [**WaveGlow**: A Flow-based Generative Network for Speech Synthesis](Papers/2018.10_WaveGlow.md)                                                                                                 |                                           Nvidia                                           |                                                                                                                                      |
|        2019.05.22(v1)<br>2019.11.20(v5)        | [ArXiv](https://arxiv.org/abs/1905.09263)<br>[NeurIPS](https://proceedings.neurips.cc/paper_files/paper/2019/hash/f63f65b503e22cb970527f23c9ad7db1-Abstract.html)            | [**FastSpeech**: Fast, Robust and Controllable Text to Speech](Papers/2019.05_FastSpeech.md)                                                                                                     |                          [MSRA](https://speechresearch.github.io)                          |                                                                                                                                      |
|        2020.05.12(v1)<br>2020.07.16(v3)        | [ArXiv](https://arxiv.org/abs/2005.05957)                                                                                                                                    | [**Flowtron**: an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis](Papers/2020.05_Flowtron.md)                                                                         |                                           Nvidia                                           |                                            [Official](https://github.com/NVIDIA/flowtron)                                            |
|        2020.06.08(v1)<br>2022.08.08(v8)        | [ArXiv](https://arxiv.org/abs/2006.04558)                                                                                                                                    | [**FastSpeech2**: Fast and High-Quality End-to-End Text-to-Speech](Papers/2020.06_FastSpeech2.md)                                                                                                |                          [MSRA](https://speechresearch.github.io)                          |                                                                                                                                      |
|                   2021.03.01                   |                                                                                                                                                                              | AdaSpeech: Adaptive Text to Speech for Custom Voice                                                                                                                                              |                          [MSRA](https://speechresearch.github.io)                          |                                                                                                                                      |
|                   2021.03.05                   |                                                                                                                                                                              | AdaSpeech 2: Adaptive Text to Speech with Untranscribed Data                                                                                                                                     |                          [MSRA](https://speechresearch.github.io)                          |                                                                                                                                      |
|                   2021.06.02                   |                                                                                                                                                                              | AdaSpeech 3: Adaptive Text to Speech for Spontaneous Style                                                                                                                                       |                          [MSRA](https://speechresearch.github.io)                          |                                                                                                                                      |
|        2021.04.01(v1)<br>2022.10.06(v2)        | [ArXiv](https://arxiv.org/abs/2104.00436)                                                                                                                                    | [**ST-TTS**: Expressive Text-to-Speech using Style Tag](Papers/2021.04_ST-TTS.md)                                                                                                                |                                         SK Telecom                                         |                                                                                                                                      |
|                 2021.06.11(v1)                 | [ArXiv](https://arxiv.org/abs/2106.06103)<br>[ICML](https://proceedings.mlr.press/v139/kim21f.html)                                                                          | [**VITS**: Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech](Papers/2021.06_VITS.md)                                                                  |                                                                                            |                                           [Official](https://github.com/jaywalnut310/vits)                                           |
|        2021.12.04(v1)<br>2023.04.30(v4)        | [ArXiv](https://arxiv.org/abs/2112.02418)<br>[ICML](https://proceedings.mlr.press/v162/casanova22a.html)                                                                     | [**YourTTS**: Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice Conversion for Everyone](Papers/2021.12_YourTTS.md)                                                                        | [Coqui](https://coqui.ai/blog/tts/yourtts-zero-shot-text-synthesis-low-resource-languages) |                                     [Demo]()<br>[Official](https://github.com/Edresson/YourTTS/)                                     |
| 2022.05.03<br>2022.05.09(v1)<br>2022.05.10(v2) | [ArXiv](https://arxiv.org/abs/2205.04421)                                                                                                                                    | **NaturalSpeech**: End-to-End Text to Speech Synthesis with Human-Level Quality                                                                                                                  |                          [MSRA](https://speechresearch.github.io)                          |                                       [Demo](https://speechresearch.github.io/naturalspeech/)                                        |
|                   2022.10.24                   | [ArXiv](https://arxiv.org/abs/2210.13438)                                                                                                                                    | [**Encodec**: High Fidelity Neural Audio Compression](Papers/2022.10_Encodec.md)                                                                                                                 |                                         Meta FAIR                                          |                                       [Official](https://github.com/facebookresearch/encodec)                                        |
|                   2022.11.22                   |                                                                                                                                                                              | **PromptTTS**: Controllable Text-to-Speech with Text Descriptions                                                                                                                                |                          [MSRA](https://speechresearch.github.io)                          |                                         [Demo](https://speechresearch.github.io/prompttts/)                                          |
|                   2022.11.30                   | [MDPI-Appl. Sci.](https://www.mdpi.com/2076-3417/13/4/2225)                                                                                                                  | [**Emo-VITS**: An Emotion Speech Synthesis Method Based on VITS](Papers/2022.11_Emo-VITS.md)                                                                                                     |                                            CUC                                             |                                                                                                                                      |
|                 2023.01.05(v1)                 | [ArXiv](https://arxiv.org/abs/2301.02111)                                                                                                                                    | [**VALL-E**: Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers](Papers/2023.01_VALL-E.md)                                                                                   |          [Microsoft](https://www.microsoft.com/en-us/research/project/vall-e-x/)           |                                                                                                                                      |
|        2023.01.31(v1)<br>2023.06.25(v2)        | [ArXiv](https://arxiv.org/abs/2301.13662)                                                                                                                                    | [**InstructTTS**: Modelling Expressive TTS in Discrete Latent Space with Natural Language Style Prompt](Papers/2023.01_InstructTTS.md)                                                           |                                           Tecent                                           |                                            [Demo](https://dongchaoyang.top/InstructTTS/)                                             |
|                 2023.03.07(v1)                 | [ArXiv](https://arxiv.org/abs/2303.03926)                                                                                                                                    | [**VALL-E X**: Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling](Papers/2023.03_VALL-E_X.md)                                                            |          [Microsoft](https://www.microsoft.com/en-us/research/project/vall-e-x/)           |                                                                                                                                      |
|        2023.04.18(v1)<br>2023.05.30(v3)        | [ArXiv](https://arxiv.org/abs/2304.09116)                                                                                                                                    | [**NaturalSpeech 2**: Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers](Papers/2023.04_NaturalSpeech2.md)                                                       |                          [MSRA](https://speechresearch.github.io)                          |                                                                                                                                      |
|                 2023.05.16(v1)                 | [ArXiv](https://arxiv.org/abs/2305.09636)                                                                                                                                    | [**SoundStorm**: Efficient Parallel Audio Generation](Papers/2023.05_SoundStorm.md)                                                                                                              |                                           Google                                           |                                      [_Unofficial](https://github.com/yangdongchao/SoundStorm/)                                      |
|                 2023.06.06(v1)                 | [ArXiv](https://arxiv.org/abs/2306.03509)                                                                                                                                    | [**Mega-TTS**: Zero-Shot Text-to-Speech at Scale with Intrinsic Inductive Bias](Papers/2023.06_Mega-TTS.md)                                                                                      |                                      ZJU<br>ByteDance                                      |                                             [Demo](https://mega-tts.github.io/demo-page)                                             |
|                   2023.06.14                   | [Blog](https://coqui.ai/blog/tts/xtts_taking_tts_to_the_next_level)                                                                                                          | [**XTTS**: Taking TTS to the Next Level](Papers/2023.06_XTTS.md)                                                                                                                                 |                                           coqui                                            |                                                                                                                                      |
|        2023.06.23(v1)<br>2023.10.19(v2)        | [Arxiv](https://arxiv.org/abs/2306.15687)<br>[NeurIPS](https://proceedings.neurips.cc/paper_files/paper/2023/hash/2d8911db9ecedf866015091b28946e15-Abstract-Conference.html) | [**VoiceBox**: Text-Guided Multilingual Universal Speech Generation at Scale](Papers/2023.06_VoiceBox.md)                                                                                        |                                         Meta FAIR                                          |                                               [Demo](https://voicebox.metademolab.com)                                               |
|        2023.07.14(v1)<br>2024.03.18(v3)        | [ArXiv](https://arxiv.org/abs/2307.07218)                                                                                                                                    | [**Mega-TTS 2**: Boosting Prompting Mechanisms for Zero-Shot Speech Synthesis](Papers/2023.07_Mega-TTS2.md)                                                                                      |                                      ZJU<br>ByteDance                                      |                                            [Demo](https://mega-tts.github.io/mega2_demo/)                                            |
|                 2023.07.31(v1)                 | [ArXiv](https://arxiv.org/abs/2307.16430)                                                                                                                                    | [**VITS2**: Improving Quality and Efficiency of Single-Stage Text-to-Speech with Adversarial Learning and Architecture Design](Papers/2023.07_VITS2.md)                                          |                                         SK Telecom                                         |                                                [Demo](https://vits-2.github.io/demo/)                                                |
|                 2023.07.31(v1)                 | [ArXiv](https://arxiv.org/abs/2307.16549)                                                                                                                                    | [**DiffProsody**: Diffusion-based Latent Prosody Generation for Expressive Speech Synthesis with Prosody Conditional Adversarial Training](Papers/2023.07_DiffProsody.md)                        |                                                                                            |                                                                                                                                      |
|        2023.09.05(v1)<br>2023.10.12(v2)        | [ArXiv](https://arxiv.org/abs/2309.02285)                                                                                                                                    | **PromptTTS 2**: Describing and Generating Voices with Text Prompt                                                                                                                               |                          [MSRA](https://speechresearch.github.io)                          | [Demo](https://speechresearch.github.io/prompttts2/)<br>[Official](https://github.com/microsoft/NeuralSpeech/tree/master/PromptTTS2) |
|        2023.11.21(v1)<br>2023.11.27(v2)        | [ArXiv](https://arxiv.org/abs/2311.12454)                                                                                                                                    | [**HierSpeech++**: Bridging the Gap between Semantic and Acoustic Representation of Speech by Hierarchical Variational Inference for Zero-shot Speech Synthesis](Papers/2023.11_HierSpeechpp.md) |                                                                                            |                                       [Official](https://github.com/sh-lee-prml/HierSpeechpp)                                        |
|        2023.12.12(v1)<br>2024.01.02(v5)        | [ArXiv](https://arxiv.org/abs/2312.01479)                                                                                                                                    | [**OpenVoice**: Versatile Instant Voice Cloning](Papers/2023.12_OpenVoice.md)                                                                                                                    |                                          MyShell                                           |                                         [Official](https://github.com/myshell-ai/OpenVoice)                                          |
|          2024.01.16<br>2024.04.03(v1)          | [ArXiv](https://arxiv.org/abs/2404.02781)<br>ICLR<br>[OpenReview](https://openreview.net/forum?id=ofzeypWosV)                                                                | [**CLaM-TTS**: Improving Neural Codec Language Model for Zero-Shot Text-to-Speech](Papers/2024.04_CLaM-TTS.md)                                                                                   |                                          KRAFTON                                           |                                                  [Demo](https://clam-tts.github.io)                                                  |
|                 2024.02.14(v1)                 | [ArXiv](https://arxiv.org/abs/2402.09378)                                                                                                                                    | [**MobileSpeech**: A Fast and High-Fidelity Framework for Mobile Zero-Shot Text-to-Speech](Papers/2024.02_MobileSpeech.md)                                                                       |                                                                                            |                                                   [Demo](mobilespeech.github.io/)                                                    |
|          2024.02.08<br>2024.03.05(v1)          | [ArXiv](https://arxiv.org/abs/2403.03100)                                                                                                                                    | [**NaturalSpeech 3**: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models](Papers/2024.03_NaturalSpeech3.md)                                                                   |                          [MSRA](https://speechresearch.github.io)                          |                                       [Demo](https://speechresearch.github.io/naturalspeech3/)                                       |
|                 2024.03.25(v1)                 | [ArXiv](https://arxiv.org/abs/2403.16973)                                                                                                                                    | [**VoiceCraft**: Zero-Shot Speech Editing and Text-to-Speech in the Wild](Papers/2024.03_VoiceCraft.md)                                                                                          |                                   UT Austin<br>Rembrand                                    |                                          [Official](https://github.com/jasonppy/VoiceCraft)                                          |
|                 2024.06.06(v1)                 | [ArXiv](https://arxiv.org/abs/2404.04645)                                                                                                                                    | [**HyperTTS**: Parameter Efficient Adaptation in Text to Speech using Hypernetworks](Papers/2024.04_HyperTTS.md)                                                                                 |                                    北京邮电大学<br>新加坡科技设计大学                                     |                                [Official<br>2023.06.24](https://github.com/declare-lab/HyperTTS)<br>                                 |


## News

|时间|链接|标题|备注|
|:-:|---|---|:-:|
|2024.03.29|[Blog](https://openai.com/blog/navigating-the-challenges-and-opportunities-of-synthetic-voices)|OpenAI: Navigating the Challenges and Opportunities of Synthetic Voices|[Demo](https://openai.com/blog/navigating-the-challenges-and-opportunities-of-synthetic-voices)