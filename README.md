# SapphireTTS

## Books

|时间|标题|作者|链接|
|:-:|---|---|---|
|2022.02|book-text-to-speech |cnlinxi|[Github](https://github.com/cnlinxi)|
|2023.05|[Neural Text-to-Speech Synthesis](Books/2023.05_Neural_TTS_Synthesis/Ch00.ToC.md)|谭旭 (Microsoft Research Asia)|[Springer](https://link.springer.com/book/10.1007/978-981-99-0827-1)|

## Surveys

|时间|发表|标题|
|:-:|---|---|
|2021.06|ArXiv|A Survey on Neural Speech Synthesis|
|2024.02.12|(Springer) EURASIP Journal on Audio, Speech, and Music Processing|[Deep Learning-Based Expressive Speech Synthesis - A Systematic Review of Approaches, Challenges, and Resources](Papers/Survey/DL-Based_Expressive_Speech_Synthesis.md)|

## Datasets

|时间|发表|标题|链接|机构|
|:-:|---|---|:-:|:-:|
|2019.12(v1)|[ArXiv](https://arxiv.org/abs/1912.07875)<br>[ICASSP](https://doi.org/10.1109/ICASSP40776.2020.9052942)|**Libri-Light**|[Github](https://github.com/facebookresearch/libri-light)|Facebook|

## Models

|时间| 发表| 标题|机构|代码|
| :-: | --- | --- | :-: | :-: |
|2017.03.29(v1)<br>2017.04.06(v2)| [ArXiv](https://arxiv.org/abs/1703.10135)<br>[InterSpeech](https://doi.org/10.21437/Interspeech.2017-1452)|**Tacotron**: Towards End-to-End Speech Synthesis|Google||
|2017.12.16(v1)<br>2017.02.16(v2)| [ArXiv](https://arxiv.org/abs/1712.05884)<br>[IEEE-ICASSP](https://doi.org/10.1109/ICASSP.2018.8461368) |**Tacotron 2**: Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions|Google||
|2018.09.19(v1)<br>2019.01.30(v3)| [ArXiv](https://arxiv.org/abs/1809.08895)<br>[AAAI](https://doi.org/10.1609/aaai.v33i01.33016706)| **Transformer TTS**: Neural Speech Synthesis with Transformer Network|MSRA||
|2018.10.31(v1)| [ArXiv](https://arxiv.org/abs/1811.00002)| **WaveGlow**: A Flow-based Generative Network for Speech Synthesis|Nvidia||
|2019.05.22(v1)<br>2019.11.20(v5)| [ArXiv](https://arxiv.org/abs/1905.09263)<br>[NeurIPS](https://proceedings.neurips.cc/paper_files/paper/2019/hash/f63f65b503e22cb970527f23c9ad7db1-Abstract.html)| **FastSpeech**: Fast, Robust and Controllable Text to Speech|[MSRA](https://speechresearch.github.io)||
|2020.05.12(v1)<br>2020.07.16(v3)| [ArXiv](https://arxiv.org/abs/2005.05957)| **Flowtron**: an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis|Nvidia|[Official](https://github.com/NVIDIA/flowtron)|
|2020.06.08(v1)<br>2022.08.08(v8)| [ArXiv](https://arxiv.org/abs/2006.04558)| **FastSpeech2**: Fast and High-Quality End-to-End Text-to-Speech|[MSRA](https://speechresearch.github.io)||
|2021.03.01|| **AdaSpeech**: Adaptive Text to Speech for Custom Voice|[MSRA](https://speechresearch.github.io)||
|2021.03.05|| **AdaSpeech 2**: Adaptive Text to Speech with Untranscribed Data|[MSRA](https://speechresearch.github.io)||
|2021.06.02|| **AdaSpeech 3**: Adaptive Text to Speech for Spontaneous Style|[MSRA](https://speechresearch.github.io)||
|2021.04.01(v1)<br>2022.10.06(v2)| [ArXiv](https://arxiv.org/abs/2104.00436)| **ST-TTS**: Expressive Text-to-Speech using Style Tag|SK Telecom||
|2021.06.11(v1)| [ArXiv](https://arxiv.org/abs/2106.06103)<br>[ICML](https://proceedings.mlr.press/v139/kim21f.html)| **VITS**: Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech||[Official](https://github.com/jaywalnut310/vits)|
|2021.12.04(v1)<br>2023.04.30(v4)| [ArXiv](https://arxiv.org/abs/2112.02418)<br>[ICML](https://proceedings.mlr.press/v162/casanova22a.html)| **YourTTS**: Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice Conversion for Everyone| [Coqui](https://coqui.ai/blog/tts/yourtts-zero-shot-text-synthesis-low-resource-languages) |[Demo]()<br>[Official](https://github.com/Edresson/YourTTS/)|
| 2022.05.03<br>2022.05.09(v1)<br>2022.05.10(v2) | [ArXiv](https://arxiv.org/abs/2205.04421)| **NaturalSpeech**: End-to-End Text to Speech Synthesis with Human-Level Quality|[MSRA](https://speechresearch.github.io)|[Demo](https://speechresearch.github.io/naturalspeech/)|
|2022.10.24| [ArXiv](https://arxiv.org/abs/2210.13438)| **Encodec**: High Fidelity Neural Audio Compression|Meta FAIR|[Official](https://github.com/facebookresearch/encodec)|
|2022.11.22|| **PromptTTS**: Controllable Text-to-Speech with Text Descriptions|[MSRA](https://speechresearch.github.io)|[Demo](https://speechresearch.github.io/prompttts/)|
|2022.11.30| [MDPI-Appl. Sci.](https://www.mdpi.com/2076-3417/13/4/2225)| **Emo-VITS**: An Emotion Speech Synthesis Method Based on VITS|CUC||
|2023.01.05(v1)| [ArXiv](https://arxiv.org/abs/2301.02111)| **VALL-E**: Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers|[Microsoft](https://www.microsoft.com/en-us/research/project/vall-e-x/)||
|2023.01.31(v1)<br>2023.06.25(v2)| [ArXiv](https://arxiv.org/abs/2301.13662)| **InstructTTS**: Modelling Expressive TTS in Discrete Latent Space with Natural Language Style Prompt|Tecent|[Demo](https://dongchaoyang.top/InstructTTS/)|
|2023.03.07(v1)| [ArXiv](https://arxiv.org/abs/2303.03926)| **VALL-E X**: Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling|[Microsoft](https://www.microsoft.com/en-us/research/project/vall-e-x/)||
|2023.04.18(v1)<br>2023.05.30(v3)| [ArXiv](https://arxiv.org/abs/2304.09116)| **NaturalSpeech 2**: Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers|[MSRA](https://speechresearch.github.io)||
|2023.05.16(v1)| [ArXiv](https://arxiv.org/abs/2305.09636)| **SoundStorm**: Efficient Parallel Audio Generation|Google|[_Unofficial](https://github.com/yangdongchao/SoundStorm/)|
|2023.06.06(v1)| [ArXiv](https://arxiv.org/abs/2306.03509)| **Mega-TTS**: Zero-Shot Text-to-Speech at Scale with Intrinsic Inductive Bias|ZJU<br>ByteDance|[Demo](https://mega-tts.github.io/demo-page)|
|2023.06.14| [Blog](https://coqui.ai/blog/tts/xtts_taking_tts_to_the_next_level)| **XTTS**: Taking TTS to the Next Level|coqui||
|2023.06.23(v1)<br>2023.10.19(v2)| [Arxiv](https://arxiv.org/abs/2306.15687)<br>[NeurIPS](https://proceedings.neurips.cc/paper_files/paper/2023/hash/2d8911db9ecedf866015091b28946e15-Abstract-Conference.html) | **VoiceBox**: Text-Guided Multilingual Universal Speech Generation at Scale|Meta FAIR| [Demo](https://voicebox.metademolab.com) |
|2023.07.14(v1)<br>2024.03.18(v3)| [ArXiv](https://arxiv.org/abs/2307.07218)| **Mega-TTS 2**: Boosting Prompting Mechanisms for Zero-Shot Speech Synthesis|ZJU<br>ByteDance|[Demo](https://mega-tts.github.io/mega2_demo/)|
|2023.07.31(v1)| [ArXiv](https://arxiv.org/abs/2307.16430)| **VITS2**: Improving Quality and Efficiency of Single-Stage Text-to-Speech with Adversarial Learning and Architecture Design|SK Telecom|[Demo](https://vits-2.github.io/demo/)|
|2023.07.31(v1)| [ArXiv](https://arxiv.org/abs/2307.16549)| **DiffProsody**: Diffusion-based Latent Prosody Generation for Expressive Speech Synthesis with Prosody Conditional Adversarial Training |||
|2023.09.05(v1)<br>2023.10.12(v2)| [ArXiv](https://arxiv.org/abs/2309.02285)| **PromptTTS 2**: Describing and Generating Voices with Text Prompt|[MSRA](https://speechresearch.github.io)| [Demo](https://speechresearch.github.io/prompttts2/)<br>[Official](https://github.com/microsoft/NeuralSpeech/tree/master/PromptTTS2) |
|2023.11.21(v1)<br>2023.11.27(v2)| [ArXiv](https://arxiv.org/abs/2311.12454)| **HierSpeech++**: Bridging the Gap between Semantic and Acoustic Representation of Speech by Hierarchical Variational Inference for Zero-shot Speech Synthesis ||[Official](https://github.com/sh-lee-prml/HierSpeechpp)|
|2023.12.12(v1)<br>2024.01.02(v5)| [ArXiv](https://arxiv.org/abs/2312.01479)| **OpenVoice**: Versatile Instant Voice Cloning |MyShell|[Official](https://github.com/myshell-ai/OpenVoice)|
|2024.01.16<br>2024.04.03(v1)| [ArXiv](https://arxiv.org/abs/2404.02781)<br>ICLR<br>[OpenReview](https://openreview.net/forum?id=ofzeypWosV)| **CLaM-TTS**: Improving Neural Codec Language Model for Zero-Shot Text-to-Speech|KRAFTON|[Demo](https://clam-tts.github.io)|
|2024.02.14(v1)| [ArXiv](https://arxiv.org/abs/2402.09378)| **MobileSpeech**: A Fast and High-Fidelity Framework for Mobile Zero-Shot Text-to-Speech||[Demo](mobilespeech.github.io/)|
|2024.02.08<br>2024.03.05(v1)| [ArXiv](https://arxiv.org/abs/2403.03100)| **NaturalSpeech 3**: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models|[MSRA](https://speechresearch.github.io)|[Demo](https://speechresearch.github.io/naturalspeech3/)|
|2024.03.25(v1)| [ArXiv](https://arxiv.org/abs/2403.16973)| **VoiceCraft**: Zero-Shot Speech Editing and Text-to-Speech in the Wild|UT Austin<br>Rembrand|[Official](https://github.com/jasonppy/VoiceCraft)|
|2024.04.06(v1)<br>2024.04.06(v2)|[ArXiv](https://arxiv.org/abs/2404.03204)| **RALL-E**: Robust Codec Language Modeling with Chain-of-Thought Prompting for Text-to-Speech Synthesis|Microsoft(7/11)<br>东京大学(3/11)<br>浙江大学(1/11)<br>中科大(1/11)<br>港中文(2/11)|[Demo](https://ralle-demo.github.io/RALL-E)
|2024.04.06(v1)| [ArXiv](https://arxiv.org/abs/2404.04645)| **HyperTTS**: Parameter Efficient Adaptation in Text to Speech using Hypernetworks|北京邮电大学<br>新加坡科技设计大学|[Official<br>2023.06.24](https://github.com/declare-lab/HyperTTS)<br>|

## News

|时间|链接|标题|备注|
|:-:|---|---|:-:|
|2024.03.29|[Blog](https://openai.com/blog/navigating-the-challenges-and-opportunities-of-synthetic-voices)|OpenAI: Navigating the Challenges and Opportunities of Synthetic Voices|[Demo](https://openai.com/blog/navigating-the-challenges-and-opportunities-of-synthetic-voices)