# 隐马尔可夫模型 (Hidden Markov Model, HMM)

隐马尔可夫模型 (Hidden Markov Model, HMM) 是可用于标注问题的机器学习模型, 描述由隐藏的马尔可夫链随机生成观测序列的过程, 属于生成模型.

本章首先介绍隐马尔可夫模型的基本概念, 然后分别叙述隐马尔可夫模型的概率计算算法, 学习算法, 预测算法.

隐马尔可夫模型在语音识别, 自然语言处理, 生物信息, 模式识别等领域有着广泛的应用.

## 1.基本概念

> **定义: 隐马尔可夫模型**
> 隐马尔可夫模型是关于时序的概率模型, 描述由一个隐藏的马尔可夫链随机生成不可观测的状态随机序列, 再由各个状态生成一个观测从而产生观测随机序列的过程.
> 隐藏的马尔可夫链随机生成的状态的序列称为状态序列 (State Sequence);
> 每个状态生成一个观测, 而由此产生的观测的随机序列称为观测序列 (Observation Sequence).
> 序列的每一个位置又可以视为一个时刻.

隐马尔可夫模型由**初始概率分布**, **状态转移概率分布**, **观测概率分布**确定.
隐马尔可夫模型的形式定义如下:

设 $Q$ 是所有可能的状态的集合 $Q = \{q_1, q_2, \cdots, q_N\}$, $V$ 是所有可能的观测的集合 $V = \{v_1, v_2, \cdots, v_M\}$, 其中 $N$ 是可能的状态数, $M$ 是可能的观测数.

$I$ 是长度为 $T$ 的状态序列 $I = (i_1, i_2, \cdots, i_T)$, $O$ 是对应的观测序列 $O = (o_1, o_2, \cdots, o_T)$.

$A$ 是状态转移概率矩阵: $A = [a_{ij}]_{N\times N}$, 其中 $a_{ij} = P(i_{t+1}=q_j|i_t=q_i), i=1,2,\cdots,N, j=1,2,\cdots,N$ 是在时刻 $t$ 处于状态 $q_t$ 的条件下在时刻 $t+1$ 转移到状态 $q_j$ 的概率.

$B$ 是观测概率矩阵 $B=[b_j(k)]_{N\times M}$, 其中 $b_j(k)=P(o_t=v_k|i_t=q_j), k=1,2,\cdots,M, j=1,2,\cdots,N$ 是在时刻 $t$ 处于状态 $q_j$ 的条件下生成观测 $v_k$ 的概率.

$\pi$ 是初始状态概率向量 $\pi=(\pi_i)$, 其中 $\pi_i=P(i_1=q_i), i=1,2,\cdots,N$ 是时刻 $t=1$ 处于状态 $q_i$ 的概率.

隐马尔可夫模型由三要素包括初始状态概率向量 $\pi$, 状态转移概率矩阵 $A$, 观测概率矩阵 $B$ 决定.
$\pi$ 和 $A$ 决定状态序列, $B$ 决定观测序列.
因此, 隐马尔科夫模型 $\lambda$ 可以用三元符号表示, 即 $\lambda=(\pi, A, B)$.

初始状态概率向量 $\pi$ 与状态转移概率矩阵 $A$ 确定了隐藏的马尔可夫链, 生成不可观测的状态序列.
观测概率矩阵 $B$ 确定了如何从状态序列生成观测, 与状态序列综合确定了如何产生观测序列.

从定义可知, 隐马尔可夫模型作了两个基本假设:
1. 齐次马尔可夫性假设: 即假设隐藏的马尔可夫链在任意时间 $t$ 的状态只依赖于其前一时刻的状态, 与其他时刻的状态及观测无关, 也和其他时刻无关:
$$
    P(i_t|i_{t-1}, o_{t-1},\cdots,i_1, o_1) = P(i_T|i_{t-1}), t=1,2,\cdots,T.    
$$
2. 观察独立性假设: 即假设任意时刻的观测只依赖于该时刻的马尔可夫链的状态, 与其他观测及状态无关:
$$
    P(o_t|i_T, o_T, i_{T-1}, o_{T-1}, \cdots, i_{1+1}, o_{t+1}, i_{t}, i_{t-1}, o_{t-1},\cdots, i_1, o_1)=P(o_t|i_t)
$$

隐马尔可夫模型可以用于标注, 这时状态对应着标记.
标注问题是给定观测的序列预测其对应的标记序列.
可以假设标注问题的数据是由隐马尔可夫模型生成, 这样我们可以用隐马尔可夫模型的学习与预测算法进行标注.

### 示例: 盒子和球模型

> 假设有 4 个盒子, 每个盒子都装有红白两种颜色的球, 盒子里的红白球数由下表列出.
> 
> | |盒子1 | 盒子2 | 盒子3 | 盒子4 |
> | ---|---- | ---- | ---- | ---- |
> |红球数|5|3|6|8|
> |白球数|5|7|4|2|
>
> 按照下面的方法抽取球, 产生一个球的颜色的观测序列:
> 首先从 4 个盒子以等概率随机选取 1 个盒子, 从这个盒子中随机抽出一个球, 记录颜色后放回.
> 然后, 从该盒子随机转移到下一个盒子, 规则为: 盒子 1-> 盒子 2, 盒子 2 和盒子 3 以概率 0.4 转移到左边, 概率 0.6 转移到右边, 盒子 4 以概率 0.5 转移到盒子 3 或停留.
> 确定转移的盒子后, 再从这个盒子里随机抽出一个球, 记录颜色后放回.
> 重复五次, 得到一个球的颜色的观测序列: $O=(r,r,w,w,r)$, 在这一个过程中, 观察者只能观测到球的颜色的序列, 观测不到球是从哪个盒子取出的, 即观测不到盒子序列.
> 此时盒子序列为隐藏的状态序列, 球色序列为可见的观测序列.
>

根据所给条件, 可以明确状态集合, 观测集合, 序列长度, 模型三要素.

状态集合: $Q=\{box_1, box_2, box_3, box_4\}$, $N=4$;
观测集合: $V=\{red, white\}$, $M=2$;
状态序列和观测序列长度为 $T=5$;
初始概率分布: $\pi=(0.25,0.25,0.25,0.25)^{\mathsf{T}}$
状态转移概率分布:
$$
    A = \begin{bmatrix}
        0 & 1 & 0 & 0 \\
        0.4 & 0 & 0.6 & 0 \\
        0 & 0.4 & 0 & 0.6 \\
        0 & 0 & 0.5 & 0.5
        \end{bmatrix}
$$

观测概率分布:
$$
    B = \begin{bmatrix}
        0.5 & 0.5 \\
        0.3 & 0.7 \\
        0.6 & 0.4 \\
        0.8 & 0.2
        \end{bmatrix}
$$

根据隐马尔可夫模型的定义, 可以将一个长度为 $T$ 的观测序列 $O=(o_1,o_2,\cdots,o_T)$ 的生成过程描述如下:

---
输入: 隐马尔可夫模型 $\lambda = (A,B,\pi)$, 观测序列长度 $T$.
输出: 观测序列 $O=(o_1,o_2,\cdots,o_T)$.

1. 按照初始状态分布 $\pi$ 产生状态 $i_1$;
2. 令 $t=1$;
3. 按照状态 $i_t$ 的观测概率分布 $b_i(k)$ 生成 $o_t$;
4. 按照状态 $i_t$ 的状态转移概率分布 $\{a_{i_{t}i_{t+1}}\}$ 产生状态 $i_{t+1}$;
5. 令 $t=t+1$, 如果 $t<T$, 转至步骤 3, 否则停止.

---

### 三个基本问题

1. 概率计算问题. 
   给定模型 $\lambda = (A,B,\pi)$, 观测序列 $O=(o_1,o_2,\cdots,o_T)$,
   计算在模型 $\lambda$ 下观测到的 $O$ 出现的概率 $P(O|\lambda)$.
2. 学习问题.
   给定观测序列 $O=(o_1,o_2,\cdots,o_T)$, 估计模型的参数 $\lambda=(A,B,\pi)$, 使得在该模型下观测预测概率 $P(O|\lambda)$ 最大, 即最大似然估计.
3. 预测问题 (解码问题).
   给定模型 $\lambda = (A,B,\pi)$, 观测序列 $O=(o_1,o_2,\cdots,o_T)$, 
   求对给定的观测序列条件概率 $P(I|O)$ 最大的状态序列 $I=(i_1,i_2,\cdots,i_T)$.
   即给定观测序列, 求最有可能的对应的状态序列.

## 2.概率计算算法

给定模型 $\lambda = (A,B,\pi)$, 观测序列 $O=(o_1,o_2,\cdots,o_T)$, 计算该序列出现的概率 $P(O|\lambda)$.

最直接的方法是按照概率公式直接计算.

通过列举所有可能长度为 $T$ 的状态序列 $I=(i_1,i_2,\cdots,i_T)$, 求各个状态序列 $I$ 与观测序列 $O$ 的联合概率 $P(O,I|\lambda)$, 然后对所有可能的状态序列求和得到 $P(O|\lambda)$.

要生成一个状态序列 $I=(i_1,\cdots,i_T)$, 按照初始状态概率值和状态转移概率矩阵, 可以推得
$$
    P(I|\lambda) = \pi_{i_1} a_{i_{1} i_{2}} a_{i_{2} i_{3}}\cdots a_{i_{T-1} i_{T}}
$$

对于给定的状态序列 $I$, 要得到观测序列的概率值由观测概率矩阵得到, 即
$$
    P(O|I,\lambda) = b_{i_1}(o_1) b_{i_2}(o_2) \cdots b_{i_T}(o_T)
$$

那么状态序列 $I$ 和观测序列 $O$ 同时出现的联合概率为
$$
\begin{aligned}
    P(O,I|\lambda) 
    &= \sum_{I} P(O|I,\lambda) P(I|\lambda) \\
    &= \sum_{I} \pi_{i_1} b_{i_1}(o_1) a_{i_{1} i_{2}} b_{i_2}(o_2) \cdots a_{i_{T-1} i_{T}} b_{i_T}(o_T) \\
\end{aligned}
$$

上式需要对所有可能出现的 $I$ 进行求和, 状态集大小为 $N$, 所以具有 $T$ 个状态的状态序列 $I$ 有 $N^T$ 种, 对观测序列的概率需要遍历 $T$, 所以时间复杂度为 $O(TN^T)$. 这种计算方式复杂度过高不可行.

### 前向算法

> **定义: 前向概率**
> 给定隐马尔科夫模型 $\lambda$, 定义到时刻 $t$ 部分观测序列为 $o_1,\cdots, o_t$, 且状态为 $q_i$ 的概率为前向概率, 记作
$$
    \alpha_t(i) = P(o_1,\cdots,o_t,i_t=q_i|\lambda)
$$

可以递推地求得前向概率 $\alpha_t(i)$ 以及观测序列概率 $P(O|\lambda)$.

输入: 隐马尔可夫模型 $\lambda$, 观测序列 $O$.
输出: 观测序列概率 $P(O|\lambda)$.

1. 初始值 $\alpha_1(i) = \pi_i b_i(o_1)$, $i=1,2,\cdots,N$;
2. 递推:
   $$
   \alpha_{t+1}(i) = \left [\sum_{j=1}^N \alpha_t(j)a_{ji}\right ] b_i(o_{t+1}), i=1,2,\cdots,N,
   $$
3. 终止:
   $$
   P(O|\lambda) = \sum_{i=1}^N \alpha_T(i)
   $$

### 后向算法