# 9.Robust TTS

## Abstract

> A good TTS system should be robust to always generate “correct” speech according to text even when encountering corner cases
> However, in neural TTS, robustness issues such as word skipping, repeating, and attention collapse1 often happen in acoustic models when generating mel-spectrogram sequence from character/phoneme sequence with encoder-decoder-attention and autoregressive generation, or some glitches such as hoarseness, metallic noise, jitter, or pitch breaking often happen in vocoders when generating waveform from mel-spectrogram sequence

> Basically speaking, the causes of these robustness issues can be categorized as follows:
> - A lack of data coverage and generalization ability can cause robustness issues
> For example, when the test domain is not well covered by the training domain, the TTS models may not perform well and cause robustness issues when generating speech.
> - The difficulty in learning the alignments between characters/phonemes and mel-spectrograms
> Vocoders do not face severe robustness issues due to this reason, since the acoustic features and waveform are already aligned frame-wisely (i.e.,each frame of acoustic features corresponds to a certain number (hop size) of waveform points).
> - The exposure bias and error propagation problems incurred in an autoregressive generation

> In this chapter, we introduce how to address these robustness issues according to their causes in different categories
> We summarize some popular techniques in these categories to improve robustness, as shown in Table 9.1
> The works addressing these problems may have overlapping, e.g., some works may enhance the attention mechanism in AR or NAR generation, and similarly, the duration prediction can be applied in both AR and NAR generation
> We introduce these technologies in Sects. 9.1, 9.2, and 9.3.

## 9.1.Improving Generalization Ability

> To alleviate the robustness issues caused by a lack of data coverage and generalization ability, some works propose to scale to the unseen domain, such as using pre-trained word embeddings or model parameters [8, 46–51] to provide better text representations, increasing the amount and diversity of the training data [6], adopting relative position encoding to support long sequence unseen in training [7, 52], or train universal acoustic models or vocoders [2–5] (usually in multi-lingual, multi-speaker, and multi-style), or sophisticated designs [53].

## 9.2.Improving Text-Speech Alignment

> There are different methods to improve the alignment learning between characters/phonemes and mel-spectrograms, in order to alleviate the robustness issues: 
> (1) enhancing the robustness of attention mechanism [1, 9, 11, 14, 15, 17, 19];
> (2) removing attention and instead predicting duration explicitly to bridge the length mismatch between text and speech [24, 30, 39, 40].
> We introduce them respectively in Sects. 9.2.1 and 9.2.2.

### 9.2.1.Enhancing Attention

> In attention-based acoustic models, a lot of word skipping/repeating and attention collapse issues are caused by the incorrect attention alignments learned in encoder-decoder attention
> To alleviate this problem, some properties of the alignments between text (characters/phonemes) sequence and mel-spectrogram sequence are considered [1]: (1) Local: one character/phoneme token can be aligned to one or multiple consecutive mel-spectrogram frames, while one mel-spectrogram frame can only be aligned to a single character/phoneme token, which can avoid the blurry attention and attention collapse; (2) Monotonic: if character A is behind character B, the mel-spectrogram corresponding to A is also behind that corresponding to B, which can avoid word repeating; (3) Complete: each character/phoneme token must be covered by at least one mel-spectrogram frame, which can avoid word skipping
> We analyze the techniques to enhance attention (from Table 9.1) according to whether they satisfy the above three properties and list them in Table 9.2
> We  describe these techniques as follows.

#### Content-Based Attention

> The early attention mechanisms adopted in TTS (e.g. Tacotron [9]) are content-based [54], where the attention distributions are determined by the degree of match between the hidden representations from the encoder and decoder
> Content-based attention is suitable for the tasks such as neural machine translation [54, 55] where the alignments between the source and target tokens are purely based on semantic meaning (content)
> However, for the tasks like automatic speech recognition [56–58] and text-to-speech synthesis [9], the alignments between text and speech have some specific properties
> For example, in TTS [1], the attention alignments should be local, monotonic, and complete
> Therefore, advanced attention mechanisms should be designed to better leverage these properties.

#### Location-Based Attention

> Considering the alignments between text and speech are depending on their positions, location-based attention [7, 59] is proposed to leverage the positional information for alignment
> Several TTS models such as Char2Wav [11], VoiceLoop [12], and MelNet [13] adopt the location-based attention
> As we summarize in Table 9.2, location-based attention can ensure the monotonicity property if properly handled.

#### Content/Location-Based Hybrid Attention

> To combine the advantages of content and location-based attention, [14, 56] introduce location-sensitive attention: when calculating the current attention alignment, the previous attention alignment is used
> In this way, the attention would be more stable due to monotonic alignment.

#### Monotonic Attention

> For monotonic attention [1, 16, 60–62], the attention position is monotonically increasing, which also leverages the prior that the alignments between text and speech are monotonic
> In this way, it can avoid skipping and repeating issues
> However, the completeness property cannot be guaranteed in the above monotonic attention
> Therefore, [1] propose stepwise monotonic attention, where in each decoding step, the attention alignment position moves forward at most one step and is not allowed to skip any input unit.

#### Windowing or Off-Diagonal Penalty

> Since attention alignments are monotonic and diagonal, [15, 17–19, 56] propose to restrict the attention on the source sequence into a window subset
> In this way, the learning flexibility and difficulty are reduced
> Chen et al. [19] use penalty loss for off-diagonal attention weights, by constructing a band mask and encouraging the attention weights to be distributed in the diagonal band.

#### Enhancing Encoder-Decoder Connection

> Since speech has more correlation among adjacent frames, the decoder itself contains enough information to predict the next frame and thus tends to ignore the text information from the encoder
> Therefore, some works propose to enhance the connection between the encoder and decoder and thus can improve attention alignment
> Wang et al. [9] and Shen et al. [14] use multi-frame prediction that generates multiple non-overlapping output frames at each decoder step
> In this way, in order to predict consecutive frames, the decoder is forced to leverage information from the encoder side, which can improve the alignment learning
> Other works also use a large dropout in the Pre-net before the decoder [9, 14, 19], or a small hidden size in the Pre-net as a bottleneck [19], which can prevent simply copying the previous speech frame when predicting the current speech frame
> The decoder will get more information from the encoder side, which benefits the alignment learning. [18, 19] propose to enhance the connection of the positional information between source and target sequences, which benefits attention alignment learning
> Liu et al. [20] leverage connectionist temporal classification (CTC) [63] based automatic speech recognition (ASR) as a cycle loss to encourage the generated mel-spectrograms to contain text information, which can also enhance the encoder-decoder connection for better attention alignment.

#### Positional Attention

> Some non-autoregressive generation models [21, 22] leverage position information as the query to attend the key and value from the encoder, which is another way to build the connection between encoder and decoder for a parallel generation.

### 9.2.2.Replacing Attention with Duration Prediction

> While improving the attention alignments between text and speech can alleviate the robustness issues to some extent, it cannot totally avoid them
> Thus, some works [24, 30, 38, 39] propose to totally remove the encoder-decoder attention, explicitly predict the duration of each character/phoneme, and expand the text hidden sequence according to the duration to match the length of the mel-spectrogram sequence
> After that, the model can generate a mel-spectrogram sequence in an autoregressive or non-autoregressive manner
> It is very interesting that the early SPSS uses duration for alignments, and then the sequence-to-sequence models remove duration but use attention instead, and the later TTS models discard attention and use duration again, which is a kind of technique renaissance

> Existing works to investigate the duration prediction in neural TTS can be categorized from two perspectives: (1) Using external alignment tools or jointly training to get the duration label. (2) Optimizing the duration prediction in an end-to-end way or using ground-truth duration in training and predicted duration in inference
> We summarize the works according to the two perspectives in Table 9.3 and describe them as follows.

#### External Alignment

> The works leveraging external alignment tools [36, 63–65] can be divided into several categories according to the used alignment tools: (1) Encoder-decoder attention: FastSpeech [24] obtains the duration label from the attention alignments of an autoregressive acoustic model
> SpeedySpeech [25] follows a similar pipeline of FastSpeech to extract the duration from an autoregressive teacher model but replaces the whole network structure with purely CNN. (2) CTC alignment
> Beliaev et al. [28] leverages a CTC [63] based ASR model to provide the alignments between phoneme and mel-spectrogram sequence. (3) HMM alignment: FastSpeech 2 [29] leverages the HMM-based  Montreal forced alignment (MFA) [65] to get the duration
> Other works such as DurIAN [30], RobuTrans [31], Parallel Tacotron [33], and Non-Attentive Tacotron [34] use forced alignment or speech recognition tools to get the alignments.

#### Internal Alignment

> AlignTTS [35] follows the basic model structure of FastSpeech but leverages a dynamic programming-based method to learn the alignments between text and mel-spectrogram sequences with multi-stage training
> JDI-T [26] follows FastSpeech to extract duration from an autoregressive teacher model, but jointly trains the autoregressive and non-autoregressive models, which does not need two-stage training
> Glow-TTS [38] leverages a novel monotonic alignment search to extract duration
> EATS [39] leverages the interpolation and soft dynamic time warping (DTW) loss to optimize the duration prediction in a fully end-to-end way.

#### Non End-to-End Optimization

> Typical duration prediction methods [24–26, 28–31, 33–35, 38] usually use duration obtained from external/internal alignment tools for training and use predicted duration for inference
> The predicted duration is not end-to-end optimized by receiving a guiding signal (gradients) from the mel-spectrogram loss.

#### End-to-End Optimization

> In order to jointly optimize the duration to achieve better prosody, EATS [39] predicts the duration using an internal module and optimizes the duration end-to-end with the help of duration interpolation and soft DTW loss
> Parallel Tacotron 2 [40] follows the practice of EATS to ensure differentiable duration prediction
> NaturalSpeech [66] also leverages a differentiable durator for end-to-end duration modeling
> Non-Attentive Tacotron [34] proposes a semi-supervised learning for duration prediction, where the predicted duration can be used for upsampling if no duration label is available.

## 9.3.Improving Autoregressive Generation

> For the exposure bias and error propagation problems in the autoregressive generation, the works can also be divided into two aspects: (1) improving autoregressive generation to alleviate the exposure bias and error propagation problems [41–44], and (2) removing autoregressive generation and instead using non-autoregressive generation [21, 24, 29, 39].

### 9.3.1.Enhancing AR Generation

> Autoregressive sequence generation usually suffers from exposure bias and error propagation [67, 68]
> Exposure bias refers to that the sequence generation model is usually trained by taking the previous ground-truth value as input (i.e., teacher-forcing), but generates the sequence autoregressively by taking the previous predicted value as input in inference
> The mismatch between training and inference can cause error propagation in inference, where the prediction errors can accumulate quickly along the generated sequence. 

> Some works have investigated different methods to alleviate exposure bias and error propagation issues
> Guo et al. [41] leverage professor forcing [69] to alleviate the mismatch between the different distributions of real and predicted data
> Liu et al. [43] conduct teacher-student distillation [70–72] to reduce the exposure bias problem, where the teacher is trained with teacher-forcing mode, and the student takes the previously predicted value as input and is optimized to reduce the distance of hidden states between the teacher and student models
> Considering the right part of the generated mel-spectrogram sequence is usually worse than that in the left part due to error propagation, some works leverage both left-to-right and right-to-left generations [73] for data augmentation [44] and regularization [45]. 
> Vainer and Dušek [25] leverage some data augmentations to alleviate the exposure bias and error propagation issues, by adding some random Gaussian noises to each input spectrogram pixel to simulate the prediction errors and degrading the input spectrograms by randomly replacing several frames with random frames to encourage the model to use temporally more distant frames
> It is worthing mention that the bottleneck structure and dropout of the Pre-net introduced in Tacotron [9] and later analyzed by MultiSpeech [19] can also alleviate the exposure bias by reducing the information needed from the previous frames.

### 9.3.2.Replacing AR Generation with NAR Generation

> Although the exposure bias and error propagation problems in AR generation can be alleviated through the above methods, the problems cannot be addressed thoroughly. 
> Therefore, some works directly adopt non-autoregressive generation to avoid these issues
> They can be divided into two categories according to the use of attention or duration prediction
> Some works such as ParaNet [21] and Flow-TTS [22] use  positional attention [18] for the text and speech alignment in a parallel generation.
> The remaining works such as FastSpeech [24, 29] and EATS [39] use duration prediction to bridge the length mismatch between text and speech sequences. 

> Based on the introductions in the above subsections, we have a new category of TTS according to the alignment learning and AR/NAR generation, as shown in 
> Table 9.4: (1) AR + Attention, such as Tacotron [9, 14], DeepVoice 3 [18], and TransformerTTS [10]. (2) AR + Non-Attention (Duration), such as DurIAN [30], RobuTrans [31], and Non-Attentive Tacotron [34]. (3) Non-AR + Attention, such as ParaNet [21], Flow-TTS [22], and VARA-TTS [23]. (4) Non-AR + Non-Attention, such as FastSpeech 1/2 [24, 29], Glow-TTS [38], and EATS [39].

## References

1. He M, Deng Y, He L (2019) Robust sequence-to-sequence acoustic modeling with stepwise monotonic attention for neural TTS. In: Proceedings of the Interspeech 2019, pp 1293–1297 
2. Lorenzo-Trueba J, Drugman T, Latorre J, Merritt T, Putrycz B, Barra-Chicote R, Moinet A, Aggarwal V (2019) Towards achieving robust universal neural vocoding. In: Proceedings of the Interspeech 2019, pp 181–185 
3. Paul D, Pantazis Y, Stylianou Y (2020) Speaker conditional WaveRNN: towards universal neural vocoder for unseen speaker and recording conditions. In: Proceedings of the Interspeech 2020, pp 235–239 
4. Jang W, Lim D, Yoon J (2020) Universal MelGAN: a robust neural vocoder for high-fidelity waveform generation in multiple domains. Preprint. arXiv:2011.09631 
5. Jiao Y, Gabrys A, Tinchev G, Putrycz B, Korzekwa D, Klimkov V (2021) Universal neural vocoding with parallel WaveNet. Preprint. arXiv:2102.01106 
6. Hwang MJ, Yamamoto R, Song E, Kim JM (2020) TTS-by-TTS: TTS-driven data augmentation for fast and high-quality speech synthesis. Preprint. arXiv:2010.13421 
7. Battenberg E, Skerry-Ryan R, Mariooryad S, Stanton D, Kao D, Shannon M, Bagby T 
(2020) Location-relative attention mechanisms for robust long-form speech synthesis. In: ICASSP 2020-2020 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 6194–6198 
8. Zhang G, Song K, Tan X, Tan D, Yan Y, Liu Y, Wang G, Zhou W, Qin T, Lee T, et al. (2022) Mixed-phoneme BERT: improving BERT with mixed phoneme and sup-phoneme representations for text to speech. Preprint. arXiv:2203.17190 
9. Wang Y, Skerry-Ryan R, Stanton D, Wu Y, Weiss RJ, Jaitly N, Yang Z, Xiao Y, Chen Z, Bengio S et al (2017) Tacotron: towards end-to-end speech synthesis. In: Proceedings of the Interspeech 2017, pp 4006–4010 
10. Li N, Liu S, Liu Y, Zhao S, Liu M (2019) Neural speech synthesis with Transformer network. In: Proceedings of the AAAI conference on artificial intelligence, vol 33, pp 6706–6713 
11. Sotelo J, Mehri S, Kumar K, Santos JF, Kastner K, Courville A, Bengio Y (2017) Char2wav: end-to-end speech synthesis
12. Taigman Y, Wolf L, Polyak A, Nachmani E (2018) VoiceLoop: voice fitting and synthesis via a phonological loop. In: International conference on learning representations 
13. Vasquez S, Lewis M (2019) MelNet: a generative model for audio in the frequency domain. Preprint. arXiv:1906.01083 
14. Shen J, Pang R, Weiss RJ, Schuster M, Jaitly N, Yang Z, Chen Z, Zhang Y, Wang Y, Skerry-Ryan R et al (2018) Natural TTS synthesis by conditioning WaveNet on mel spectrogram predictions. In: 2018 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 4779–4783 
15. Zhang JX, Ling ZH, Dai LR (2018) Forward attention in sequence-to-sequence acoustic modeling for speech synthesis. In: 2018 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 4789–4793 
16. Yasuda Y, Wang X, Yamagishi J (2019) Initial investigation of an encoder-decoder end-to-end TTS framework using marginalization of monotonic hard latent alignments. In: Proceedings of the 10th ISCA Speech Synthesis Workshop 
17. Tachibana H, Uenoyama K, Aihara S (2018) Efficiently trainable text-to-speech system based on deep convolutional networks with guided attention. In: 2018 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 4784–4788 
18. Ping W, Peng K, Gibiansky A, Arik SO, Kannan A, Narang S, Raiman J, Miller J (2018) Deep voice 3: 2000-speaker neural text-to-speech. In: Proceedings of the international conference on learning representations, pp 214–217 
19. Chen M, Tan X, Ren Y, Xu J, Sun H, Zhao S, Qin T (2020) MultiSpeech: multi-speaker text to speech with Transformer. In: INTERSPEECH, pp 4024–4028 
20. Liu P, Wu X, Kang S, Li G, Su D, Yu D (2019) Maximizing mutual information for Tacotron. 
Preprint. arXiv:1909.01145 
21. Peng K, Ping W, Song Z, Zhao K (2020) Non-autoregressive neural text-to-speech. In: 
International conference on machine learning (PMLR), pp 7586–7598 
22. Miao C, Liang S, Chen M, Ma J, Wang S, Xiao J (2020) Flow-TTS: a non-autoregressive network for text to speech based on flow. In: ICASSP 2020-2020 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 7209–7213 
23. Liu P, Cao Y, Liu S, Hu N, Li G, Weng C, Su D (2021) VARA-TTS: non-autoregressive text-to-speech synthesis based on very deep VAE with residual attention. Preprint. arXiv:2102.06431 
24. Ren Y, Ruan Y, Tan X, Qin T, Zhao S, Zhao Z, Liu TY (2019) FastSpeech: fast, robust and controllable text to speech. In: NeurIPS 
25. Vainer J, Dušek O (2020) SpeedySpeech: efficient neural speech synthesis. In: Proceedings of the Interspeech 2020, pp 3575–3579 
26. Lim D, Jang W, Gyeonghwan O, Park H, Kim B, Yoon J (2020) JDI-T: jointly trained duration informed transformer for text-to-speech without explicit alignment. In: Proceedings of the 
Interspeech 2020, pp 4004–4008 
27. Ła´ncucki A (2020) FastPitch: parallel text-to-speech with pitch prediction. Preprint, arXiv:2006.06873 
28. Beliaev S, Rebryk Y, Ginsburg B (2020) TalkNet: fully-convolutional non-autoregressive speech synthesis model. Preprint. arXiv:2005.05514 
29. Ren Y, Hu C, Tan X, Qin T, Zhao S, Zhao Z, Liu TY (2021) FastSpeech 2: fast and high-quality end-to-end text to speech. In: International conference on learning representations. https://openreview.net/forum?id=piLPYqxtWuA 
30. Yu C, Lu H, Hu N, Yu M, Weng C, Xu K, Liu P, Tuo D, Kang S, Lei G et al (2020) DurIAN: duration informed attention network for speech synthesis. In: Proceedings of the Interspeech 
2020, pp 2027–2031 
31. Li N, Liu Y, Wu Y, Liu S, Zhao S, Liu M (2020) RobuTrans: a robust transformer-based text-to-speech model. In: Proceedings of the AAAI conference on artificial intelligence, vol 34, pp 
8228–8235 
32. Okamoto T, Toda T, Shiga Y, Kawai H (2019) Tacotron-based acoustic model using phoneme alignment for practical neural text-to-speech systems. In: 2019 IEEE automatic speech recognition and understanding workshop (ASRU). IEEE, pp 214–221
33. Elias I, Zen H, Shen J, Zhang Y, Jia Y, Weiss R, Wu Y (2020) Parallel Tacotron: non-autoregressive and controllable TTS. Preprint. arXiv:2010.11439 
34. Shen J, Jia Y, Chrzanowski M, Zhang Y, Elias I, Zen H, Wu Y (2020) Non-attentive Tacotron: robust and controllable neural TTS synthesis including unsupervised duration modeling. 
Preprint. arXiv:2010.04301 
35. Zeng Z, Wang J, Cheng N, Xia T, Xiao J (2020) AlignTTS: efficient feed-forward text-to-speech system without explicit alignment. In: ICASSP 2020-2020 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 6714–6718 
36. Li N, Liu S, Liu Y, Zhao S, Liu M, Zhou M (2020) MoBoAligner: a neural alignment model for non-autoregressive TTS with monotonic boundary search. In: Proceedings of the Interspeech 
2020, pp 3999–4003 
37. Miao C, Liang S, Liu Z, Chen M, Ma J, Wang S, Xiao J (2020) EfficientTTS: an efficient and high-quality text-to-speech architecture. Preprint. arXiv:2012.03500 
38. Kim J, Kim S, Kong J, Yoon S (2020) Glow-TTS: a generative flow for text-to-speech via monotonic alignment search. Adv Neural Inf Process Syst 33 
39. Donahue J, Dieleman S, Bi´nkowski M, Elsen E, Simonyan K (2021) End-to-end adversarial text-to-speech. In: International conference on learning representations 
40. Elias I, Zen H, Shen J, Zhang Y, Ye J, Skerry-Ryan R, Wu Y (2021) Parallel Tacotron 
2: a non-autoregressive neural TTS model with differentiable duration modeling. Preprint. arXiv:2103.14574 
41. Guo H, Soong FK, He L, Xie L (2019) A new GAN-based end-to-end TTS training algorithm. 
In: Proceedings of the Interspeech 2019, pp 1288–1292 
42. Liu R, Yang J, Liu M (2019) A new end-to-end long-time speech synthesis system based on 
Tacotron2. In: Proceedings of the 2019 international symposium on signal processing systems, pp 46–50 
43. Liu R, Sisman B, Li J, Bao F, Gao G, Li H (2020) Teacher-student training for robust Tacotron-based TTS. In: ICASSP 2020-2020 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 6274–6278 
44. Ren Y, Tan X, Qin T, Zhao S, Zhao Z, Liu TY (2019) Almost unsupervised text to speech and automatic speech recognition. In: International conference on machine learning (PMLR), pp 
5410–5419 
45. Zheng Y, Tao J, Wen Z, Yi J (2019) Forward-backward decoding sequence for regularizing end-to-end tts. IEEE/ACM Trans Audio Speech Lang Process 27(12):2067–2079 
46. Fang W, Chung YA, Glass J (2019) Towards transfer learning for end-to-end speech synthesis from deep pre-trained language models. Preprint. arXiv:1906.07307 
47. Hayashi T, Watanabe S, Toda T, Takeda K, Toshniwal S, Livescu K (2019) Pre-trained text embeddings for enhanced text-to-speech synthesis. In: Proceedings of the Interspeech 2019, pp 4430–4434 
48. Xiao Y, He L, Ming H, Soong FK (2020) Improving prosody with linguistic and BERT derived features in multi-speaker based Mandarin Chinese neural TTS. In: ICASSP 2020-2020 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 6704–6708 
49. Jia Y, Zen H, Shen J, Zhang Y, Wu Y (2021) PnG BERT: augmented BERT on phonemes and graphemes for neural TTS. Preprint. arXiv:2103.15060 
50. Guo H, Soong FK, He L, Xie L (2019) Exploiting syntactic features in a parsed tree to improve end-to-end TTS. In: Proceedings of the Interspeech 2019, pp 4460–4464 
51. Zhou Y, Song C, Li J, Wu Z, Meng H (2021) Dependency parsing based semantic representation learning with graph neural network for enhancing expressiveness of text-to-speech. 
Preprint. arXiv:2104.06835 
52. Zeng Z, Wang J, Cheng N, Xiao J (2020) Prosody learning mechanism for speech synthesis system without text length limit. In: Proceedings of the Interspeech 2020, pp 4422–4426 
53. Chen J, Tan X, Luan J, Qin T, Liu TY (2020) HiFiSinger: towards high-fidelity neural singing voice synthesis. Preprint. arXiv:2009.01776
54. Bahdanau D, Cho K, Bengio Y (2014) Neural machine translation by jointly learning to align and translate. Preprint. arXiv:1409.0473 
55. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser Ł, Polosukhin I 
(2017) Attention is all you need. In: Advances in neural information processing systems, pp 
5998–6008 
56. Chorowski J, Bahdanau D, Serdyuk D, Cho K, Bengio Y (2015) Attention-based models for speech recognition. In: Proceedings of the 28th international conference on neural information processing systems, vol 1, pp 577–585 
57. Chan W, Jaitly N, Le Q, Vinyals O (2016) Listen, attend and spell: a neural network for large vocabulary conversational speech recognition. In: 2016 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 4960–4964 
58. Chiu CC, Sainath TN, Wu Y, Prabhavalkar R, Nguyen P, Chen Z, Kannan A, Weiss RJ, Rao K, Gonina E et al (2018) State-of-the-art speech recognition with sequence-to-sequence models. In: 2018 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 4774–4778 
59. Graves A (2013) Generating sequences with recurrent neural networks. Preprint. arXiv:1308.0850 
60. Raffel C, Luong MT, Liu PJ, Weiss RJ, Eck D (2017) Online and linear-time attention by enforcing monotonic alignments. In: International conference on machine learning (PMLR), pp 2837–2846 
61. Chiu CC, Raffel C (2018) Monotonic chunkwise attention. In: International conference on learning representations 
62. Tian Q, Zhang Z, Liu C, Lu H, Chen L, Wei B, He P, Liu S (2020) FeatherTTS: robust and efficient attention based neural TTS. Preprint. arXiv:2011.00935 
63. Graves A, Fernández S, Gomez F, Schmidhuber J (2006) Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks. In: Proceedings of the 
23rd international conference on Machine learning, pp 369–376 
64. Wightman CW, Talkin DT (1997) The aligner: Text-to-speech alignment using Markov models. 
In: Progress in speech synthesis. Springer, pp 313–323 
65. McAuliffe M, Socolof M, Mihuc S, Wagner M, Sonderegger M (2017) Montreal forced aligner: trainable text-speech alignment using kaldi. In: Interspeech, vol 2017, pp 498–502 
66. Tan X, Chen J, Liu H, Cong J, Zhang C, Liu Y, Wang X, Leng Y, Yi Y, He L et al 
(2022) NaturalSpeech: end-to-end text to speech synthesis with human-level quality. Preprint. arXiv:2205.04421 
67. Bengio S, Vinyals O, Jaitly N, Shazeer N (2015) Scheduled sampling for sequence prediction with recurrent neural networks. In: Proceedings of the 28th international conference on neural information processing systems, vol 1, pp 1171–1179 
68. Wu L, Tan X, He D, Tian F, Qin T, Lai J, Liu TY (2018) Beyond error propagation in neural machine translation: characteristics of language also matter. In: Proceedings of the 2018 conference on empirical methods in natural language processing, pp 3602–3611 
69. Goyal A, Lamb A, Zhang Y, Zhang S, Courville A, Bengio Y (2016) Professor forcing: a new algorithm for training recurrent networks. In: Proceedings of the 30th international conference on neural information processing systems, pp 4608–4616 
70. Hinton G, Vinyals O, Dean J (2015) Distilling the knowledge in a neural network. Preprint. arXiv:1503.02531 
71. Kim Y, Rush AM (2016) Sequence-level knowledge distillation. In: Proceedings of the 2016 conference on empirical methods in natural language processing, pp 1317–1327 
72. Tan X, Ren Y, He D, Qin T, Liu TY (2019) Multilingual neural machine translation with knowledge distillation. In: International conference on learning representations. https:// openreview.net/forum?id=S1gUsoR9YX 
73. Tan X, Xia Y, Wu L, Qin T (2019) Efficient bidirectional neural machine translation. Preprint. arXiv:1908.09329
