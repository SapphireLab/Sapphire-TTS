# 10.Model-Efficient TTS

## Abstract

> Text-to-speech synthesis systems are usually deployed in different environments with various cloud-edge-end devices, which require fast synthesis speed, small memory storage, and low computation cost.
> However, early neural TTS models are usually with large computation/memory/time costs.
> In this chapter, we introduce the technologies for model-efficient TTS. 
> Since technologies can reduce the cost of computation, memory, or inference time at the same time, we do not categorize the technologies according to the aspects of computation/memory/time.
> Instead, we categorize according to the methods they use as follows: 
> (1) Parallel generation, as introduced in Sect. 10.1, which can increase the parallelism of the computation and improve the inference (or training) speed. 
> (2) Lightweight modeling, as introduced in Sect. 10.2, which aims to develop lightweight and efficient models with small model sizes, low computation, and fast inference speed. 
> (3) Efficient modeling with domain knowledge, as introduced in Sect. 10.3, which designs efficient models by leveraging the domain knowledge of speech.

## 10.1.Parallel Generation

> Previous neural TTS models usually adopt autoregressive mel-spectrogram and waveform generation, which are very slow considering the long speech sequence (e.g., a 5-s speech has 500 mel-spectrograms if hop size is 10 ms, and 120 k waveform points if the sampling rate is 24 kHz).
> To solve this problem, parallel generation has been leveraged to speed up the inference of TTS models. 
> Table 10.1 summarizes some typical modeling paradigms for sequence generation, the corresponding TTS models, and time complexity in training and inference. 
> As can be seen, TTS models that use RNN-based autoregressive models [1–4] are slow in both training and inference, with .O(N) computation, where N is the sequence length.
> To avoid the slow training time caused by RNN structure, 
> DeepVoice 3 [5] and TransformerTTS [6] leverage CNN or self-attention-based structure that can support parallel training but still require autoregressive inference. 
> In the following subsections, we introduce different parallel generation methods, including 
> (1) parallel generation with CNN/Transformer (Sect. 10.1.1), which modifies CNN/Transformer from autoregressive generation into non-autoregressive generation; 
> (2) parallel generation with GAN/VAE/Flow (Sect. 10.1.2), which leverages generative models with either a non-iterative (GAN/VAE) or iterative process (Flow) for a parallel generation; 
> (3) parallel generation with Diffusion (Sect. 10.1.3), which leverages diffusion models with an iterative process for a parallel generation.

### 10.1.1.Non-Autoregressive Generation with CNN or Transformer

> To speed up inference, FastSpeech 1/2 [7, 8] design a feed-forward Transformer that leverages self-attention structure for both parallel training and inference, where the computation is reduced to $\mathcal{O}(1)$.
> ParaNet [9] is a fully convolutional-based non-autoregressive model that can speed up the mel-spectrogram generation.
> To generate speech sequence in parallel with good voice quality, FastSpeech leverages an autoregressive teacher model for data distillation, similar to that used in Parallel WaveNet [10] and ClariNet [11].
> FastSpeech and Parallel WaveNet can both be regarded as an inverse autoregressive flow [12], with parallel inference but require teacher distillation for parallel training, as described in Sect. 6.2.2.
> FastSpeech 2 [8] improves FastSpeech by modeling more variance information, without the need for teacher distillation. 
> Besides pure non-autoregressive generation, there are other methods that combine autoregressive and non-autoregressive generation.
> For example, [13] proposes a semi-autoregressive mode for mel-spectrogram generation, where the mel-spectrograms are generated in an autoregressive manner inside each phoneme while in a non-autoregressive mode for different phonemes.

### 10.1.2.Non-Autoregressive Generation with GAN, VAE, or Flow

> Most GAN-based [8, 14–16] and VAE-based [17, 18] models for mel-spectrogram and waveform generation are non-autoregressive, with $\mathcal{O}(1)$ computation in both training and inference.
> As introduced in Sect. 3.3.2, normalizing flows based on bipartite transforms such as WaveGlow [19] and FloWaveNet [20] can ensure parallel training and inference.
> However, they usually need to stack multiple flow iterations T to ensure the quality of the mapping between data and prior distributions.

### 10.1.3.Iterative Generation with Diffusion

> Diffusion models [21–25] require multiple diffusion steps T in the forward and reverse process, which increases the computation.
> However, its training complexity is $\mathcal{O}(1)$ according to Algorithm 1 in Sect. 3.3.4 since we only need to forward the model once for a sampling step and data.
> Note that some diffusion models such as InferGrad [26] take inference procedure into training, and thus its training complexity is $\mathcal{O}(T)$. 
> The inference complexity of the diffusion model is usually $\mathcal{O}(T)$.
> However, there are a lot of methods to speed up the inference of the diffusion model.
> Here we just briefly overview some methods as follows: 
> - Improving prior distribution.
> By choosing a prior distribution that is closer to the data distribution, such as [23, 25, 27], the diffusion and denoising processes can be easier in training and inference, resulting in fewer sampling steps, and thus faster sampling speed.
> - Improving the forward/diffusion or reverse/denoising process.
> By learning the diffusion process [28], choosing a better schedule in reverse process [29], or taking the non-Markov assumption [30], the inference steps can be reduced. 
> - Combining diffusion model with other methods.
> We can further combine the diffusion model with other methods such as GAN [31] and VAE [32]. 
> - Speedup with SDE/ODE solver.
> We can formulate a diffusion model as an SDE or ODE process, and speed up the inference with some numerical SDE/ODE solvers [33].

## 10.2.Lightweight Modeling

> While non-autoregressive generation can fully leverage the parallel computation for inference speedup, the number of model parameters and total computation cost is not reduced, which makes it costly when deploying on mobile phones or embedded devices since the parallel computation capabilities and memory storage in these devices are not powerful enough.
> Therefore, we need to design lightweight and efficient models with small model sizes and less computation costs to satisfy different devices, even using autoregressive generation.
> Some widely used techniques for designing lightweight models include model compression (e.g., pruning, quantization, knowledge distillation [34]) and neural architecture search [35, 36], etc.
> We also introduce other modeling tricks for lightweight model design.

### 10.2.1 Model Compression

> WaveRNN [37] uses techniques like dual softmax, weight pruning, and subscale prediction to speed up inference.
> SqueezeWave [38] leverages waveform reshaping to reduce the temporal length and replaces the 1D convolution with depthwise separable convolution to reduce computation cost while achieving similar audio quality.
> Kanagawa and Ijima [39] compress the model parameters of LPCNet with tensor decomposition.
> Hsu and Lee [40] proposes a heavily compressed flow-based model to reduce computational resources, and a WaveNet-based post-filter to maintain audio quality.

### 10.2.2 Neural Architecture Search

> LightSpeech [35] leverages neural architecture search [41, 42] to find lightweight architectures to further speed up the inference of FastSpeech 2 [8] by 6.5. × while maintaining voice quality.

### 10.2.3 Other Technologies

> DeviceTTS [43] leverages the model structure of DFSMN [44] and mix-resolution decoder to predict multiple frames in one decoding step to speed up inference.
> LVC-Net [45] adopts a location-variable convolution for different waveform intervals, where the convolution coefficients are predicted from mel-spectrograms.
> It speeds up the Parallel WaveGAN vocoder by 4. × without any degradation in sound quality.

## 10.3 Efficient Modeling with Domain Knowledge

> Domain knowledge from speech can be leveraged to speed up inference, such as linear prediction [4], multiband modeling [46–48], subscale prediction [37], multi-frame prediction [1, 13, 43, 49, 50], streaming synthesis [51], etc.

### 10.3.1 Linear Prediction

> LPCNet [4] combines digital signal processing with neural networks, by using linear prediction coefficients to calculate the next waveform and a lightweight model to predict the residual value, which can speed the inference of autoregressive waveform generation.

### 10.3.2 Multiband Modeling

> Another technique that is widely used to speed up the inference of vocoders is subband modeling, which divides the waveform into multiple subbands for fast inference.
> Typical models include DurIAN [46], multi-band MelGAN [47], subband WaveNet [52], and multi-band LPCNet [48, 53].

### 10.3.3 Subscale Prediction

> WaveRNN [37] leverages subscale prediction to speed up inference.

### 10.3.4 Multi-Frame Prediction

> In Tacotron 1/2, to help the attention alignment learning [1] between character/phoneme sequence and mel-spectrogram sequence, the decoder predicts multiple frames at each autoregressive step [54].
> As a by-product, it can also help speed up the inference process.

### 10.3.5 Streaming or Chunk-Wise Synthesis

> Streaming TTS [51, 55–61] synthesizes speech once some input tokens are coming, without waiting for the whole input sentence, which can also speed up inference.

### 10.3.6 Other Technologies

> Bunched LPCNet [62] reduces the computation complexity of LPCNet with sample bunching and bit bunching, achieving more than 2. × speedup.
> FFTNet [63] uses a  simple architecture to mimic the Fast Fourier Transform (FFT), which can generate audio samples in real time.
> Okamoto et al. [64] further enhances FFTNet with noise shaping and subband techniques, improving the voice quality while keeping a small model size.
> Popov et al. [65] propose frame splitting and cross-fading to synthesize some parts of the waveform in parallel and then concatenate the synthesized waveforms together to ensure fast synthesis on low-end devices.
> Kang et al. [66] accelerate DCTTS [67] with network reduction and fidelity improvement techniques such as group highway activation, which can synthesize speech in real time with a single CPU thread.

## References

1. Wang Y, Skerry-Ryan R, Stanton D, Wu Y, Weiss RJ, Jaitly N, Yang Z, Xiao Y, Chen Z, Bengio S et al (2017) Tacotron: Towards end-to-end speech synthesis. In: Proceedings of the Interspeech 2017, pp 4006–4010 
2. Shen J, Pang R, Weiss RJ, Schuster M, Jaitly N, Yang Z, Chen Z, Zhang Y, Wang Y, Skerry-Ryan R et al (2018) Natural TTS synthesis by conditioning WaveNet on mel spectrogram predictions. In: 2018 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 4779–4783 
3. Mehri S, Kumar K, Gulrajani I, Kumar R, Jain S, Sotelo J, Courville A, Bengio Y (2017) SampleRNN: an unconditional end-to-end neural audio generation model. In: International conference on learning representations
4. Valin JM, Skoglund J (2019) LPCNet: improving neural speech synthesis through linear prediction. In: ICASSP 2019-2019 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 5891–5895 
5. Ping W, Peng K, Gibiansky A, Arik SO, Kannan A, Narang S, Raiman J, Miller J (2018) Deep voice 3: 2000-speaker neural text-to-speech. In: Proceedings of the international conference on learning representations, pp 214–217 
6. Li N, Liu S, Liu Y, Zhao S, Liu M (2019) Neural speech synthesis with Transformer network. In: Proceedings of the AAAI conference on artificial intelligence, vol 33, pp 6706–6713 
7. Ren Y, Ruan Y, Tan X, Qin T, Zhao S, Zhao Z, Liu TY (2019) FastSpeech: fast, robust and controllable text to speech. In: NeurIPS 
8. Ren Y, Hu C, Tan X, Qin T, Zhao S, Zhao Z, Liu TY (2021) FastSpeech 2: fast and high-quality end-to-end text to speech. In: International conference on learning representations. https://openreview.net/forum?id=piLPYqxtWuA 
9. Peng K, Ping W, Song Z, Zhao K (2020) Non-autoregressive neural text-to-speech. In: International conference on machine learning (PMLR), pp 7586–7598 
10. van der Oord A, Li Y, Babuschkin I, Simonyan K, Vinyals O, Kavukcuoglu K, Driessche G, Lockhart E, Cobo L, Stimberg F et al (2018) Parallel WaveNet: fast high-fidelity speech synthesis. In: International conference on machine learning (PMLR), pp 3918–3926 
11. Ping W, Peng K, Chen J (2018) ClariNet: parallel wave generation in end-to-end text-to-speech. In: International conference on learning representations 
12. Kingma DP, Salimans T, Jozefowicz R, Chen X, Sutskever I, Welling M (2016) Improved variational inference with inverse autoregressive flow. Adv Neural Inf Process Syst 29:4743–4751 
13. Wang D, Deng L, Zhang Y, Zheng N, Yeung YT, Chen X, Liu X, Meng H (2021) FCL-TACO2: towards fast, controllable and lightweight text-to-speech synthesis. In: ICASSP 2021 - 2021 IEEE international conference on acoustics, speech and signal processing 
14. Kumar K, Kumar R, de Boissiere T, Gestin L, Teoh WZ, Sotelo J, de Brébisson A, Bengio Y, Courville A (2019) MelGAN: generative adversarial networks for conditional waveform synthesis. In: NeurIPS 
15. Kong J, Kim J, Bae J (2020) HiFi-GAN: generative adversarial networks for efficient and high fidelity speech synthesis. Adv Neural Inf Process Syst 33 
16. Donahue J, Dieleman S, Bi´nkowski M, Elsen E, Simonyan K (2021) End-to-end adversarial text-to-speech. In: International conference on learning representations 
17. Kim J, Kong J, Son J (2021) Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech. Preprint. arXiv:2106.06103 
18. Tan X, Chen J, Liu H, Cong J, Zhang C, Liu Y, Wang X, Leng Y, Yi Y, He L, et al. (2022) NaturalSpeech: end-to-end text to speech synthesis with human-level quality. Preprint. arXiv:2205.04421 
19. Prenger R, Valle R, Catanzaro B (2019) WaveGlow: a flow-based generative network for speech synthesis. In: ICASSP 2019-2019 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 3617–3621 
20. Kim S, Lee SG, Song J, Kim J, Yoon S (2019) FloWaveNet: a generative flow for raw audio. In: International conference on machine learning (PMLR), pp 3370–3378 
21. Chen N, Zhang Y, Zen H, Weiss RJ, Norouzi M, Chan W (2021) WaveGrad: estimating gradients for waveform generation. In: International conference on learning representations 
22. Kong Z, Ping W, Huang J, Zhao K, Catanzaro B (2021) DiffWave: a versatile diffusion model for audio synthesis. In: International conference on learning representations 
23. Lee S-G, Kim H, Shin C, Tan X, Liu C, Meng Q, Qin T, Chen W, Yoon S, Liu TY (2021) PriorGrad: improving conditional denoising diffusion models with data-driven adaptive prior. Preprint. arXiv:2106.06406 
24. Jeong M, Kim H, Cheon SJ, Choi BJ, Kim NS (2021) Diff-TTS: a denoising diffusion model for text-to-speech. Preprint. arXiv:2104.01409 
25. Popov V, Vovk I, Gogoryan V, Sadekova T, Kudinov M (2021) Grad-TTS: a diffusion probabilistic model for text-to-speech. Preprint. arXiv:2105.06337
26. Chen Z, Tan X, Wang K, Pan S, Mandic D, He L, Zhao S (2022) InferGrad: improving diffusion models for vocoder by considering inference in training. In: ICASSP 2022-2022 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 8432–8436 
27. Koizumi Y, Zen H, Yatabe K, Chen N, Bacchiani M (2022) SpecGrad: diffusion probabilistic model based neural vocoder with adaptive noise spectral shaping. Preprint. arXiv:2203.16749 
28. Kingma D, Salimans T, Poole B, Ho J (2021) Variational diffusion models. Adv Neural Inf Process Syst 34:21696–21707 
29. Nichol AQ, Dhariwal P (2021) Improved denoising diffusion probabilistic models. In: International conference on machine learning (PMLR), pp 8162–8171 
30. Song J, Meng C, Ermon S (2020) Denoising diffusion implicit models. Preprint. arXiv:2010.02502 
31. Xiao Z, Kreis K, Vahdat A (2021) Tackling the generative learning trilemma with denoising diffusion GANs. In: International conference on learning representations 
32. Rombach R, Blattmann A, Lorenz D, Esser P, Ommer B (2022) High-resolution image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp 10684–10695 
33. Lu C, Zhou Y, Bao F, Chen J, Li C, Zhu J (2022) DPM-Solver: a fast ODE solver for diffusion probabilistic model sampling in around 10 steps. Preprint. arXiv:2206.00927 
34. Hinton G, Vinyals O, Dean J (2015) Distilling the knowledge in a neural network. Preprint. arXiv:1503.02531 
35. Luo R, Tan X, Wang R, Qin T, Li J, Zhao S, Chen E, Liu TY (2021) LightSpeech: lightweight and fast text to speech with neural architecture search. In: 2021 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE 
36. Xu J, Tan X, Luo R, Song K, Li J, Qin T, Liu TY (2021) NAS-BERT: task-agnostic and adaptive-size BERT compression with neural architecture search. In: Proceedings of the 27th ACM SIGKDD international conference on knowledge discovery and data mining 
37. Kalchbrenner N, Elsen E, Simonyan K, Noury S, Casagrande N, Lockhart E, Stimberg F, van der Oord A, Dieleman S, Kavukcuoglu K (2018) Efficient neural audio synthesis. In: International conference on machine learning (PMLR), pp 2410–2419 
38. Zhai B, Gao T, Xue F, Rothchild D, Wu B, Gonzalez JE, Keutzer K (2020) SqueezeWave: extremely lightweight vocoders for on-device speech synthesis. Preprint. arXiv:2001.05685 
39. Kanagawa H, Ijima Y (2020) Lightweight LPCNet-based neural vocoder with tensor decomposition. In: Proceedings of the Interspeech 2020, pp 205–209 
40. Hsu P-C, Lee Hy (2020) WG-WaveNet: real-time high-fidelity speech synthesis without gpu. In: Proceedings of the Interspeech 2020, pp 210–214 
41. Zoph B, Le QV (2016) Neural architecture search with reinforcement learning. Preprint. arXiv:1611.01578 
42. Luo R, Tan X, Wang R, Qin T, Chen E, Liu TY (2020) Neural architecture search with GBDT. Preprint. arXiv:2007.04785 
43. Huang Z, Li H, Lei M (2020) DeviceTTS: a small-footprint, fast, stable network for on-device text-to-speech. Preprint. arXiv:2010.15311 
44. Zhang S, Lei M, Yan Z, Dai L (2018) Deep-FSMN for large vocabulary continuous speech recognition. In: 2018 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 5869–5873 
45. Zeng Z, Wang J, Cheng N, Xiao J (2021) Lvcnet: Efficient condition-dependent modeling network for waveform generation. Preprint. arXiv:2102.10815 
46. Yu C, Lu H, Hu N, Yu M, Weng C, Xu K, Liu P, Tuo D, Kang S, Lei G et al (2020) DurIAN: duration informed attention network for speech synthesis. In: Proceedings of the Interspeech 2020, pp 2027–2031 
47. Yang G, Yang S, Liu K, Fang P, Chen W, Xie L (2020) Multi-band MelGAN: faster waveform generation for high-quality text-to-speech. Preprint. arXiv:2005.05106 
48. Cui Y, Wang X, He L, Soong FK (2020) An efficient subband linear prediction for LPCNet-based neural synthesis. In: INTERSPEECH, pp 3555–3559
49. Zen H, Agiomyrgiannakis Y, Egberts N, Henderson F, Szczepaniak P (2016) Fast, compact, and high quality LSTM-RNN based statistical parametric speech synthesizers for mobile devices. In: Proceedings of the Interspeech 2016, pp 2273–2277 
50. Liu R, Sisman B, Lin Y, Li H (2021) FastTalker: a neural text-to-speech architecture with shallow and group autoregression. Neural Netw 141:306–314 
51. Ellinas N, Vamvoukakis G, Markopoulos K, Chalamandaris A, Maniati G, Kakoulidis P, Raptis S, Sung JS, Park H, Tsiakoulis P (2020) High quality streaming speech synthesis with low, sentence-length-independent latency. In: Proceedings of the Interspeech 2020, pp 2022–2026 
52. Okamoto T, Tachibana K, Toda T, Shiga Y, Kawai H (2018) An investigation of subband WaveNet vocoder covering entire audible frequency range with limited acoustic features. In: 2018 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 5654–5658 
53. Tian Q, Zhang Z, Lu H, Chen LH, Liu S (2020) Featherwave: an efficient high-fidelity neural vocoder with multi-band linear prediction. In: Proceedings of the Interspeech 2020, pp 195-199 
54. Chen M, Tan X, Ren Y, Xu J, Sun H, Zhao S, Qin T (2020) MultiSpeech: multi-speaker text to speech with Transformer. In: INTERSPEECH, pp 4024–4028 
55. Ma M, Zheng B, Liu K, Zheng R, Liu H, Peng K, Church K, Huang L (2020) Incremental text-to-speech synthesis with prefix-to-prefix framework. In: Proceedings of the 2020 conference on empirical methods in natural language processing: findings, pp 3886–3896 
56. Stephenson B, Besacier L, Girin L, Hueber T (2020) What the future brings: Investigating the impact of lookahead for incremental neural TTS. In: Proceedings of the Interspeech 2020, pp 215–219 
57. Yanagita T, Sakti S, Nakamura S (2019) Neural iTTS: toward synthesizing speech in real-time with end-to-end neural text-to-speech framework. In: Proceedings of the 10th ISCA speech synthesis workshop, pp 183–188 
58. Stephenson B, Hueber T, Girin L, Besacier L (2021) Alternate endings: improving prosody for incremental neural TTS with predicted future text input. Preprint. arXiv:2102.09914 
59. Mohan DSR, Lenain R, Foglianti L, Teh TH, Staib M, Torresquintero A, Gao J (2020) Incremental text to speech for neural sequence-to-sequence models using reinforcement learning. In: Proceedings of the Interspeech 2020, pp 3186–3190 
60. Saeki T, Takamichi S, Saruwatari H (2021) Low-latency incremental text-to-speech synthesis with distilled context prediction network. In: 2021 IEEE automatic speech recognition and understanding workshop (ASRU). IEEE, pp 749–756 
61. Saeki T, Takamichi S, Saruwatari H (2021) Incremental text-to-speech synthesis using pseudo lookahead with large pretrained language model. IEEE Signal Process Lett 28:857–861 
62. Vipperla R, Park S, Choo K, Ishtiaq S, Min K, Bhattacharya S, Mehrotra A, Ramos AGC, Lane ND (2020) Bunched LPCNet: vocoder for low-cost neural text-to-speech systems. In: Proceedings of the Interspeech 2020, pp 3565–3569 
63. Jin Z, Finkelstein A, Mysore GJ, Lu J (2018) FFTNet: a real-time speaker-dependent neural vocoder. In: 2018 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 2251–2255 
64. Okamoto T, Toda T, Shiga Y, Kawai H (2018) Improving FFTNet vocoder with noise shaping and subband approaches. In: 2018 IEEE spoken language technology workshop (SLT). IEEE, pp 304–311 
65. Popov V, Kamenev S, Kudinov M, Repyevsky S, Sadekova T, Bushaev VK, Parkhomenko D (2020) Fast and lightweight on-device TTS with Tacotron2 and LPCNet. In: Proceedings of the Interspeech 2020, pp 220–224 
66. Kang M, Lee J, Kim S, Kim I (2021) Fast DCTTS: efficient deep convolutional text-to-speech. Preprint. arXiv:2104.00624 
67. Tachibana H, Uenoyama K, Aihara S (2018) Efficiently trainable text-to-speech system based on deep convolutional networks with guided attention. In: 2018 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 4784–4788