# 1.Introduction

## Abstract

> In this chapter, we first discuss the motivation of this book and then introduce the history of text-to-speech (TTS) technologies.
> We further overview neural network-based TTS through different taxonomies, and finally, describe the organization of this book. 

## 1.1.Motivation

> Speaking is one of the most important language capabilities (the others being listening, reading, and writing) of human beings.
> Text-to-speech synthesis (TTS or speech synthesis for short), which aims to generate intelligible and natural speech from text [1, 2], plays a key role to enable machines to speak [3], and is an important task in artificial intelligence and natural language/speech processing [4– 6].
> > Broadly speaking, speech synthesis covers the tasks to generate speech from any information source, such as from text (text-to-speech synthesis) or from another voice (voice conversion). 
> Here we use speech synthesis to denote text-to-speech synthesis from a narrow sense.
>
> Developing a TTS system requires knowledge of languages and human speech production, and involves multiple disciplines including linguistics [7], acoustics [8], digital signal processing [9], and machine learning [10, 11]. 

> Previous works usually leverage concatenative [12] or statistical parametric [13] based methods to build TTS systems, which suffer from poor generation quality (e.g., low intelligibility and naturalness, voice artifacts such as muffled, buzzing, noisy, or robotic voice).
> With the development of deep learning [14, 15], neural network-based TTS (neural TTS for short) has evolved quickly and improved the intelligibility and naturalness of generated speech a lot [16–23].
> Neural TTS discards most of the prior knowledge needed in previous TTS systems and conducts end-to-end learning purely from data.
> Due to the powerful capabilities to learn data representations (representation learning) and model data distributions (generative modeling), neural TTS can achieve high voice quality that is as natural as human recordings [24].

> A systematic and comprehensive introduction to neural TTS and a good understanding of the current research status and future research trends are very helpful and necessary for people working on TTS.
> While there is a lot of research work focusing on different aspects of neural TTS and there are also some survey papers on neural TTS [2, 25–31], a comprehensive book to introduce neural TTS is necessary, especially in the era of content creation and metaverse where there is a strong demand on text-to-speech synthesis. 

> This book originates from our previous survey paper and tutorials: 
> - A Survey on Neural Speech Synthesis, https://arxiv.org/abs/2106.15561. 
> - TTS tutorial at ISCSLP 2021, https://tts-tutorial.github.io/iscslp2021/. 
> - TTS tutorial at IJCAI 2021, https://github.com/tts-tutorial/ijcai2021. 
> - TTS tutorial at ICASSP 2022, https://github.com/tts-tutorial/icassp2022. 
> - TTS tutorial at INTERSPEECH 2022, https://github.com/tts-tutorial/interspeech2022.

> Readers can use this GitHub page (https://github.com/tts-tutorial/book) to check updates and initiate discussions on this book.

> In the following sections, we briefly review the history of TTS technologies, introduce some basic knowledge of neural TTS, and describe the taxonomy of neural TTS and the organization of this book. 

## 1.2.History of TTS Technology

> People have tried to build machines to synthesize human speech dating back to the twelfth century [32].
> In the 2nd half of the eighteenth century, the Hungarian scientist, Wolfgang von Kempelen, had constructed a speaking machine with a series of bellows, springs, bagpipes, and resonance boxes to produce some simple words and short sentences [33].
> The first speech synthesis system that was built upon computers came out in the latter half of the twentieth century [32].
> The early computer-based speech synthesis methods include articulatory synthesis [34, 35], formant synthesis [36–39], and concatenative synthesis [12, 40–43].
> Later, with the development of statistical machine learning, statistical parametric speech synthesis (SPSS) is proposed [13, 44–46], which predicts parameters such as spectrum, fundamental frequency, and duration for speech synthesis.
> Since the 2010s, neural network-based speech synthesis [16–18, 47–51] has gradually become the dominant method and achieved much better voice quality. 

### 1.2.1.Articulatory Synthesis

> Articulatory synthesis [34, 35] produces speech by simulating the behavior of human articulators such as lips, tongue, glottis, and moving vocal tract.
> Ideally, articulatory synthesis can be the most effective method for speech synthesis since it is the way how human generates speech.
> However, it is very difficult to model these articulator behaviors in practice.
> For example, it is hard to collect the data for articulator simulation.
> Therefore, the speech quality by articulatory synthesis is usually worse than that by later formant synthesis and concatenative synthesis. 

### 1.2.2.Formant Synthesis

> Formant synthesis [36–38] produces speech based on a set of rules that control a simplified source-filter model.
> These rules are usually developed by linguists to mimic the formant structure and other spectral properties of speech as closely as possible.
> The speech is synthesized by an additive synthesis module and an acoustic model with varying parameters like fundamental frequency, voicing, and noise levels.
> Formant synthesis can produce highly intelligible speech with moderate computation resources that are well-suited for embedded systems and does not rely on large-scale human speech corpus as in concatenative synthesis.
> However, the synthesized speech sounds less natural and has artifacts.
> Moreover, it is challenging to specify rules for synthesis. 

### 1.2.3.Concatenative Synthesis

> Concatenative synthesis [12, 40–43] relies on the concatenation of speech segments that are stored in a database.
> In inference, the concatenative TTS system searches speech segments to match the given input text and produces a speech waveform by concatenating these units together.
> There are mainly two types of concatenative speech synthesis: diphone synthesis [41] and unit selection synthesis [12].
> Diphone synthesis leverages diphones that describe the transitions between phones, and stores a single example of each diphone in the database, while unit selection synthesis leverages speech units ranging from whole sentences to individual phones and stores multiple segments of each unit in the database.

> Generally speaking, concatenative TTS can generate audio with high intelligibility and authentic timbre close to the original voice actor.
> However, concatenative TTS requires a vast recording database in order to cover all possible combinations of speech units for spoken words.
> Another drawback is that the generated voice is less natural and emotional since concatenation can result in less smoothness in stress, emotion, prosody, etc. 

### 1.2.4.Statistical Parametric Speech Synthesis

> To address the drawbacks of concatenative TTS, statistical parametric speech synthesis (SPSS) is proposed [13, 44–46, 52].
> The basic idea is that instead of direct generating waveform through concatenation, we can first generate the acoustic parameters [53–55] that are necessary to produce speech and then recover speech from the generated acoustic parameters using some algorithms [56–59].
> SPSS usually consists of three components: a text analysis module, a parameter prediction module (acoustic model), and a vocoder analysis/synthesis module (vocoder).
> The text analysis module first processes the text, including text normalization [60], grapheme-to-phoneme conversion [61], word segmentation, etc.
> Then it extracts the linguistic features, such as phonemes and POS tags from different granularities.
> The acoustic models (e.g., hidden Markov model (HMM) based) are trained with the paired linguistic features and parameters (acoustic features), where the acoustic features include fundamental frequency, spectrum or cepstrum [53, 54], etc, and are extracted from the speech through vocoder analysis [56, 58, 59].
> The vocoders synthesize speech from the predicted acoustic features.
> SPSS has several advantages over previous TTS systems: (1) flexibility, as it is convenient to modify the parameters to control the generated speech; (2) low data cost, as it requires fewer recordings than concatenative synthesis.
> However, SPSS also has its drawbacks: (1) the generated speech has lower audio fidelity due to artifacts such as muffled, buzzing, or noisy audio; (2) the generated voice is still robotic and can be easily differentiated from human recording speech.

> In the 2010s, as neural networks and deep learning have achieved rapid progress, some works first introduce deep neural networks into SPSS, such as deep neural network (DNN) based [16, 47] and recurrent neural network (RNN) based [48, 49, 62].
> However, these models replace HMM with neural networks and still predict the acoustic features from linguistic features, which follow the paradigm of SPSS.
> Later, [50] propose directly generating acoustic features from phoneme sequence instead of linguistic features, which is the first encoder-decoder-based TTS model with a sequence-to-sequence framework.
> In this book, we focus on neural-based speech synthesis and especially end-to-end models. Since later SPSS also uses neural networks as the acoustic models, we briefly describe these models but do not dive deep into the details. 
> > The term “end-to-end” in TTS has a vague meaning.
> In early studies, “end-to-end” refers to the text-to-spectrogram model being end-to-end but still using a separate waveform synthesizer (vocoder).
> It can also broadly refer to the neural-based TTS models which do not use complicated linguistic or acoustic features.
> For example, WaveNet [17] does not use acoustic features but directly generates waveform from linguistic features, and Tacotron [18] does not use linguistic features but directly generates spectrogram from character or phoneme.
> However, the strict end-to-end model refers to directly generating waveform from text.
> Therefore, in this paper we use “end-to-end”, “more end-to-end”, and “fully end-to-end” to differentiate the degree of end-to-end for TTS models. 

## 1.3.Overview of Neural TTS

### 1.3.1.TTS in the Era of Deep Learning

> With the development of deep learning, neural network-based TTS (neural TTS for short) is proposed, which adopts (deep) neural networks as the model backbone for speech synthesis.
> Some early neural models are adopted in SPSS to replace HMM for acoustic modeling.
> Later, WaveNet [17] is proposed to directly generate waveform from linguistic features, which can be regarded as the first modern neural TTS model.
> Other models like DeepVoice 1/2 [63, 64] still follow the three components in statistical parametric synthesis but upgrade them with the corresponding neural network-based models.
> Furthermore, some end-to-end models (e.g., Char2Wav [65], Tacotron 1/2 [18, 19], Deep Voice 3 [21], and FastSpeech 1/2 [23, 66]) are proposed to simplify text analysis modules and directly take character/phoneme sequences as input and simplify acoustic features with mel-spectrograms.
> Later, fully end-to-end TTS systems are developed to directly generate waveform from text, such as ClariNet [67], FastSpeech 2s [66], EATS [68], and NaturalSpeech [24].
> Compared to previous TTS systems based on concatenative synthesis and statistical parametric synthesis, the advantages of neural network-based speech synthesis include high voice quality in terms of both intelligibility and naturalness and less requirement on human preprocessing and feature development. 

### 1.3.2.Key Components of TTS

> A neural TTS system consists of three basic components: a text analysis module, an acoustic model, and a vocoder.
> > Although some end-to-end models do not explicitly use text analysis (e.g., Tacotron 2 [19]), acoustic models (e.g., WaveNet [17]), or vocoders (e.g., Tacotron [18]), and some systems only use a single end-to-end model (e.g., FastSpeech 2s [66] and NaturalSpeech [24]), using these components are still popular in current TTS research and product. 

> As shown in Fig. 1.1, the text analysis module converts a text sequence into linguistic features, the acoustic models generate acoustic features from linguistic features, and then the vocoders synthesize waveform from acoustic features.
> We introduce the three components of neural TTS in Part II.
> Specifically, we first introduce the main taxonomy for the basic components of neural TTS and then introduce text analysis, acoustic models, and vocoders in Chaps. 4, 5, and 6 respectively. We further introduce fully end-to-end TTS in Chap. 7.

### 1.3.3.Advanced Topics in TTS

> Besides the key components of neural TTS, we further categorize neural TTS according to some advanced topics and introduce the details in Part III: 
> (1) To improve the naturalness and expressiveness, we introduce how to model, control, and transfer the style/prosody of speech in order to generate expressive speech (Chap. 8); 
> (2) Since TTS models are facing robustness issues where word skipping and repeating problems in generated speech affect the speech quality, we introduce how to improve the robustness of speech synthesis (Chap. 9); 
> (3) Since neural TTS is modeled as a sequence-to-sequence generation task that leverages deep neural networks as the model backbone and generates speech in an autoregressive way, it usually requires large inference time and high computation/memory cost.
> Thus, we introduce how to speed up the autoregressive generation and reduce the model and computation size (Chap. 10); 
> (4) In low data resource scenarios where the data to train a TTS model is insufficient, the synthesized speech may be of both low intelligibility and naturalness.
> Therefore, we introduce how to build data-efficient TTS models for both new languages and new speakers (Chap. 11); 
> (5) At last, we briefly introduce some tasks beyond text-to-speech synthesis, including singing voice synthesis, voice conversion, speech enhancement, and speech separation (Chap. 12). 

### 1.3.4.Other Taxonomies of TTS

> Besides the main taxonomy from the perspective of key components and advanced topics, we can also categorize neural TTS from several different taxonomies, as shown in Fig. 1.2: (1) Autoregressive or non-autoregressive.
> We can divide neural TTS into autoregressive and non-autoregressive generations. (2) Generative model.
> Since TTS is a typical sequence generation task and can be modeled through deep generative models, we categorize it in terms of different generative models: normal sequence generation model, normalizing flows (Flow), generative adversarial networks (GAN), variational auto-encoders (VAE), and denoising diffusion probabilistic models (DDPM or Diffusion). (3) Network structure.
> We can divide neural TTS according to the network structures, such as CNN, RNN, self-attention, and hybrid structures (which contain more than one type of structure, such as CNN+RNN, and CNN+self-attention). 

### 1.3.5.Evolution of Neural TTS

> In order to better understand the development of neural TTS, we illustrate the evolution of neural TTS models, as shown in Fig. 1.3.
> Note that we organize the research works according to the time that the paper is open to the public (e.g., put on arXiv), but not formally published later.
> We choose the early time since we appreciate researchers making their paper public early to encourage knowledge sharing.
> Since the research works on neural TTS are so abundant, we only choose some representative works in Fig. 1.3, and list more works in Table B.1. 

## 1.4.Organization of This Book

> In the introduction (Chap. 1), we describe the motivation of this book, the history of TTS technologies, and the recently developed neural TTS technology, and then introduce some taxonomies about neural TTS.
> The remaining of this book consists of four parts: Preliminary (Part I), Key Components in TTS (Part II), Advanced Topics in TTS (Part III), and Summary/Outlook (Part IV), as shown in Fig. 1.4.
> In Part I, some preliminary knowledge about neural TTS is introduced, including the basics of spoken language processing (Chap. 2) and deep learning (Chap. 3) since neural TTS involves the processing of spoken languages and leverages the methods from deep learning.
> Particularly, since TTS is a data generation task that relies on deep generative models, we cover some background of deep generative models in Chap. 3.
> In Part II, we introduce several key components in neural TTS, including text analyses (Chap. 4), acoustic models (Chap. 5), vocoders (Chap. 6), and fully end-to-end models (Chap. 7).
> In Part III, we introduce some advanced topics to address these challenges in neural TTS, including expressive and controllable TTS (Chap. 8), robust TTS (Chap. 9), model-efficient TTS (Chap. 10), data-efficient TTS (Chap. 11), and some tasks beyond TTS (Chap. 12), such as singing voice synthesis, voice conversion, and speech enhancement/separation.
> In the last part, we summarize this book and discuss future research directions (Chap. 13).
> To further enrich this book, we summarize TTS-related resources including open-source implementations, corpora, and TTS model list in Appendix. 

## 1.5.References (100)

1. Taylor P (2009) Text-to-speech synthesis. Cambridge University Press 
2. Tan X, Qin T, Soong F, Liu TY (2021) A survey on neural speech synthesis. Preprint. arXiv:2106.15561 
3. Adler RB, Rodman GR, Sévigny A (1991) Understanding human communication. Holt, Rinehart and Winston Chicago 
4. Russell S, Norvig P (2020) Artificial intelligence: a modern approach (4th Edition). Pearson. http://aima.cs.berkeley.edu/ 
5. Manning C, Schutze H (1999) Foundations of statistical natural language processing. MIT Press 
6. Jurafsky D (2000) Speech & language processing. Pearson Education India 
7. De Saussure F (2011) Course in general linguistics. Columbia University Press 
8. Kinsler LE, Frey AR, Coppens AB, Sanders JV (1999) Fundamentals of acoustics. John Wiley & Sons 
9. Yuen CK (1978) Review of “Theory and Application of Digital Signal Processing” by Lawrence R. Rabiner and Bernard Gold. IEEE Trans Syst Man Cybern 8(2):146. https://doi. org/10.1109/TSMC.1978.4309918 
10. Bishop CM (2006) Pattern recognition and machine learning. Springer 
11. Jordan MI, Mitchell TM (2015) Machine learning: trends, perspectives, and prospects. Science 349(6245):255–260 
12. Hunt AJ, Black AW (1996) Unit selection in a concatenative speech synthesis system using a large speech database. In: 1996 IEEE International conference on acoustics, speech, and signal processing conference proceedings, vol 1. IEEE, pp 373–376 
13. Zen H, Tokuda K, Black AW (2009) Statistical parametric speech synthesis. Speech Commun 51(11):1039–1064 
14. LeCun Y, Bengio Y, Hinton G (2015) Deep learning. Nature 521(7553):436–444 
15. Goodfellow I, Bengio Y, Courville A (2016) Deep learning. MIT Press 
16. Zen H, Senior A, Schuster M (2013) Statistical parametric speech synthesis using deep neural networks. In: 2013 IEEE international conference on acoustics, speech and signal processing. IEEE, pp 7962–7966 
17. van den Oord A, Dieleman S, Zen H, Simonyan K, Vinyals O, Graves A, Kalchbrenner N, Senior A, Kavukcuoglu K (2016) WaveNet: A generative model for raw audio. Preprint. arXiv:1609.03499 
18. Wang Y, Skerry-Ryan R, Stanton D, Wu Y, Weiss RJ, Jaitly N, Yang Z, Xiao Y, Chen Z, Bengio S, et al (2017) Tacotron: Towards end-to-end speech synthesis. In: Proc Interspeech 2017, pp 4006–4010 
19. Shen J, Pang R, Weiss RJ, Schuster M, Jaitly N, Yang Z, Chen Z, Zhang Y, Wang Y, Skerry-Ryan R, et al (2018) Natural TTS synthesis by conditioning WaveNet on mel spectrogram predictions. In: 2018 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 4779–4783 
20. Kalchbrenner N, Elsen E, Simonyan K, Noury S, Casagrande N, Lockhart E, Stimberg F, Oord A, Dieleman S, Kavukcuoglu K (2018) Efficient neural audio synthesis. In: International conference on machine learning. PMLR, pp 2410–2419 
21. Ping W, Peng K, Gibiansky A, Arik SO, Kannan A, Narang S, Raiman J, Miller J (2018) Deep Voice 3: 2000-speaker neural text-to-speech. In: Proc ICLR, pp 214–217 
22. Li N, Liu S, Liu Y, Zhao S, Liu M (2019) Neural speech synthesis with Transformer network. In: Proceedings of the AAAI conference on artificial intelligence, vol 33, pp 6706–6713 
23. Ren Y, Ruan Y, Tan X, Qin T, Zhao S, Zhao Z, Liu TY (2019) FastSpeech: fast, robust and controllable text to speech. In: NeurIPS 
24. Tan X, Chen J, Liu H, Cong J, Zhang C, Liu Y, Wang X, Leng Y, Yi Y, He L, et al 
(2022) NaturalSpeech: End-to-end text to speech synthesis with human-level quality. Preprint. arXiv:2205.04421
25. Tabet Y, Boughazi M (2011) Speech synthesis techniques. a survey. In: International workshop on systems, signal processing and their applications, WOSSPA. IEEE, pp 67–70 
26. Mali P (2014) A survey on text to speech translation of multi language. Int J Res Adv Eng Technol. ISSN 2347-2812 
27. Siddhi D, Verghese JM, Bhavik D (2017) Survey on various methods of text to speech synthesis. Int J Comput Appl 165(6):26–30 
28. Ning Y, He S, Wu Z, Xing C, Zhang LJ (2019) A review of deep learning based speech synthesis. Appl Sci 9(19):4050 
29. Hsu Pc, Wang Ch, Liu AT, Lee Hy (2019) Towards robust neural vocoding for speech generation: A survey. Preprint. arXiv:1912.02461 
30. Panda SP, Nayak AK, Rai SC (2020) A survey on speech synthesis techniques in Indian languages. Multimedia Syst 26:453–478 
31. Mu Z, Yang X, Dong Y (2021) Review of end-to-end speech synthesis technology based on deep learning. Preprint. arXiv:2104.09995 
32. Wikipedia (2021) Speech synthesis — Wikipedia, the free encyclopedia. http://en.wikipedia. org/w/index.php?title=Speech%20synthesis&oldid=1020857981 
33. Dudley H, Tarnoczy TH (1950) The speaking machine of Wolfgang von Kempelen. J Acoust Soc Am 22(2):151–166 
34. Coker CH (1976) A model of articulatory dynamics and control. Proc IEEE 64(4):452–460 
35. Shadle CH, Damper RI (2001) Prospects for articulatory synthesis: a position paper. In: 4th ISCA tutorial and research workshop (ITRW) on speech synthesis 
36. Seeviour P, Holmes J, Judd M (1976) Automatic generation of control signals for a parallel formant speech synthesizer. In: ICASSP’76. IEEE International conference on acoustics, speech, and signal processing, vol 1. IEEE, pp 690–693 
37. Allen J, Hunnicutt S, Carlson R, Granstrom B (1979) MITalk-79: The 1979 MIT text-to-speech system. J Acoust Soc Am 65(S1):S130–S130 
38. Klatt DH (1980) Software for a cascade/parallel formant synthesizer. J Acoust Soc Am 67(3):971–995 
39. Klatt DH (1987) Review of text-to-speech conversion for English. J Acoust Soc Am 82(3):737–793 
40. Olive J (1977) Rule synthesis of speech from dyadic units. In: ICASSP’77. IEEE International conference on acoustics, speech, and signal processing, vol 2. IEEE, pp 568–570 
41. Moulines E, Charpentier F (1990) Pitch-synchronous waveform processing techniques for text-to-speech synthesis using diphones. Speech Commun 9(5–6):453–467 
42. Sagisaka Y, Kaiki N, Iwahashi N, Mimura K (1992) ATR ν-Talk speech synthesis system. In: Second international conference on spoken language processing 
43. Taylor P, Black AW, Caley R (1998) The architecture of the Festival speech synthesis system. In: The Third ESCA/COCOSDA Workshop on Speech Synthesis, Blue Mountains, Australia, November 26–29, 1998. ISCA, pp 147–152. http://www.isca-speech.org/archive_open/ssw3/ ssw3_147.html 
44. Yoshimura T, Tokuda K, Masuko T, Kobayashi T, Kitamura T (1999) Simultaneous modeling of spectrum, pitch and duration in HMM-based speech synthesis. In: Sixth European conference on speech communication and technology 
45. Tokuda K, Yoshimura T, Masuko T, Kobayashi T, Kitamura T (2000) Speech parameter generation algorithms for HMM-based speech synthesis. In: 2000 IEEE international conference on acoustics, speech, and signal processing. proceedings (Cat. No. 00CH37100), vol 3. IEEE, pp 1315–1318 
46. Tokuda K, Nankaku Y, Toda T, Zen H, Yamagishi J, Oura K (2013) Speech synthesis based on hidden Markov models. Proc IEEE 101(5):1234–1252 
47. Qian Y, Fan Y, Hu W, Soong FK (2014) On the training aspects of deep neural network (DNN) for parametric TTS synthesis. In: 2014 IEEE International conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 3829–3833
48. Fan Y, Qian Y, Xie FL, Soong FK (2014) TTS synthesis with bidirectional LSTM based recurrent neural networks. In: Fifteenth annual conference of the international speech communication association 
49. Zen H, Sak H (2015) Unidirectional long short-term memory recurrent neural network with recurrent output layer for low-latency speech synthesis. In: 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 4470–4474 
50. Wang W, Xu S, Xu B (2016) First step towards end-to-end parametric TTS synthesis: Generating spectral parameters with neural attention. In: Interspeech, pp 2243–2247 
51. Li H, Kang Y, Wang Z (2018) EMPHASIS: An emotional phoneme-based acoustic model for speech synthesis system. In: Proc Interspeech 2018, pp 3077–3081 
52. Yoshimura T (2002) Simultaneous modeling of phonetic and prosodic parameters, and characteristic conversion for HMM-based text-to-speech systems. PhD diss, Nagoya Institute of Technology 
53. Fukada T, Tokuda K, Kobayashi T, Imai S (1992) An adaptive algorithm for mel-cepstral analysis of speech. In: Proc. ICASSP, vol 1, pp 137–140 
54. Tokuda K, Kobayashi T, Masuko T, Imai S (1994) Mel-generalized cepstral analysis-a unified approach to speech spectral estimation. In: Third international conference on spoken language processing 
55. Kawahara H, Masuda-Katsuse I, De Cheveigne A (1999) Restructuring speech representations using a pitch-adaptive time–frequency smoothing and an instantaneous-frequency-based F0 extraction: Possible role of a repetitive structure in sounds. Speech Commun 27(3–4):187–207 
56. Imai S, Sumita K, Furuichi C (1983) Mel log spectrum approximation (MLSA) filter for speech synthesis. Electron Commun Japan (Part I: Commun) 66(2):10–18 
57. Imai S (1983) Cepstral analysis synthesis on the mel frequency scale. In: ICASSP’83. IEEE International conference on acoustics, speech, and signal processing, vol 8. IEEE, pp 93–96 
58. Kawahara H (2006) STRAIGHT, exploitation of the other aspect of vocoder: perceptually isomorphic decomposition of speech sounds. Acoust Sci Technol 27(6):349–353 
59. Morise M, Yokomori F, Ozawa K (2016) WORLD: a vocoder-based high-quality speech synthesis system for real-time applications. IEICE Trans Inf Syst 99(7):1877–1884 
60. Sproat R, Black AW, Chen S, Kumar S, Ostendorf M, Richards C (2001) Normalization of non-standard words. Comput Speech Lang 15(3):287–333 
61. Bisani M, Ney H (2008) Joint-sequence models for grapheme-to-phoneme conversion. Speech Commun 50(5):434–451 
62. Zen H (2015) Acoustic modeling in statistical parametric speech synthesis-from HMM to LSTM-RNN. In: Proc MLSLP. Invited paper 
63. Arık SÖ, Chrzanowski M, Coates A, Diamos G, Gibiansky A, Kang Y, Li X, Miller J, Ng A, Raiman J, et al (2017) Deep Voice: Real-time neural text-to-speech. In: International conference on machine learning, PMLR, pp 195–204 
64. Gibiansky A, Arik SÖ, Diamos GF, Miller J, Peng K, Ping W, Raiman J, Zhou Y (2017) Deep Voice 2: Multi-speaker neural text-to-speech. In: NIPS 
65. Sotelo J, Mehri S, Kumar K, Santos JF, Kastner K, Courville AC, Bengio Y (2017) Char2wav: End-to-end speech synthesis. In: 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24–26, 2017, Workshop Track Proceedings. OpenReview.net. https://openreview.net/forum?id=B1VWyySKx 
66. Ren Y, Hu C, Tan X, Qin T, Zhao S, Zhao Z, Liu TY (2021) FastSpeech 2: fast and high-quality end-to-end text to speech. In: International conference on learning representations. https://openreview.net/forum?id=piLPYqxtWuA 
67. Ping W, Peng K, Chen J (2018) ClariNet: parallel wave generation in end-to-end text-to-speech. In: International conference on learning representations 
68. Donahue J, Dieleman S, Bi´nkowski M, Elsen E, Simonyan K (2021) End-to-end adversarial text-to-speech. In: ICLR 
69. Mehri S, Kumar K, Gulrajani I, Kumar R, Jain S, Sotelo J, Courville A, Bengio Y (2017) SampleRNN: An unconditional end-to-end neural audio generation model. In: ICLR
70. van den Oord A, Li Y, Babuschkin I, Simonyan K, Vinyals O, Kavukcuoglu K, Driessche G, Lockhart E, Cobo L, Stimberg F, et al (2018) Parallel WaveNet: Fast high-fidelity speech synthesis. In: International conference on machine learning. PMLR, pp 3918–3926 
71. Prenger R, Valle R, Catanzaro B (2019) WaveGlow: a flow-based generative network for speech synthesis. In: ICASSP 2019-2019 IEEE International conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 3617–3621 
72. Kim S, Lee SG, Song J, Kim J, Yoon S (2019) FloWaveNet: a generative flow for raw audio. In: International conference on machine learning. PMLR, pp 3370–3378 
73. Kumar K, Kumar R, de Boissiere T, Gestin L, Teoh WZ, Sotelo J, de Brébisson A, Bengio Y, Courville A (2019) MelGAN: Generative adversarial networks for conditional waveform synthesis. In: NeurIPS 
74. Kim J, Kong J, Son J (2021) Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech. Preprint. arXiv:2106.06103 
75. Kong J, Kim J, Bae J (2020) HiFi-GAN: Generative adversarial networks for efficient and high fidelity speech synthesis. Adv Neural Inf Process Syst 33:17022 
76. Ping W, Peng K, Zhao K, Song Z (2020) WaveFlow: a compact flow-based model for raw audio. In: International conference on machine learning. PMLR, pp 7706–7716 
77. Kim J, Kim S, Kong J, Yoon S (2020) Glow-TTS: A generative flow for text-to-speech via monotonic alignment search. Adv Neural Inf Process Syst 33:8067 
78. Miao C, Liang S, Chen M, Ma J, Wang S, Xiao J (2020) Flow-TTS: A non-autoregressive network for text to speech based on flow. In: ICASSP 2020-2020 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 7209–7213 
79. Valle R, Shih K, Prenger R, Catanzaro B (2020) Flowtron: an autoregressive flow-based generative network for text-to-speech synthesis. Preprint. arXiv:2005.05957 
80. Weiss RJ, Skerry-Ryan R, Battenberg E, Mariooryad S, Kingma DP (2021) Wave-Tacotron: Spectrogram-free end-to-end text-to-speech synthesis. In: 2021 IEEE international confer- ence on acoustics, speech and signal processing (ICASSP). IEEE 
81. Donahue C, McAuley J, Puckette M (2018) Adversarial audio synthesis. In: International conference on learning representations 
82. Bi´nkowski M, Donahue J, Dieleman S, Clark A, Elsen E, Casagrande N, Cobo LC, Simonyan K (2019) High fidelity speech synthesis with adversarial networks. In: International conference on learning representations 
83. Yamamoto R, Song E, Kim JM (2020) Parallel WaveGAN: a fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram. In: ICASSP 2020-2020 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 6199–6203 
84. Yang J, Lee J, Kim Y, Cho HY, Kim I (2020) VocGAN: A high-fidelity real-time vocoder with a hierarchically-nested adversarial network. In: Proc Interspeech 2020, pp 200–204 
85. Lee SH, Yoon HW, Noh HR, Kim JH, Lee SW (2020) Multi-SpectroGAN: High-diversity and high-fidelity spectrogram generation with adversarial style combination for speech synthesis. Preprint. arXiv:2012.07267 
86. Peng K, Ping W, Song Z, Zhao K (2020) Non-autoregressive neural text-to-speech. In: International conference on machine learning. PMLR, pp 7586–7598 
87. Zhang YJ, Pan S, He L, Ling ZH (2019) Learning latent representations for style control and transfer in end-to-end speech synthesis. In: ICASSP 2019-2019 IEEE International conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 6945–6949 
88. Hsu WN, Zhang Y, Weiss RJ, Zen H, Wu Y, Wang Y, Cao Y, Jia Y, Chen Z, Shen J, et al (2018) Hierarchical generative modeling for controllable speech synthesis. In: International conference on learning representations 
89. Chen N, Zhang Y, Zen H, Weiss RJ, Norouzi M, Chan W (2021) WaveGrad: Estimating gradients for waveform generation. In: ICLR 
90. Kong Z, Ping W, Huang J, Zhao K, Catanzaro B (2021) DiffWave: A versatile diffusion model for audio synthesis. In: ICLR
91. Jeong M, Kim H, Cheon SJ, Choi BJ, Kim NS (2021) Diff-TTS: A denoising diffusion model for text-to-speech. Preprint. arXiv:2104.01409 
92. Popov V, Vovk I, Gogoryan V, Sadekova T, Kudinov M (2021) Grad-TTS: a diffusion probabilistic model for text-to-speech. Preprint. arXiv:2105.06337 
93. Lee Sg, Kim H, Shin C, Tan X, Liu C, Meng Q, Qin T, Chen W, Yoon S, Liu TY (2021) PriorGrad: Improving conditional denoising diffusion models with data-driven adaptive prior. Preprint. arXiv:2106.06406 
94. Tachibana H, Uenoyama K, Aihara S (2018) Efficiently trainable text-to-speech system based on deep convolutional networks with guided attention. In: 2018 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 4784–4788 
95. Valin JM, Skoglund J (2019) LPCNet: Improving neural speech synthesis through linear prediction. In: ICASSP 2019-2019 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 5891–5895 
96. Yu C, Lu H, Hu N, Yu M, Weng C, Xu K, Liu P, Tuo D, Kang S, Lei G, et al (2020) DurIAN: Duration informed attention network for speech synthesis. In: Proc Interspeech 2020, pp 2027–2031 
97. Zeng Z, Wang J, Cheng N, Xia T, Xiao J (2020) AlignTTS: Efficient feed-forward text-to-speech system without explicit alignment. In: ICASSP 2020-2020 IEEE International conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 6714–6718 
98. Ła´ncucki A (2020) FastPitch: Parallel text-to-speech with pitch prediction. Preprint. arXiv:2006.06873 
99. Lim D, Jang W, Gyeonghwan O, Park H, Kim B, Yoon J (2020) JDI-T: Jointly trained duration informed Transformer for text-to-speech without explicit alignment. In: Proc Interspeech 2020, pp 4004–4008 
100. Luo R, Tan X, Wang R, Qin T, Li J, Zhao S, Chen E, Liu TY (2021) LightSpeech: Lightweight and fast text to speech with neural architecture search. In: 2021 IEEE International conference on acoustics, speech and signal processing (ICASSP). IEEE