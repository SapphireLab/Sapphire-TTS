## 13.Summary & Outlook

## 13.1.Summary

> In this book, we introduced neural text-to-speech synthesis, with a focus on (1) the basic model components of TTS including text analyses, acoustic models, vocoders, and fully end-to-end models; (2) several advanced topics in TTS including expressive and controllable TTS, robust TTS, model-efficient TTS, data-efficient TTS, and some tasks beyond TTS.
> As a quick summary, we list representative TTS models in Table B.1.
> Due to page limitations, we only introduced core algorithms of TTS; readers can refer to other papers for TTS-related problems and applications, such as singing voice synthesis [1–3], voice conversion [4], speech enhancement/separation [5], and talking face synthesis [6], etc.

## 13.2.Future Directions

> We point out some future research directions on neural TTS according to the end goals of TTS.

### 13.2.1.High-Quality Speech Synthesis

> The most important goal of TTS is to synthesize high-quality speech.
> The quality of speech is determined by many aspects that influence the perception of speech, including intelligibility, naturalness, expressiveness, prosody, emotion, style, robustness, controllability, etc.
> While neural approaches have significantly improved the quality of synthesized speech, there is still large room to make further improvements.

#### Powerful Generative Models

> TTS is a generation task, including the generation of waveform and/or acoustic features, which can be better handled by powerful generative models.
> Although advanced generative models based on VAE, GAN, Flow, or Diffusion have been adopted in acoustic models, vocoders, and fully end-to-to models, research efforts on more powerful and efficient generative models are appealing to further improve the quality of synthesized speech.

#### Better Representation Learning

> Good representations of text and speech are beneficial for neural TTS models, which can improve the quality of synthesized speech.
> Some initial explorations on text pre-training indicate that better text representations can indeed improve speech prosody.
> How to learn powerful representations for text/phoneme sequence and especially for speech sequence through unsupervised/self-supervised learning and pre-training is challenging and worth further exploration.

#### Robust Speech Synthesis

> While current TTS models eliminate word skipping and repeating issues caused by incorrect attention alignments, they still suffer from robustness issues when encountering corner cases that are not covered in the training set, such as longer text length, different text domains, etc.
> Improving the generalizability of the TTS model to different domains is critical for robust synthesis.

#### Expressive/Controllable/Transferrable Speech Synthesis

> The expressiveness, controllability, and transferability of TTS models rely on better variation information modeling.
> Existing methods leverage reference encoder or explicit prosody features (e.g., pitch, duration, energy) for variation modeling, which enjoys good controllability and transferability in inference but suffers from training/inference mismatch since ground-truth reference speech or prosody features used in training are usually unavailable in inference.
> Advanced TTS models capture the variation information implicitly, which enjoy good expressiveness in synthesized speech but perform not well in control and transfer, since sampling from latent space cannot explicitly and precisely control and transfer each prosody feature (e.g., pitch, style).
> How to design better methods for expressive/controllable/transferrable speech synthesis is also appealing.

#### More Human-Like Speech Synthesis

> Current speech recordings used in TTS training are usually in formal reading styles, where no pauses, repeats, changing speeds, varying emotions, or errors are permitted.
> However, in casual or conversational talking, human seldom speaks like standard reading.
> Therefore, better modeling the casual, emotional, and spontaneous styles are critical to improving the naturalness of synthesized speech.

### 13.2.2.Efficient Speech Synthesis

> Once we can synthesize high-quality speech, the next most important task is efficient synthesis, i.e., how to reduce the cost of speech synthesis including the cost of collecting and labeling training data, training, and serving TTS models, etc.

#### Data-efficient TTS

> Many low-resource languages lack training data.
> How to leverage unsupervised/semi-supervised learning and cross-lingual transfer learning to help the low-resource languages is an interesting direction.
> For example, the ZeroSpeech Challenge [7] is a good initiative to explore the techniques to learn only from speech, without any text or linguistic knowledge.
> Besides, in voice adaptation, a target speaker usually has little adaptation data, which is another application scenario for data-efficient TTS.

#### Parameter-efficient TTS

> Today’s neural TTS systems usually employ large neural networks with tens of millions of parameters to synthesize high-quality speech, which blocks the applications in mobile, IoT, and other low-end devices due to their limited memory and power consumption.
> Designing compact and lightweight models with fewer memory footprints, power consumption, and latency are critical for those application scenarios.

#### Energy-efficient TTS

> Training and serving a high-quality TTS model consume a lot of energy and emit a lot of carbon.
> Improving energy efficiency, e.g., reducing the FLOPs in TTS training and inference, is important to let more populations benefit from advanced TTS techniques while reducing carbon emissions to protect our environment.

## References

1. Hono Y, Hashimoto K, Oura K, Nankaku Y, Tokuda K (2019) Singing voice synthesis based on generative adversarial networks. In: ICASSP 2019-2019 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 6955–6959 
2. Lu P, Wu J, Luan J, Tan X, Zhou L (2020) XiaoiceSing: a high-quality and integrated singing voice synthesis system. In: Proceedings of the Interspeech 2020, pp 1306–1310 
3. Chen J, Tan X, Luan J, Qin T, Liu TY (2020) HiFiSinger: towards high-fidelity neural singing voice synthesis. Preprint. arXiv:2009.01776 
4. Sisman B, Yamagishi J, King S, Li H (2020) An overview of voice conversion and its challenges: from statistical modeling to deep learning. IEEE/ACM Trans Audio Speech Lang Process 29:132–157 
5. Wang D, Chen J (2018) Supervised speech separation based on deep learning: an overview. IEEE/ACM Trans Audio Speech Lang Process 26(10):1702–1726 
6. Chen L, Cui G, Kou Z, Zheng H, Xu C (2020) What comprises a good talking-head video generation? A survey and benchmark. Preprint. arXiv:2005.03201 
7. ZeroSpeech, Zero resource speech challenge. (2021) https://www.zerospeech.com/