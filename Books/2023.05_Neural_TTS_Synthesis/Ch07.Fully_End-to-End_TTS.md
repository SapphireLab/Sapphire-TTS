# 7. Fully End-to-End TTS

## Abstract

> Fully end-to-end TTS models can generate speech waveform from character or phoneme sequence directly, which have the following advantages: 
> (1) They require less human annotation and feature development (e.g., alignment information between text and speech); 
> (2) The joint and end-to-end optimization can avoid error propagation in cascaded models (e.g., Text Analysis + Acoustic Model + Vocoder); 
> (3) They can also reduce the training, development, and deployment cost.

> However, there are big challenges to training TTS models in an end-to-end way, mainly due to the different modalities between text and speech waveform, as well as the huge length mismatch between character/phoneme sequence and waveform sequence.
> For example, for a speech with a length of 5 s and about 20 words, the length of the phoneme sequence is just about 100, while the length of the waveform sequence is 80k (if the sample rate is 16 kHz).
> It is hard to put the waveform points of the whole utterance into model training, due to the limit of memory.
> It is hard to capture the context representations if only using a short audio clip for the end-to-end training.
> Thus, the development of end-to-end TTS is progressive.
> In this chapter, we first review the progressively end-to-end process in TTS from a historical perspective and then introduce some fully end-to-end TTS models..

## 7.1.Prerequisite Knowledge for Reading This Chapter
> - Language and Speech Processing
> - Deep generative models, such as Autoregressive Models, Normalizing Flows, Variational Auto-Encoders, Denoising Diffusion Probabilistic Models, and Generative Adversarial Networks.
> - The key components in neural TTS, including acoustic model and vocoders

## 7.2.End-to-End TTS from a Historic Perspective

> Due to the difficulty of fully end-to-end training, the development of neural TTS follows a progressive process towards fully end-to-end models.
> Figure 7.1 illustrates this progressive process starting from early statistical parametric syn thesis [1–5].
> The process towards fully end-to-end models typically contains these upgrades: (1) Simplifying text analysis module and linguistic features.
> In SPSS, the text analysis module contains different functionalities such as text normalization, phrase/word/syllable segmentation, POS tagging, prosody prediction, and grapheme-to-phoneme conversion (including polyphone disambiguation).
> In end to-end models, only the text normalization and grapheme-to-phoneme conversion are retained to convert characters into phonemes, or the grapheme-to-phoneme conversion module is removed by directly taking characters as input. (2) Simplifying acoustic features, where the complicated acoustic features such as MGC, BAP, and F0 used in SPSS are simplified into mel-spectrograms. (3) Replacing two or three modules with a single end-to-end model.
> For example, the acoustic models and vocoders can be replaced with a single vocoder model such as WaveNet.
> Accordingly, we illustrate the progressive process in Fig.7.1 and describe it as follows.

### 7.2.1.Stage 0: Character→Linguistic→Acoustic→Waveform

> Statistical parametric synthesis [1–5] uses three basic modules, where text analyses convert characters into linguistic features, and acoustic models generate acoustic features from linguistic features (where the target acoustic features are obtained through vocoder analysis), and then vocoders synthesize speech waveform from acoustic features through parametric calculation..

### 7.2.2.Stage 1: Character/Phoneme→Acoustic→Waveform

> ARST [6] in statistical parametric synthesis combines the text analysis and acoustic model into an end-to-end acoustic model that directly generates acoustic features from phoneme sequence and then uses a vocoder in SPSS to generate the waveform..

### 7.2.3.Stage 2: Character→Linguistic→Waveform

> Some works [24–26] in SPSS propose to directly generate speech waveform from linguistic features.
> Later, WaveNet [7] learns the mapping between linguistic features and speech waveform with a deep neural network, which can be regarded as a combination of an acoustic model and a vocoder.
> This kind of model [7, 10–12] still requires a text analysis module to generate linguistic features..

### 7.2.4.Stage 3: Character/Phoneme→Spectrogram→Waveform

> [Tacotron (2017)](../../Models/TTS2_Acoustic/2017.03.29_Tacotron.md) is further proposed to simplify linguistic and acoustic features, which directly predicts linear-spectrograms from characters/phonemes with an encoder-attention-decoder model, and converts linear-spectrograms into waveform with Griffin-Lim [28].
> The following works such as DeepVoice 3 [13], [Tacotron 2 (2017)](../../Models/TTS2_Acoustic/2017.12.16_Tacotron2.md), [TransformerTTS (2018)](../../Models/TTS2_Acoustic/2018.09.19_TransformerTTS.md), and [FastSpeech (2019)](../../Models/TTS2_Acoustic/2019.05.22_FastSpeech.md), [FastSpeech2 (2020)](../../Models/TTS2_Acoustic/2020.06.08_FastSpeech2.md) predict mel-spectrograms from characters/phonemes and further use a neural vocoder such as [WaveNet (2016)](../../Models/TTS3_Vocoder/2016.09.12_WaveNet.md), [WaveRNN (2018)](../../Models/TTS3_Vocoder/2018.02.23_WaveRNN.md), [WaveGlow (2018)](../../Models/TTS3_Vocoder/2018.10.31_WaveGlow.md), [FloWaveNet (2018)](../../Models/TTS3_Vocoder/2018.11.06_FloWaveNet.md), and [Parallel WaveGAN](../../Models/TTS3_Vocoder/2019.10.25_Parallel_WaveGAN.md) to generate the waveform..

### 7.2.5.Stage 4: Character/Phoneme→Waveform

> In recent years, some fully end-to-end TTS models are developed for direct text-to waveform synthesis, as listed in Tab.7.1.
> We will introduce these models in the next section..

> Tab.7.1.A List of Fully End-to-End TTS Models
> |Model|One-stage training |AR/NAR |Modeling|Architecture|
> | --- | --- | --- | --- | --- |
> |Char2Wav [19]| N|AR|Seq2Seq|RNN|
> |ClariNet [20]| N|AR|Flow|CNN|
> |FastSpeech 2s [16] |Y|NAR|GAN|Self-Att/CNN 
> |EATS [21]|Y|NAR|GAN|CNN 
> |Wave-Tacotron [31] |Y|AR|Flow|CNN/RNN/Hybrid 
> |EfficientTTS-Wav [32] |Y|NAR|GAN|CNN 
> |WaveGrad 2 [33]|Y|NAR|Diffusion|CNN/RNN/Hybrid 
> |VITS [22]|Y|NAR|VAE+Flow+GAN |CNN/Self-Att/Hybrid 
> |NaturalSpeech [23] |Y|NAR|VAE+Flow+GAN |CNN/Self-Att/Hybrid

## 7.3.Fully End-to-End Models

> In this section, we introduce some representative fully end-to-end TTS models from several aspects: 
> 1. two-stage training which first trains acoustic models and/or vocoder separately and then jointly optimizes them. 
> 2. Using generative models for end-to-end training such as GAN, Flow, or Diffusion.
> 3. Hybrid system that combines several different generative models including VAE, Flow, and GAN. 
> 4. A fully end-to-end TTS system that achieves human-level quality..

### 7.3.1.Two-Stage Training

> In the early investigation, researchers usually cascade acoustic models and vocoders together and optimize them jointly to build fully end-to-end models, such as Char2Wav and Clarinet.
> Char2Wav [19] leverages an RNN-based encoder-attention decoder model to generate acoustic features from characters and then uses Sam pleRNN [34] to generate the waveform.
> The two models are jointly tuned for direct speech synthesis.
> Similarly, ClariNet [20] jointly tunes an autoregressive acoustic model and a non-autoregressive vocoder for direct waveform generation..

### 7.3.2.One-Stage Training

> FastSpeech 2s [16] directly generates speech from text with a fully parallel structure, which can greatly speed up inference.
> To alleviate the difficulty of joint text-to waveform training, it leverages an auxiliary mel-spectrogram decoder to help learn the contextual representations of phoneme sequences.
> A concurrent work called EATS [21] also directly generates waveform from characters/phonemes, which leverages duration interpolation and soft dynamic time wrapping loss for end-to-end alignment learning.
> Wave-Tacotron [31] builds a flow-based decoder on Tacotron to directly generate a waveform, which uses parallel waveform generation in the flow part but still an autoregressive generation in the Tacotron part.
> WaveGrad 2 [33] leverages a diffusion model to iteratively denoise the waveform conditioned on a phoneme encoder.
> [VITS (2021)](../../Models/E2E/2021.06.11_VITS.md) leverages a VAE to reconstruct the waveform sequence and uses a GAN-based loss to help optimize the waveform generation.
> It also uses leverage a Glow-TTS [35] based module to predict the prior distribution from the phoneme sequence to fulfill phoneme to waveform generation..

### 7.3.3.Human-Level Quality
> NaturalSpeech [23] is the first TTS model that achieves comparable voice quality with human recordings on a benchmarking dataset (i.e., LJSpeech [36]).
> As shown in Fig.7.2, NaturalSpeech leverages a variational auto-encoder (VAE) [37] to com press the high-dimensional speech ($x$) into continuous frame-level representations (denoted as posterior.$q(z|x)$), which are used to reconstruct the waveform (denoted as $p(x|z)$).
> The corresponding prior (denoted as $p(z|y)$) is obtained from the text sequence $y$.
> Considering the posterior from the speech is more complicated than the prior from text, NaturalSpeech designs several modules (see Fig.7.3) to match the posterior and prior as close to each other as possible, to enable text-to-speech synthesis through $p(z|y) \to p(x|z)$: 
> 1. a large-scale pre-training on the phoneme encoder to extract better representations from phoneme sequence; 
> 2. a fully differentiable durator [23] that consists of a duration predictor and an upsampling layer to improve the duration modeling; 
> 3. a bidirectional prior/posterior module based on flow models [38–40] to further enhance the prior $p(z|y)$ and reduce the complexity of posterior $q(z|x)$;
> 4. a memory-based VAE to reduce the complexity of the posterior needed to reconstruct the waveform.
> 
> NaturalSpeech achieves a CMOS of $−0.01$ compared to recordings, with a Wilcoxon signed rank test [41] at p-level $p>>0.05$, which shows that NaturalSpeech generates speech with no statistically significant difference from human recordings.