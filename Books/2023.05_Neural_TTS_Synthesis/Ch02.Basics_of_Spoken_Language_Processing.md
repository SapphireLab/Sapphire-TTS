# 2.Basics of Spoken Language Processing

## Prerequisite Knowledge

> - Basic knowledge of algebra, calculus, and probability theory. 
> - Basic knowledge of language and speech processing.

## Abstract

> In this chapter, we introduce some basics of spoken language processing (both speech and natural language), which are fundamental to text-to-speech synthesis.
> Since speech and language are studied in the discipline of linguistics, we first overview some basic knowledge in linguistics (Sect. 2.1) and discuss a key concept called speech chain that is closely related to speech synthesis (Sect. 2.2).
> Then, we introduce speech signal processing, which covers the topics of digital signal processing, speech processing in the time and frequency domain, cepstral analysis, linear prediction analysis, and speech parameter estimation (Sect. 2.3).
> At last, we overview some typical speech processing tasks. 


## 2.1.Overview of Linguistics

> We first introduce some basic concepts of language and speech and their relationship to linguistics.
> - Language
> Language is a structured system of communication used by humans and consists of different types such as spoken language, written language, and sign language [1].
> That is to say, language is the ability to produce and comprehend spoken, written, or sign words.

> - Speech
> Speech is the vocalized form of human communication and is a way to express spoken language [2].
> Here we further illustrate some terminologies related to speech, including sound, voice, and audio: (1) Sound refers to anything that can be heard by humans, and may come from humans (e.g., talking, laughing, clapping hands together), animals (e.g., birdcall, bark), or physical objects (e.g., object collision, slamming a door). (2) Voice is the sound produced by humans or other vertebrates using the lungs and the vocal cords in the larynx or voice box. (3) Speech is the sound produced by humans for communication purposes (some voices from humans are not speech, e.g., the sound of snoring).
> It can be seen that speech belongs to voice and voice belongs to sound. (4) Audio is used to represent sound electrically and is made of electrical energy (analog or digital signals).
> As a comparison, the sound is made of mechanical wave energy (longitudinal sound waves).
> A speech signal can be represented as a mechanical waveform, and can be converted into an electrical waveform (audio) with a microphone and then processed by digital or analog signal processing technologies and converted back into a mechanical waveform with a loudspeaker. 

> - Linguistics.
> Linguistics is the scientific study of language [3] and covers several branches: phonetics, phonology, morphology, syntax, semantics, and pragmatics.
> These branches roughly correspond to different ingredients in linguistic systems, including speech sounds (and gestures, in the case of sign languages), basic units (phonemes, words, morphemes), phrases and sentences, meaning, and language usage, as shown in Fig. 2.1.

> We introduce these branches in linguistics in the following subsections. 

### 2.1.1.Phonetics and Phonology

> Phonetics and phonology both deal with speech sounds [4].
> Phonetics is the science that studies speech sounds and their production, transmission, and perception, and provides methods for their analysis, classification, and transcription [4].
> It consists of three important subdisciplines that correspond to speech production, transmission, and perception respectively [5]: 
> (1) Articulatory phonetics, which studies how the sounds are made with articulators. 
> (2) Acoustic phonetics, which studies the acoustic results of different articulations. 
> (3) Auditory phonetics, which studies how the speech sounds are perceived and understood by listeners.

> Phonology [6] is the science that studies the systematic organization of sounds in languages and studies the systems of phonemes (the smallest set of units that represent distinctive sounds of a language and bring a difference in word meaning) in particular languages.
> It cares about patterns of sounds in different positions of words or in different languages.

> There are key differences between phonetics and phonology [7]: 
> (1) Phonetics is related to phones, while phonology is related to phonemes. 
> (2) Phonetics is about the physical aspect of sounds (human ear), while phonology is the abstract aspect of sounds (human brain). 
> (3) Phonetics studies speech sounds in general, regardless of different languages, while phonology studies speech sounds in particular, in one language or different languages. 

### 2.1.2.Morphology and Syntax

> Morphology [8] studies the structure of words, while syntax studies the structure of sentences.
> In morphology, we understand how words are formed from morphemes, which are the smallest meaningful units of language.
> Morphemes contain the root, stem, prefix, and suffix of words, such as “cat”, “sat”, “un-”, “-ed”.
> Morphology determines the meaning of words through these morphemes.
> In syntax [8], we understand how sentences are developed from words, which are the smallest units in the study of syntax.
> The syntax looks at the processes and rules of constructing a sentence, the relations between words, and the grammatical structure of sentences, which can determine the meaning of a sentence.
> For example, “the cat sat” is a simple sentence made of a subject and a verb. 

### 2.1.3.Semantics and Pragmatics

> Semantics studies the meaning of morphemes, words, phrases, sentences, and their relation, while pragmatics studies the use of language and how people produce and comprehend the meaning in different contexts [9].
> Therefore, there are some key differences between semantics and pragmatics: (1) Semantics studies the meaning without considering the context of language usage, while pragmatics studies the meaning by considering the context of language usage. (2) Semantics cares about conceptual meaning based on vocabulary and grammar, and pragmatics cares about the intended meaning based on the context and the inference from readers or listeners when interpreting the sentence. (3) Pragmatics is a broader field compared to semantics.
> Table 2.1 lists the characteristics of phonetics, phonology, morphology, syntax, semantics, and pragmatics in linguistics, and their relevance to TTS. 

## 2.2.Speech Chain

> An important concept related to speech signal processing is the speech chain [10], which is a model of speech communication between speakers and listeners.
> The speech chain analyzes the process of converting an intention from a speaker to an understanding of this intention by the listener, as shown in Fig. 2.2.
> It consists of five levels: (1) Linguistic level, where an intention/message is expressed by selecting and ordering suitable sentences/words/phonemes (related to grammar and phonological coding), which are used to generate the basic sounds of the communication. (2) Physiological level, where the sounds of the linguistics units of the message are generated by the vocal tract components guided by neural and muscular activities (related to articulatory phonetics). (3) Acoustic level, where the sound waves are generated from the lips and nostrils and transmitted to both the speaker through sound feedback and the listener through airborne (related to acoustic phonetics). (4) Physiological level, where the sounds come at the ear of the listener and activate the hearing system, and are perceived by the auditory nerves (related to auditory phonetics). (5) Linguistic level, where neural activities perceived in the previous stage are recognized as phonemes/words/sentences to understand the intention/message transmitted by the speaker (related to cognitive understanding).
> In the next subsections, we introduce the three processes in the speech chain that are closely related to text-to-speech synthesis: speech production, speech transmission, and speech perception, which correspond to the three important subdisciplines of phonetics respectively: articulatory phonetics, acoustic phonetics, and auditory phonetics [5]. 

### 2.2.1.Speech Production and Articulatory Phonetics

> Articulatory phonetics studies how sounds are made with human articulators.
> If a human wants to generate speech, the human brain first generates the concept of text and controls the organs of the speech production system to generate the speech waveform corresponding to the text concept.
> The human speech production system consists of three parts: lungs, voice box, and vocal tract, as shown in Fig. 2.3.
> The lungs pump air up towards the voice box and vocal tract, as shown in Fig. 2.3a.
> If the vocal cords in the voice box are tensed, then the airflow causes them to vibrate, which produces voiced sounds or quasi-periodic sounds.
> If the vocal cords are relaxed, then the airflow just passes through the voice box and enters the vocal tract to produce unvoiced sounds.
> The nasal cavity and oral cavity (as shown in Fig. 2.3b) in the vocal tract resonate voice to generate sounds such as vowels and consonants. 

> **Voiced vs Unvoiced and Vowels vs Consonants** 
> We describe two different classifications of sounds: voiced vs unvoiced, and vowels vs consonants.
> Voiced and unvoiced sounds depend on whether the vocal cords vibrate or not when producing the sounds: voiced sounds correspond to vibration (e.g., /a/, /m/) while unvoiced sounds correspond to no vibration (e.g., /s/, /t/).
> The difference between vowels and consonants depends on whether the airflow is constricted by the nasal cavity or oral cavity: vowels are produced by keeping the nasal or oral cavity open (e.g., /a/, /i/), while consonants are produced by constricting the oral or nasal cavity (e.g., /t/, /m/).
> Consonants can be voiced (e.g., /m/, /b/) or unvoiced (e.g., /p/, /t/).
> Most of the vowels are voiced, while there exist unvoiced vowels in some languages or in whispers.
> Table 2.2 shows some examples of vowels and consonants, as well as their voiced and unvoiced property.
> Figure 2.4 illustrates the tongue positions of some vowels, where the vertical axis represents the vowel closeness, with close/high vowels at the top of the chart (e.g., i, u) while open/low vowels at the bottom of the chart (e.g., a), and the horizontal axis represents the vowel backness, with front vowels at the left of the chart (e.g., i, a) while back vowels at the right of the chart (e.g., u, 6).

> **Source-Filter Model** 
> The source–filter model describes the process of speech production as a two-stage process, as shown in Fig. 2.5, where a sound source is first generated (e.g., by the vocal cords) and then shaped/filtered by an acoustic filter (e.g., the vocal tract).
> The sound sources can be either periodic (pulse generator) or aperiodic (noise generator), or a mixture of the two.
> Basically, a filter is used to selectively let something pass through or block something.
> The vocal tract filter can allow some frequencies to pass through by reducing the intensity of other frequencies.
> The source-filter model can be used to analyze human speech production and artificial speech synthesis systems.
> In human speech production, the vocal cords produce sound sources as either periodic sounds when vibrating or aperiodic sounds when relaxing, and the vocal tract (i.e., the pharynx, mouth, and nasal cavity) manipulate the sound sources to produce the speech sounds.
> From the frequency perspective, the vocal cords produce a series of harmonics (including the fundamental frequency) of different amplitudes, and the vocal tract either amplifies or attenuates certain frequencies to generate desired sounds.
> In artificial speech synthesis, the systems based on the source-filter model usually adopt a periodic impulse train as the source for voiced sound and white noise as the source for unvoiced sound.
> The filter of the systems is usually implemented with an all-pole filter, and the coefficients of this all-pole filter are learned by linear prediction via minimizing the errors between the synthesized speech and ground-truth speech [12].
> The filter can be also implemented with neural networks [13] in the era of deep learning. 

### 2.2.2.Speech Transmission and Acoustic Phonetics

> After the speech is generated by a human articulator in the speech production process, we can get the speech sound.
> In the subsection, we describe the speech transmission part and introduce acoustic phonetics, which studies the physical properties of speech produced by articulators, such as amplitudes, frequencies, and durations.
> We introduce several physical properties as follows: 
> - Amplitude.
> It measures the degree of fluctuations in the air particles caused by sound waves.
> An example of speech sound waves is shown in Fig. 2.6a where the horizon axis represents the time and the vertical axis represents the amplitude.
> Amplitude is related to several terms: sound energy, sound power, sound intensity, and sound pressure.
> We explain these terminologies as follows: (1) Sound energy measures the ability/capacity of sound to move the air particles, and is proportional to the square of the amplitude.
> The unit of sound energy is the joule (J). (2) Sound power measures the energy created by a sound source every second, and it maintains constant no matter how far the sound travels from the source.
> The unit of sound power is the watt (.W = J/S). (3) Sound intensity is the flow of energy (power) through a unit area in one second, measured in power/area.
> The unit of sound intensity is watts/square meter (.W/m2).
> As the sound travels from the source, the sound intensity level decreases in a proportion to the square of the distance from the source. (4) Sound pressure is the amount of force caused by the vibration of particles at a unit area.
> The unit of sound pressure is the Pascal (Pa), where .Pa = N/m2.
> The human ear usually responds to sound pressure in a large range and in a logarithmic scale.
> Thus, we define sound pressure level as.SPL = 10 log10P2P20= 20 log10PP0in decibel (dB) scale, where .
> P0is a reference value that usually set as the sound pressure at the threshold of hearing.2 ∗ 10−5Pa. 
> - Frequency.
> It measures the number of cycles on a repeating/periodic waveform per unit of time.
> The unit of measurement is Hertz (Hz).
> The higher frequency of a speech waveform is, the higher the pitch human can perceive. 
> - Fundamental frequency and harmonics.
> The fundamental frequency is defined as the lowest frequency of a periodic waveform, denoted as .
> F0, and harmonics are the higher frequency components that are integer multiples of the fundamental frequency.
> Fundamental frequency usually has a higher amplitude than harmonics and plays an important role in speech processing.
> For the sound of a musical note, the fundamental frequency is the pitch of this note. 
> - Sine waves and complex waves.
> Most speech sounds are complex waves, which consist of two or more simple sine waves.
> The lowest frequency of these sine waves is the fundamental frequency of this complex wave, which represents the number of times the vocal cords vibrate in a unit of time.
> The harmonics of this complex wave can be regarded as the amplified frequencies by the human vocal tract under a mechanism called natural resonances, where the frequency of the harmonic that is most augmented by this resonance is called a formant. 
> - Periodic and aperiodic waves.
> The voiced and unvoiced sounds described in Sect. 2.2.1 correspond to the periodic and aperiodic waves here.
> The periodic waves are generated under the vibration of vocal cords and contain fundamental frequency and harmonics.
> The aperiodic waves are generated without the vibration of vocal cords, which means they do not repeat in a regular pattern and do not contain a fundamental frequency. 
> - Spectrograms. 
> It is obtained by converting a time-domain waveform signal into the frequency domain and can be regarded as a 2D image, where the horizontal axis of the spectrograms measures time while the vertical axis measures the frequency, and the value of each position in this image is the magnitude of this frequency in a certain time. An example of a spectrogram is shown in Figs. 2.6b and 2.6c.

### 2.2.3.Speech Perception and Auditory Phonetics

> In this subsection, we introduce how speech sound is perceived by humans, i.e., auditory phonetics.
#### How Human Perceives Sound 

> Auditory phonetics studies how speech sounds are perceived and understood by listeners.
> When the speech sounds arrive at the ear of the listener, the auditory system perceives the speech sounds as the following steps: (1) Acoustic-to-neural converter, where the acoustic signals are converted into neural representation through the outer, middle, and inner ears.
> Specifically, the outer ear receives sound in the ear canal, and the tympanic membrane in the middle ear converts the acoustical sound waves into mechanical vibrations, which are further fired as neural (electrical) impulses in the inner ear. (2) Neural transduction, where neural signals fired by the inner ear are transmitted in the auditory nerve, which acts as the neural pathway to the brain. (3) Neural processing, where the neural firing signals are perceived and processed by the human brain.

#### Difference Between Auditory Perceptions and Physical Property of Sound 

> In speech perception, we care about several aspects of the perceived sound, such as the sound pressure level, frequency, and spectral structure (formants).
> However, there are differences between psychophysical observations from humans and the physical attributes of sounds.
> Table 2.3 lists the different but corresponding concepts between physical property and auditory perception: 
> - Loudness is the sound pressure perceived by humans and is measured by the pressure of a sound relative to the pressure at the threshold of hearing, i.e., sound pressure level.
> It is denoted as .SPL = 10 log10P2P20= 20 log10PP0dB, where .P0= 20 μPa. 
> - Pitch is the sensation of sound frequency by humans.
> Pitch is related to the fundamental frequency of sounds but not exactly the same.
> Although the pitch is monotonously increasing with frequency, the relationship is not linear, 
> since humans have different sensibilities on different frequencies.
> Usually, the relationship between perceived pitch and physical frequency is characterized by the mel-scale frequency: .
> Pitch (mels) = 2595 log10(1 + F/700) = 1127 ln(1 + F/700).
> An example of linear-scale and mel-scale spectrogram is shown in Figs. 2.6b and 2.6c. 
> - The content and timbre of a speech waveform are largely determined by the details of the spectrum, such as harmonic, envelope, and formant.
>
> The human ear has a range of sound pressure levels (loudness) and frequencies, from the threshold of hearing to the threshold of pain, as shown in Fig. 2.7.

#### Evaluation Metrics for Speech Perception 

> We list some metrics to evaluate how good the speech is perceived by humans, including both objective metrics and subjective metrics, as shown in Table 2.4.
> For objective metrics: (1) Mel-Cepstral Distortion (MCD) measures the difference between two speech sequences in terms of the mel-cepstra.
> The smaller the MCD between the synthesized speech and natural (ground-truth) speech, the closer the synthesized speech is to the natural speech. (2) Signal-to-Distortion Ratio (SDR) measures the logarithmic ratio between the power of the ground-truth speech and the error between the synthesized speech and ground-truth speech (i.e., distortion).
> A larger SDR means better speech synthesis quality. (3) Perceptual Evaluation of Speech Quality (PESQ) [14] and Short-Time Objective Intelligibility (STOI) [15] are two metrics to measure the quality and intelligibility of the synthesized speech.
> For subjective metrics: (1) Intelligibility Score (IS) measures how a speech sentence or word can be understandable by listeners. (2) Mean Opinion Score (MOS) measures the speech quality on a 5-point scale. (3) Comparison Mean Opinion Score (CMOS) measures the speech quality by comparing samples from two systems head by head. (4) Similarity Mean Opinion Score (SMOS) measures the speaker similarity between the synthesized speech and the reference speech. 


## 2.3.Speech Signal Processing

> Speech signal processing is to represent, transform, analyze, and understand speech signals.
> We mainly focus on speech signal processing in the digital/discrete form (i.e., digital speech signal processing), which can be easily processed by computer and deep learning technology.
> We first introduce some basic knowledge about digital signal processing and introduce speech processing in the time and frequency domain.
> Then, we introduce some advanced topics in speech processing, such as cepstral analysis and speech parameter estimation. 

### 2.3.1.Analog-to-Digital Conversion

> When a sound is captured by a microphone, it becomes a continuous-time and continuous-amplitude analog signal, which is further converted into a discrete-time and discrete-amplitude digital signal for digital signal processing.
> This analog-to-digital conversion process involves sampling and quantization of the analog signal. 

#### Sampling

> A discrete digital signal .x(n) can be sampled from continuous analog signal: .x(n) = xa(t), where .t = nT , .xa(t) is the analog signal, t is the time, and T is the sampling period.
> If the sampling rate .1/T is greater than twice the bands of the continuous signal, then this continuous signal can be reconstructed perfectly according to Nyquist–Shannon sampling theorem [16].
> The half of the sampling rate (frequency) is called Nyquist frequency.
> The bandwidth of an analog-to-digital conversion module is determined by the sampling rate and Nyquist frequency.
> When the bandwidth (or highest frequency) of a signal is above the Nyquist frequency, the sampled discrete signal will suffer from artifacts, which are known as aliasing.
> Aliasing refers to the phenomenon that different signals are indistinguishable or aliased together after being sampled, and as a consequence, there are artifacts when the signal is reconstructed from samples. 

#### Quantization

> Quantization is used to convert continuous values of a signal into discrete values, which is also called pulse-code modulation (PCM).
> For example, for a signal with a value range of .[−1, 1], we can quantize the values into integers in a range of .[0, 255], which is stored in binary with 8 bits.
> Thus, the resolution of the quantization is 256, or 8 bits.
> Since the resolution is not infinitely large, quantization usually introduces a small amount of error/noise to the signal, which can be measured by the signal-to-noise ratio (SNR).
> There are different types of PCM: (1) linear PCM, where the quantization is conducted in linearly uniform with the amplitude; (2) non-linear PCM, where the quantization levels vary with the amplitude according to different algorithms, such as .μ-law (in North America and Japan) or A-law algorithm (in Europe).
> For an input value x, the output of the .μ-law encoding is .F(x) = sgn(x)ln(1+μ|x|) ln(1+μ), where .−1 ≤ x ≤ 1,.μ = 255 in North America and Japan, and.sgn(x) is the sign function.
> The output of the A-law encoding is .F(x) = sgn(x)�A|x|1+ln(A),|x| <1A1+ln(A|x|)1+ln(A),1A≤ |x| < 1 , where A is the compression parameter, and.A = 87.6 in Europe. 

### 2.3.2.Time to Frequency Domain Transformation

> We introduce several methods to transform the audio signal from the time domain to the frequency domain, including Discrete-Time Fourier Transform (DTFT), Discrete Fourier Transform (DFT), Fast Fourier Transform (FFT), and Short-Time Fourier Transform (STFT). 

#### Discrete-Time Fourier Transform (DTFT)
#### Discrete Fourier Transform (DFT)
#### Fast Fourier Transform (FFT)

> FFT has the same mathematical formulation as DFT, but with fast implementation with optimization in the computation, resulting in.N log2N times speedup.
> Meanwhile, the FFT size (N) should be a power of two to ensure this fast speed. 

#### Short-Time Fourier Transform (STFT)

> For unstable signals like speech, DFT/FFF are not suitable.
> Therefore, we usually divide speech into frames, where each frame lasts for dozens of milliseconds and can be regarded as a stable signal.
> Then DFT/FFT can be applied.
> This process is called STFT, which usually consists of several steps: • Framing, which divides the waveform points into frames. • Windowing, which adds a window to each frame. • Transformation, which converts the time-domain signal in each frame into a frequency-domain signal (i.e., spectrum) using DFT or FFT. • Concatenation, which concatenates the spectrum of each frame together along the time axis to get the spectrograms of the speech signal.
> Besides the above processing steps, we usually have some preprocessing and postprocessing steps for STFT: • Pre-emphasis and de-emphasis.
> To avoid the distortions caused by the noisy signal, pre-emphasis is usually applied before the framing and windowing to boost the frequency range that is susceptible to noise.
> After the transmission or processing of this signal, de-emphasis is applied to transform it back to the original signal.
> In this way, the noise added to the susceptible frequency range in transmission or processing is attenuated. • Linear-to-mel scale transform.
> As we introduced in Sect. 2.2.3, human sensibility in terms of frequency is not in linear scale but in mel scale.
> Therefore, to make the spectrograms more consistent with human sensitivity, we further convert the linear-scale spectrograms obtained by STFT into mel-scale spectrograms using mel filters, such as triangular overlapping windows or cosine overlapping windows. 

### 2.3.3.Cepstral Analysis

> We first show a segment of waveform sequence in Fig. 2.8a and its spectrum after applying FFT in Fig. 2.8b.
> By looking at the spectrum of this speech segment, we can locate the formants and the spectral envelope (which can be regarded as a smoothing curve that connects the formants together), which contain important information about the speech.
> The high-frequency peaks in the spectrum represent the harmonics, and the low-frequency peaks denote the resonances.
> We can conduct cepstral analysis by applying the inverse Fourier transform of the logarithm of the spectrum, and get the cepstrum of this speech segment as shown in Fig. 2.8c.
> The term “cepstrum” is derived from the term “spectrum”, by reversing the first letters “spec”.
> Cepstral analysis is to explore the periodic structures in frequency spectra, which has many applications, such as pitch detection and spectral envelope analysis.
> We can apply the inverse of DFT of the logarithm magnitude of the spectrum .X(k) to get the cepstrum: 

$$
\tag{2.5}
$$

> where .0 ≤ n < N − 1.
> By looking at the resulting cepstrum as shown in Fig. 2.8c, the x-axis represents time quefrency (by swapping the letters “fre” and “que” in “frequency”), where larger time represents lower frequency, while lower time represents higher frequency.
> Note that there is a peak around .0.005 s in quefrency (nearly 200 Hz), which equals to vocal cord excitation period, i.e., inverse of fundamental frequency.
> We can also infer from Fig. 2.8a that this segment of speech waveform has a fundamental frequency of about 200 Hz (there are nearly 10 periods in .0.05 s, and thus the period is nearly .0.005 s and frequency is about 200 Hz). There is an empirical way to locate fundamental frequency from the cepstrum: we can assume the fundamental frequency of human voice is in the range from 80 to 450 Hz, and then the corresponding peak in the cepstrum should lie in the quefrency range from .0.0022 to.0.0125 s.



### 2.3.4.Linear Predictive Coding/Analysis

> Linear predictive coding (LPC) [12] is a method for compressed coding in speech processing, which leverages a linear predictive model to represent the spectral envelope of a speech signal.
> LPC is based on the source-filter model, where a sound source .e(n) (from vocal cords) goes through a filter .h(n) (vocal tract) to formulate a signal .x(n) = h(n) ∗ e(n).
> We usually have the resulting signal .x(n), but need to estimate the filter.h(n) and source signal.e(n).
> In LPC, the filter is formulated as a p-th order all-pole filter.
> Let us recall the digital filter design in digital signal processing.
> Each pole in a discrete-time system corresponds to a time delay, and thus the output signal .x(n) at time step n depends on the current input.e(n) and the previous samples.x(n − k), k ∈ [1, p], i.e.,. x(n) = �p k=1ak∗ x(n − k) + e(n).
> It is in accordance with the intuition that a speech signal at any time step can be approximated by the linear combination of the past samples.
> The coefficients can be obtained by minimizing the error.e(n) between the ground-truth speech samples and the linear-predicted ones.
> Given N samples, where .
> N � p, we can have N equations to determine the coefficient .ak, k ∈ [1, p], which can be solved in a mathematical way [12].
> After we have estimated the filter .h(n) (i.e., .ak, k ∈ [1, p]), we can estimate the source signal.e(n) by.x(n)−�p k=1ak∗x(n−k).
> The source signal .e(n) can either be an impulse train of a pitch frequency (voiced) or random noise (unvoiced), or a mixture of both voiced and unvoiced sound.
> We then describe how LPC can be used for compressed coding in speech processing.
> Usually, the compression is done on each frame of speech signal due to its time-varying property.
> We take an example to illustrate compression.
> For a frame of speech signal with 200 waveform samples, if choosing .p = 16, we have 16 coefficients . ak and a variance .e(n).
> Thus, we reduce the data size from 200 samples to 17 samples, with a reduction ratio of about 12. 

### 2.3.5.Speech Parameter Estimation

> In this subsection, we introduce several methods to estimate the speech parameters such as voiced/unvoiced/silent speech detection, fundamental frequency (usually denoted as F0) detection, and formant estimation. 

#### Voiced/Unvoiced/Silent Speech Detection 

> Voice activity detection is to detect speech segments and non-speech segments from a speech utterance (can be either clean or noisy speech).
> For clean speech, it is relatively easy for voice activity detection, e.g., using some energy-based method.
> However, for noisy speech, it is challenging.
> Therefore, we need to design sophisticated methods for voice activity detection.

> There are different methods for voice activity detection, such as traditional feature-based and learning-based methods.
> For feature-based methods, we usually extract some features that can determine speech or non-speech segments.
> Some useful features for voice activity detection include: (1) Speech energy.
> When the signal-to-noise ratio (SNR) is large, we can assume that the energy of the voiced part is larger than that of the unvoiced part and larger than that of silence.
> However, when SNR is small, e.g., the energy of speech is similar to that of noise, we cannot tell apart it is speech (voiced or unvoiced) or non-speech (noisy). (2) Frequency or spectrum feature.
> We can apply a short-time Fourier transform to get the spectrogram features of speech and judge speech and non-speech segments accordingly. (3) Cepstrum feature.
> The cepstrum feature can determine the pitch of the speech, which can decide whether a segment is voiced speech or not. (4) Zero-crossing rate (ZCR), which measures the ratio between the number of times that the speech waveform is crossing the zero point and the number of total speech waveform points in a certain time (e.g., a speech frame).
> Voiced speech is produced by periodic vibration and usually has a low zero-crossing rate.
> Unvoiced speech is produced by the high-frequency noisy source signal and usually has a high zero-crossing rate.
> The zero-crossing rate of silent speech is usually lower than that of unvoiced speech. (5) Autocorrelation coefficient at unit sample delay, which calculates the correlation between adjacent speech samples.
> Since voiced speech usually has a low frequency, adjacent waveform points of voiced speech are highly correlated while those of unvoiced speech are weakly correlated.

> After getting these features, we can judge voiced/unvoiced/silent speech either by some rules/thresholds or by training a machine learning model with labeled training data to infer voiced/unvoiced/silent speech.
> For example, we can simply use energy to separate voiced speech from unvoiced speech and silence and then use a zero-crossing rate to separate unvoiced speech from silence.

#### F0 Detection

> After we determine whether a speech segment is voiced, unvoiced, or silent, we can further detect the pitch/F0 of the voiced speech.
> There are different methods to detect F0, and we introduce a few methods as follows: (1) Autocorrelation for F0 estimation.
> We shift the speech waveform sequence x by n points to get a new waveform sequence . xn, and we calculate the correlation between x and . xn.
> The period (inverse of frequency) of this speech waveform is the n which has the largest and peak correlation. (2) DIO algorithm [17] used in WORLD [18] vocoder.
> We > can first use different low-pass filters, each with a different cutoff frequency to get filtered waveforms.
> For each waveform, we can calculate four types of intervals: the interval between two adjacent positive/negative zero-crossing points, and the intervals between two adjacent peaks and valleys.
> If the four interval values are close to each other, then the average of the intervals is more likely to be the period of the waveform. (3) F0 detection based on cepstral analysis.
> We can first compute the cepstrum of each speech frame and search for the cepstrum peak in a reasonable range.1 The peak can be usually found for a voiced speech, while not for an unvoiced speech.

#### Formant Estimation 

> Formant estimation refers to the estimation of the formant parameters such as resonant frequency and bandwidth.
> A typical method to estimate formant parameters is to use a low-pass lifter (inverse of filter) to filter the cepstrum to get the spectral envelope, and further locate the maximal value to get the resonant frequency and also the bandwidth. 


### 2.3.6.Overview of Speech Processing Tasks

> In the previous subsections, we have introduced some basic knowledge and methods in speech processing.
> Here we give a brief overview of some typical speech processing tasks, as shown in Fig. 2.9.
> There are some basic speech processing tasks, such as speech recognition, speaker recognition, speech synthesis, speech enhancement, speech separation, speech translation, etc.
> For speech synthesis, there are also several subtasks, such as text-to-speech synthesis, singing voice synthesis, voice conversion, EEG (electroencephalography) based speech synthesis, etc.
> The difference among these subtasks is the source input (e.g., text, music score, speech, or EEG signal).
> However, other subtasks such as singing voice synthesis, voice conversion, and EEG-based speech synthesis share some or most methodologies with text-to-speech synthesis.
> Thus, in this book, we mainly focus on text-to-speech synthesis. 

## 2.4.Reference

1. Manning C, Schutze H (1999) Foundations of statistical natural language processing. MIT Press 
2. Dance FEX, Larson C (1985) The functions of human communication. Inf Behav 1(1):62–75 
3. Akmajian A, Farmer AK, Bickmore L, Demers RA, Harnish RM (2017) Linguistics: An introduction to language and communication. MIT Press 
4. Crystal D (2011) A dictionary of linguistics and phonetics. John Wiley & Sons 
5. Stevens KN (1997) Articulatory-acoustic-auditory relationships. Handbook of phonetic sciences. Oxford 
6. Lass R (1984) Phonology: an introduction to basic concepts. Cambridge University Press 
7. Blumstein SE (1991) The relation between phonetics and phonology. Phonetica 48(2–4):108–119 
8. Borer H (2017) Morphology and syntax. The handbook of morphology, pp 149–190 
9. Cruse A (2006) Glossary of semantics and pragmatics. Edinburgh University Press 
10. Denes PB, Denes P, Pinson E (1993) The speech chain. Macmillan 
11. Denes PB, Pinson EN (2015) The speech chain: the physics and biology of spoken language. Waveland Press 
12. Makhoul J (1975) Linear prediction: A tutorial review. Proc IEEE 63(4):561–580 
13. Wang X, Takaki S, Yamagishi J (2019) Neural source-filter waveform models for statistical parametric speech synthesis. IEEE/ACM Trans Audio Speech Lang Process 28:402–415 
14. Cernak M, Rusko M (2005) An evaluation of synthetic speech using the PESQ measure. In Proc. European congress on acoustics, pp 2725–2728 
15. Taal CH, Hendriks RC, Heusdens R, Jensen J (2011) An algorithm for intelligibility prediction of time-frequency weighted noisy speech. IEEE Trans Audio Speech Lang Process 19(7):2125–2136 
16. Shannon CE (1949) Communication in the presence of noise. Proc IRE 37(1):10–21 
17. Morise M, Kawahara H, Katayose H (2009) Fast and reliable f0 estimation method based on the period extraction of vocal fold vibration of singing voice and speech. In Audio engineering society conference: 35th international conference: audio for games. Audio Engineering Society 
18. Morise M, Yokomori F, Ozawa K (2016) WORLD: a vocoder-based high-quality speech synthesis system for real-time applications. IEICE Trans Inf Syst 99(7):1877–1884