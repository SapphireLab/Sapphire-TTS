# 3.Basics of Deep Learning

## Abstract

> This chapter introduces some basics of deep learning [1], including some learning paradigms and key components of machine learning [2], and some representative model structures and frameworks in deep learning. Since TTS is a typical data generation task, we also introduce some representative deep generative models. 

> Prerequisite Knowledge for Reading This Chapter
> - Basic knowledge of algebra, calculus, and probability theory.
> - Basic knowledge of machine learning and deep learning.

## 3.1.Machine Learning Basics

Since deep learning is a branch of machine learning, we first introduce some basics of machine learning including learning paradigms and key components of machine learning [3].

### 3.1.1.Learning Paradigms

> Machine learning covers different learning paradigms. The most fundamental learning paradigms are supervised learning, unsupervised learning, and reinforcement learning.

#### Supervised Learning

> Supervised learning aims to learn a mapping function (the optimal solution in the hypothesis space) that converts the input to the output based on a set of training examples, where a training example is a pair of input and output. A supervised learning algorithm analyzes and detects the underlying patterns and relationships between the input and output of the training examples, and can generate output for unseen examples. Supervised learning is one of the most basic learning paradigms, and is widely used for regression and classification. It is easy to understand, train, and control what to learn by providing the training inputs and outputs. Some disadvantages of supervised learning include it is expensive and time-consuming to collect labeled training data, and it is difficult to learn information beyond the training pairs.

#### Unsupervised Learning

> Different from supervised learning which needs input and output pairs for training, unsupervised learning analyzes and discoveries data patterns from purely unlabeled data. Typical unsupervised learning approaches include clustering [4], anomaly detection [5], dimensionality reduction [6], latent variable models [7], etc. A benefit of unsupervised learning is that it does not rely on labeled data that is costly to collect. Some advantages of unsupervised learning include it is inaccurate and time-consuming for model learning.

#### Reinforcement Learning

> Different from supervised learning which requires labeled input and output data pairs for training or unsupervised learning that purely relies on unlabeled data to learn data patterns, reinforcement learning regards the feedback from the environment as the learning signal [8]. It is usually used in scenarios where an intelligent agent takes action in an environment to maximize its cumulative reward. Instead of finding an exact solution to map the input to the output as in supervised learning, reinforcement learning focuses on finding a balance between exploration 
(of uncharted territory) and exploitation (of current knowledge). Reinforcement learning is widely used in games, self-driving cars, finance, business management, and scenarios where exact labeled data is hard to get but feedback signal is easy to get. Beyond the three basic learning paradigms, there are a lot of learning paradigms that are practical when applying machine learning in different scenarios, including semi-supervised learning, self-supervised learning, pre-training/fine-tuning, transfer learning, curriculum learning, and active learning, etc.

#### Semi-Supervised Learning

> Semi-supervised learning lies in between supervised learning and unsupervised learning. It usually combines a small amount of labeled data with a large amount of unlabeled data for training. In practice, it first trains a model on the labeled data and then uses this model to predict the target of the unlabeled data. The unlabeled data with pseudo labels are combined with the labeled data to train the model. This process can be repeated several times for better learning accuracy.

#### Self-Supervised Learning

> Self-supervised learning is to learn from unlabeled data by constructing labels from the unlabeled data itself. It belongs to supervised learning since it relies on input-output data pairs for training. It can be also regarded as unsupervised learning since it does not need explicit data labels. However, it is also different from unsupervised learning since that learning is not achieved using inherent data patterns. Self-supervised learning has been widely used in natural language processing, speech, and computer vision in recent years, such as language modeling, masked language modeling, masked auto-encoder, contrastive learning, etc. Some prominent work include BERT [9], GPT [10–12], MASS [13] in natural language processing, and Wav2Vec [14, 15] in speech processing, and SimCLR [16] and MAE [17] in  computer vision.

#### Pre-training/Fine-Tuning 

> Pre-training and fine-tuning is a learning paradigm that first pre-trains a model with pretext tasks on large-scale datasets, and then fine-tunes the model on downstream tasks. Both supervised learning and self-supervised learning can be used in the pre-training stage. Most self-supervised learning methods introduced in the previous paragraph adopt the pre-training and fine-tuning paradigm. 

#### Transfer Learning 

> Transfer learning aims to reuse or transfer the knowledge learned in previous tasks/problems for learning or solving different but related tasks/problems, which can improve the efficiency of training data. Pre-training and fine-tuning are also regarded as a type of transfer learning. For other learning paradigms such as curriculum learning [18], active learning [19] or lifelong learning [20], we skip the introduction here and readers can find more details from some related machine learning materials. As text-to-speech synthesis is a typical machine learning task, most of the learning paradigms are applied to this task to enable speech synthesis in multi-lingual,
multi-speaker, and multi-domain scenarios. We will cover some popular learning paradigms such as supervised learning, unsupervised learning, semi-supervised learning, self-supervised learning, pre-training/fine-tuning, transfer learning, and curriculum learning in later chapters.

### 3.1.2.Key Components of Machine Learning

> Basically speaking, there are three key ingredients in machine learning: model, strategy, and algorithm [21]. The model is defined as a hypothesis space that covers all possible mappings from input to output, where this hypothesis space is usually constructed based on human prior. Machine learning methods try to find an optimal solution in this hypothesis space (i.e., to find the optimal parameters of the model). The strategy is a criterion or a method (e.g., a loss function) to judge whether a solution is optimal in this hypothesis space (i.e., whether the model parameters are optimal). Since the strategy can usually convert the goal of machine learning into solving the minimum of a loss function, the algorithm is a method (e.g., gradient descent) to get the minimum of the loss function. We take logistic regression [22] as an example to illustrate the three key ingredients of machine learning. The model of logistic regression maps the input x into output .f (x) =1
1+e−wx, where w is the model parameter. Denote y as the label of the input x. We choose cross-entropy loss function as the criterion (i.e., strategy):.L(x, y; w) = y log1
1+e−wx+(1−y) log(1−11+e−wx). Finally, we can use stochastic gradient descent (SGD) [23] as the algorithm to find the minimum of the cross-entropy loss function.

## 3.2.Deep Learning Basics

> As a branch of machine learning, deep learning leverages deep neural networks to learn good representations for data understanding or learn good data distributions for data generation. Deep learning adopts similar learning paradigms and components in machine learning. We further introduce some distinctive model structures and frameworks in deep learning.

### 3.2.1.Model Structures: DNN/CNN/RNN/Self-attention

> Deep learning has a more complicated hypothesis space with deeper and wider models than traditional machine learning. How to construct the model to form a good hypothesis space is important in deep learning. We introduce several typical building blocks for models in deep learning: dense neural network (DNN), convolutional neural network (CNN), recurrent neural network (RNN), and Transformer 
(i.e., self-attention based networks). DNN A dense neural network (DNN), also called a fully connected network (FCN), consists of multiple densely connected layers, or multi-layer perceptron, as shown in Fig. 3.1. Each node (neuron) i in layer .l + 1 connects to each node j in layer l with a weight . wij, and then followed with an activation function, such as tanh, sigmoid, ReLU and its variants. CNN A convolutional neural network (CNN) consists of convolution kernels or filters that slide along input features with shared weights and provide responses (i.e., feature maps), as shown in Fig. 3.2a. CNN is inspired by the biological process in visual perception that the connectivity pattern between neurons in CNN resembles the organization of the animal visual cortex, where each cortical neuron only responds to the stimuli in a restricted region of the visual field (i.e., receptive field). CNN can be regarded as a regularized version of DNN (multi-layer perceptron), by restricting each node i in layer .l + 1 to only connect to neighboring nodes in layer l. RNN In a recurrent neural network (RNN), there is a concept of “memory” to store the states or information of previous inputs and influence the generation of current output. Recurrent neural networks are used to process temporal sequences, where the data in a time step depends on the data points in previous time steps, as shown
in Fig. 3.2b. Since vanilla RNNs are prone to gradient vanishing or exploding problems, a lot of variants such as GRU (gated recurrent unit) [24] and LSTM (long short-term memory) [25] are proposed to solve this problem. Self-attention Different from CNN organizes the neuron connections in a local region with shared weights, or RNN organizes the neuron connections in a recurrent way, self-attention proposed in Transformer [26] (as shown in Fig.  3.2c) organizes the connections with a self-attention mechanism based on similarity measurement. After the cross-position information aggregations through self-attention, a feed-forward network is leveraged for position-wise information processing. When self-attention is used for autoregressive sequence generation, a causal attention mask is usually used to force the model to only attend to hidden states in previous steps. Thus, this attention mechanism is called causal self-attention, as shown in Fig. 3.2d. Self-attention as well as Transformer has been widely used as the backbone models in natural language processing, speech, and computer vision. Comparison Between Different Structures We compare DNN, CNN, RNN, and self-attention in Table 3.1 in terms of several characteristics: (1) computation complexity: the computation cost for each model structure; (2) sequential operations: how many sequential operations are needed to process a sequence; (3) maximal path length: the maximal length (in terms of
the number of tokens) of the path that connects any two tokens in a sequence. We assume the sequence has n tokens and the model hidden dimension is h. As  can be seen from Table 3.1, DNN has the largest computation cost (.n2d2) since it uses dense connections between different tokens and different neurons to calculate the hidden of each token. CNN and RNN have the computation proportional to the sequence length. Self-attention has a computation quadratic to the sequence length.1 In terms of sequential operation, RNN has sequential operations of n while other structures have parallel computation. The maximal path length captures the interaction efficiency between any two tokens in a sequence. For DNN, since it has dense connections between any two tokens, the maximal path length is 1. For CNN, if the kernel size is k, we need to stack .logk(n) layers (in the case of dilated convolutions [28, 29]) to reach a receptive field of n tokens, i.e., two farthest tokens can be processed in a convolutional kernel. Thus, the maximal path length is.logk(n). Due to the sequential operations in RNN, the maximal path length is n. In self-attention, since any two tokens can be connected through the self-attention mechanism, the maximal path length is 1. As can be seen, self-attention has advantages over other model structures in either computation complexity, sequential operations, or maximal path length.

### 3.2.2.Model Frameworks: Encoder/Decoder/Encoder-Decoder

> Since speech synthesis is a typical sequence processing task, we introduce some typical frameworks for sequence modeling. Generally speaking, there are three kinds of sequence tasks: (1) many-to-one, where a sequence is mapped to a single label, such as sequence classification; (2) one-to-many, where a sequence is generated based on a single label, such as conditional sequence generation; (3) many-to-many, where an input sequence is converted into another sequence, either in the same length (e.g., sequence tagging), or different lengths (e.g., machine translation, text summarization, speech-to-text recognition, text-to-speech synthesis). Based on this categorization, we introduce the corresponding framework for each kind of
sequence task: Encoder, Decoder, and Encoder-Decoder. Although we categorize different model frameworks for different sequence tasks, these frameworks are model structure agnostic, which means each framework can be implemented with DNN, RNN, CNN, or self-attention. Encoder An encoder usually takes a sequence of tokens as input and outputs a sequence of hidden representations. After that, these hidden representations are further used for classification or regression. Early works use RNN or CNN as the basic model structures for encoders, while recently Transformer with self-attention has been a popular structure for encoders. The most prominent one is BERT [9], which pre-trains a Transformer encoder on large-scale language corpus and fine-tunes it in downstream tasks to achieve state-of-the-art performance on sequence classification and language understanding. To enable classification or prediction, BERT introduces a special “CLS” token, which aggregates information across different token positions and is used as the prediction head in downstream tasks. Decoder A decoder usually models sequence with a next token prediction task.2 It is usually trained in a teacher-forcing way, where the previous ground-truth tokens are taken as the input to generate the current token. When generating the current token in inference, it takes the previously generated token as input, i.e., in an autoregressive way. An example is GPT [10–12], which pre-trains on large-scale language corpus and can be used for language modeling and generation. This autoregressive generation would cause error propagation, where the errors that occurred in previous tokens would affect the generation of later tokens. Encoder-Decoder An encoder-decoder framework is usually used for sequence-to-sequence tasks, where the encoder takes the source sequence as input and outputs a sequence of hidden representations, and the decoder generates target tokens conditioned on source hidden representations. In the early encoder-decoder framework, the decoder only takes the last hidden of source representations as condition [30, 31], while
later attention mechanism [32] is introduced into the encoder-decoder framework to extract information from source hidden representations for target token generation.

## 3.3.Deep Generative Models

> Intuitively speaking, a generative model describes how data is generated by capturing the probability of data through statistical learning. Then new data can be generated by sampling from the probability distribution learned by this model. A deep generative model follows the basic definition of generative models, is parameterized by a deep neural network . θ, and is trained to maximize its likelihood given the training data D. In this way, it learns a data distribution . Pθthat is similar to the true data distribution .PDand can generate new data from the learned data distribution . Pθ. Therefore, the key problem in deep generative models is probability/density estimation: to estimate an unobservable underlying probability/density function based on observed data. After getting a good density estimation, we can sample new data points (i.e., data generation), predict the density/probability of a data point (i.e., density estimation), etc. Depending on the types of density learned in model training, deep generative models can be divided into different categories, as shown in Fig. 3.3.
> - Exact density. They estimate data density by explicitly defining the density and designing specific model structures to compute it in a tractable way, such as autoregressive models and normalizing flows [33–37].
> - Approximate density. They model the tractable approximations of the data density, such as variational auto-encoders [38] and denoising diffusion probabilistic models [39, 40].
> - Unnormalized density. They learn the unnormalized density due to the intractability of the partition function, such as energy-based models [41].
> - Gradient of density. They estimate the gradient of the log probability density and use some methods such as Langevin dynamics or differential equations to generate data, such as score matching with Langevin dynamics [42, 43], stochastic differential equations (SDEs) [43], and probability flow ordinary differential equations (ODEs) [43, 44].
> - Implicit density. They learn the data density by comparing it with the true data density without explicitly defining it, such as generative adversarial networks [45]. Text-to-speech synthesis is a kind of data generation task, which can leverage deep generative models for better speech generation. Therefore, in this section, we introduce some representative generative models that are commonly used in text-to-speech synthesis, including autoregressive models (AR), normalizing flows 
(Flow), variational auto-encoders (VAE), denoising diffusion probabilistic models 
(Diffusion), score matching with Langevin Dynamics (SMLD), SDE/ODE, and generative adversarial networks (GAN), etc.

### 3.3.1.Autoregressive Models

> The term “autoregressive” comes from the term “autoregression”, which means it is a regression of the sequence element against itself. For example, a linear autoregressive model of order p is formulated as .yt= c + �p
i=1wiyt−i+ �t, where 
. wi,.i ∈ [1, p] are the model parameters, c is a constant, and. �tis white noise. In deep autoregressive models, . wiis implemented with non-linear deep neural networks, such as CNN, RNN, or self-attention. Autoregressive models can estimate the probability of data x in an exact way, by assuming each output in the current step depends on the data in the previous time steps and using the chain rule to decompose the probability of a sequence data x into a product of conditional probability in each step:

$$
P(x)=\sum_{i-1}^n P(x_i|x_{<i})
$$

where $x_{<i}$ represents the elements before position i in sequence x, and n is the sequence length. Autoregressive models are widely used in data generation, such as the RNN language model and GPT [10–12] for text generation, WaveNet [28] for speech synthesis, and PixelRNN [46]/PixelCNN [47] for image generation. Specifically, WaveNet [28] is the first neural-based vocoder for speech synthesis, which leverages dilated convolution to generate waveform points autoregressively. By regarding the data sample as a sequence of data points and modeling the dependency among data points in an autoregressive way, autoregressive models can better learn the complex internal structure dependency in data and provide exact data probability estimation. A side effect is that they suffer from slow generation speed
due to autoregressive modeling. Therefore, a lot of generative models are proposed to speed up the generation process by removing the autoregressive dependency among data points, such as normalizing flows, variational auto-encoders, and generative adversarial networks. Among them, normalizing flows can still provide the exact probability estimation as in autoregressive models.


### 3.3.2.Normalizing Flows

> Normalizing flows [33–37] are a kind of generative models that transform a data point from a standard distribution into a data point following complex data distribution with a sequence of invertible mapping functions [35]: .x = f0◦ f1◦ ...fk(z), where .z ∼ N(0, 1) is a data point from a standard Gaussian distribution, and x is a data point that follows the distribution that we want to generate, . fiwhere . i ∈ [1, k]
denotes the invertible mapping function. Since we can get a normalized (standard) probability distribution (e.g., Gaussian) from the data distribution through a flow 
(sequence) of invertible mapping functions: .z = f−1k◦ f−1
k−1◦ ...f−10(x), this kind  of flow-based generative models is called as normalizing flows. We can train a flow-based model by maximizing the log-likelihood of its model parameters given the data based on the rules of change of variables:

> where J denotes the Jacobian matrix of.f−1i(x), and.det(·) is the determinant of the matrix. Therefore, normalizing flows can estimate the data probability in an exact way, as in autoregressive models. To maximize the log-likelihood in an easy way based on Eq. 3.2, the transformation function f should satisfy two requirements: (1) it is easily invertible; (2) its Jacobian determinant is easy to compute.3 To reduce the complexity of Jacobian determinant, previous works either carefully design the model architectures with low-rank (e.g., Planar NF [35], Sylvester NF [48]), coupling (NICE [33], RealNVP [34], Glow [37]), or autoregressive (e.g., inverse AF [36], neural AF [49], Masked AF [50]) technologies, or design stochastic estimator of free-form Jacobian (e.g., FFJORD [51], Residual Flows [52]). We mainly introduce coupling (bipartite) and autoregressive technologies, which can ensure the invertible functions have triangular Jacobians so that the determinant can be easily calculated from the diagonal elements. Bipartite transformation and autoregressive transformation use two different granularities to split data x for dependency modeling: In autoregressive transformation, x is split into each step
. xtwhere .t ∈ [1, n] and n is the length of x, while in bipartite transformation, x is split into two parts . xaand . xb. We introduce different normalized flows based on these transformation functions, as shown in Table 3.2. Based on autoregressive transformations, there are two types of flows: autoregressive flow (AF) and inverse autoregressive flow (IAF). AF can be regarded as a basic autoregressive model parameterized with a single Gaussian:

$$

$$

where .μt(x<t) and .σt(x<t) are the shifting and scaling variables modeled by autoregressive models like PixelCNN [47] or WaveNet [28]. The inverse mapping is

$$

$$

Note that the training (evaluating z) can be parallel since. ztdoes not depend on. z<t, but the inference (generating x) is autoregressive since . xtdepends on . x<t. Instead of modeling the generation of x in an autoregressive manner like in AF, IAF models the generation of x in parallel but models the evaluation of z in an autoregressive manner:

$$

$$

IAF can be regarded as a dual formulation of autoregressive flow (AF) [49, 50]. The training of AF is parallel while the sampling is sequential. In contrast, the sampling in IAF is parallel while the inference for probability estimation is sequential. Based on bipartite transformations, NICE [33], RealNVP [34], and Glow [37] leverage coupling layer to ensure the output can be computed from the input and vice versa. The inverse mapping is:

$$

$$

where .μb(za) and .σb(za) are the shifting and scaling variables modeled by feed-forward networks. The forward mapping to generate x from z is:

$$

$$

Different from autoregressive transformation-based flows, bipartite transformation-based flows can enable parallel computation in both the training (evaluating z) and inference (generating x). There are some variations in bipartite transformation-based flows. Some works only use additive coupling layers, where the scaling term.σb(za) is set to 1, i.e., only additive term for shifting but no scaling, like that in NICE [33]. Some works use affine coupling layers, where the.σb(za) is usually not 1. In one coupling layer, some dimensions of x (i.e.,. xa) are unchanged. If these dimensions are always unchanged in all the coupling layers, it will greatly limit the model capacity. Thus, the order of dimension of x is reversed in each coupling layer, to ensure that all the dimensions have the chance to be changed. In Glow [37], this reverse operation is replaced by an invertible .1 ∗ 1 convolution, which can be regarded as a generalization of any permutation of the dimension order. Both autoregressive and bipartite transforms have their advantages and disadvantages [53]: (1) Autoregressive transforms are more expressive than bipartite transforms in modeling dependency between data distribution x and standard probability distribution z but require teacher distillation that is complicated in training. (2) Bipartite transforms enjoy a much simpler training pipeline, but usually require a larger number of parameters (e.g., deeper layers, larger hidden size) to reach comparable capacity with autoregressive transforms. Generally speaking, normalizing flows have several advantages: (1) The training process is very stable and much easier to converge, as simple as autoregressive models, unlike other generative models (e.g., VAEs and GANs) that require careful tuning. (2) Normalizing flows can estimate the data density in an exact way. Some disadvantages of normalizing flows include: (1) the model expressiveness is limited due to the requirement of invertible (bijective) mapping; (2) the bijective mapping requires a high dimensional latent space, which is usually difficult to interpret.

### 3.3.3.Variational Auto-encoders.

> Variational auto-encoders (VAEs) [38] are a kind of generative model that compress the input data into a constrained/regularized multivariate latent distribution through an encoder and reconstruct the input data as accurately as possible through a decoder. The latent distribution is regularized to some prior distribution during training to ensure that it is easy to sample from the latent space to generate new data. The term “variational” means that there is some relationship between the regularization and variational inference [54]. We first explain why we need regularization in VAEs by discussing naive auto-encoder. In auto-encoder, the encoder converts the data into latent representations, and the decoder converts the latent representations back into data, and
the encoder and decoder are trained by minimizing the reconstruction error, e.g., 
.�x − dec(enc(x))�2. When using an auto-encoder for data generation, we usually sample a random point from the latent space and decode it to obtain new data. Since no regularization on the latent space of the auto-encoder and the high complexity of the encoder and decoder in the auto-encoder, there would be overfitting, which means that some points in the latent space (corresponding to the training data) are highly optimized with low reconstruction loss, while other points (not covered in training) will correspond to high reconstruction loss with meaningless data. As a consequence, the latent space is extremely irregular and non-smoothing, which means some points that are close to each other in the latent space can decode to data points that are very different in the data space, and some points in the latent space can decode to data points that are meaningless in the data space. Thus, to enable data generation with good generalization, we need to regularize the latent space. A VAE also consists of an encoder and a decoder similar to an auto-encoder and is trained to minimize the reconstruction error. However, different from the auto-encoder that converts a data input into a single point in the latent space, the VAE encoder converts a data input x into a distribution .p(z|x) over the latent space, where a point is sampled from this distribution (.z ∼ p(z|x)) and decoded into a data output by the VAE decoder. We usually add a regularization term on the latent space to ensure the distribution encoded by the encoder to facilitate data generation. Thus, the loss function of VAEs is as follows:

where .z ∼ N(μx, σx) and .μx= encμ(x), σx= encσ(x). Generally speaking, the regularization should satisfy two properties: continuity and completeness. Continuity means two points that are close to each other in the latent space should be decoded into data in the data space that are also close to each other. Completeness means a point sampled from the distribution of the latent space should be decoded into meaningful data in the data space. We regularize both the mean and covariance matrix of the distributions generated by the encoder to be close to those of a standard Gaussian distribution. In this way, by constraining the mean to be close to 0, we can ensure the distribution is near to each other, and by constraining the covariance matrix to be close to the identity, we can ensure a smooth distribution instead of a punctual distribution. In this way, we encourage the encoder to encode data to be close to and overlapped with each other in the latent space, better satisfying the continuity and completeness conditions. We can also interpret the loss formulation of VAEs from the perspective of variational inference. Considering the data generation is depending on some random process involved by random variable z, we first sample z from.p(z) and then sample x from.p(x|z; θ), where . θ and z are unknown to us. The log-likelihood of its model

parameter . θ given data x can be formulated as 
.
log p(x) = log�p(x|z)p(z)dz = log�q(z|x)p(x|z)p(z)
q(z|x)dz
= log Ez∼q(z|x)
p(x|z)p(z)
q(z|x)≥ Ez∼q(z|x)log p(x|z)p(z)q(z|x)
= Ez∼q(z|x)log p(x|z) − KL(q(z|x)||p(z)),
(3.9) where the . ≥ in the above equation is derived based on Jensen inequality [55]. To maximize the log-likelihood .log p(x), instead, we can maximize its lower bound 
.Ez∼q(z|x)log p(x|z) − KL(q(z|x)||p(z)). Thus, the loss function becomes 
.L(x; θ, φ) = −Ez∼q(z|x;φ)log p(x|z; θ) + KL(q(z|x; φ)||p(z))(3.10) VAEs have some advantages: (1) the latent code is generated by data x itself, so the correspondence between latent z and data x in the VAE decoder is stronger; (2) the latent space is regularized with continuity and completeness, so the generated data is more regular than auto-encoders. However, since VAEs use point-wise loss without global information, it is blurry with lower quality. VAEs can only obtain the lower bound of the log-likelihood, not like autoregressive models and normalizing flows that can obtain exact likelihood.

### 3.3.4.Denoising Diffusion Probabilistic Models

Denoising diffusion probabilistic models (DDPMs or Diffusion Models for short) [39, 40] are a kind of generative model that consists of two processes: a forward process and a backward process. The forward process is a Markov chain that transforms the data. x0into a prior. xT(usually standard Gaussian) by iteratively injecting Gaussian noise into . x0according to a pre-defined noise schedule . β with 
.0 < β1< ... < βT< 1, as follows: 
.q(x1:T|x0) =
T
�
t=1
q(xt|xt−1),q(xt|xt−1) = N(xt;�1 − βtxt−1, βtI),(3.11) where.q(xt|xt−1) denotes the transition probability at time step t. We can obtain the noisy distribution of. xtby.q(xt|x0) = N(xt; √¯αtx0, (1− ¯αt)I), where.αt:= 1−βt, and .¯αt:= �ts=1αsrepresents the noise level at time step t. .q(xT|x0) converges to standard Gaussian distribution .N(xT; 0, I) if .¯αTis small enough. The reverse
process gradually transforms the prior noise from .xT∼ N(0, I) to data . x0through 
. pθ(x0:T) = p(xT)
T
�
t=1
pθ(xt−1|xt),pθ(xt−1|xt) = N (xt−1; μθ(xt, t), �θ(xt, t)) ,
(3.12) where .p(xT) = N(xT; 0, I), and .pθ(xt−1|xt) is the transition probability in each reverse step, parameterized by . μθand . �θ. The network . θ is trained by maximizing the evidence lower bound (ELBO) of the likelihood of.pθ(x0). To deduce the ELBO, we can first write the log-likelihood as 
. log p(x0) = log�p(x0:T)dx1:T= log�q(x1:T|x0) p(x0:T)
q(x1:T|x0)dx1:T
= log Ex1:T∼q(x1:T|x0))p(x0:T)
q(x1:T|x0) ≥ Ex1:T∼q(x1:T|x0))logp(x0:T)q(x1:T|x0)
= ELBO.
(3.13) Then we can obtain the ELBO as 
. ELBO = Ex1:T∼q(x1:T|x0))logp(x0:T)
q(x1:T|x0)
= −Eq�KL(q(xT|x0)||p(xT))����LT+T�t=2KL(q(xt−1|xt, x0)||pθ(xt−1|xt))����Lt−1
−
log pθ(x0|x1)
����
L0�.
(3.14) Since the variance . βtis fixed to constants in the forward process, the posterior 
.q(xT|x0) has no learnable parameters and .p(xT) is a standard Gaussian, and thus 
.LTis a constant during training and can be ignored. For the term .Lt−1, we first rewrite the forward process posteriors .q(xt−1|xt, x0) by Bayes rule as: 
.q(xt−1|xt, x0) = N(xt−1; ˜μt(xt, x0), ˜βtI),(3.15) 
.
˜μt(xt, x0) :=
√¯αt−1βt
1 − ¯αtx0+
√αt(1 − ¯αt−1)
1 − ¯αtxt,
˜
βt:= 1 − ¯αt−1
1 − ¯αtβt.(3.16) We usually set .�θ(xt, t) = ˜βtI so that we do not need to learn the variance term. To learn the mean term .μθ(xt, t), since .x0=1
√ ¯αt(xt(x0, �) − √1 − ¯αt�), we can
rewrite.˜μt(xt, x0) in above equation as.1
√αt(xt(x0, �)−βt√1− ¯αt�). Instead of directly setting.˜
μt(xt, x0) as the learning target, we usually set the standard Gaussian noise. �
as the learning target, parameterized by.�θ(√¯αtx0+√1 − ¯αt�), t). Then the ELBO becomes 
. − ELBO = C +
T
�
t=1
Ex0,��β2t
2σ2tαt(1 − ¯αt)�� − �θ(�¯αtx0+�1 − ¯αt�, t)�2�.
(3.17) Ho et al. [40] further demonstrate the effectiveness to drop the weighting factor in the above loss functions and use a simplified training objective as follows: 
.Lsimple(θ) := Et,x0,���� − �θ(xt, t)�2�.(3.18) As a summary, the training and inference procedure of diffusion models are shown in Algorithms 1 and 2.

### 3.3.5.Score Matching with Langevin Dynamics, SDEs, and ODEs

Score matching with Langevin dynamics (SMLD) [42, 43] is similar to diffusion models in that both of them involve the process that corrupts data by gradually adding noises and generates the data by gradually removing the noises. The term 
“score” denotes the gradient of the log probability density with regard to the data.
SMLD estimates the score of the data at different noise levels with a learnable score function and leverages Langevin dynamics to generate data sequentially. Similar to diffusion models, we first denote the noise perturbation (forward) process .qσ(xt|x0) that transforms the data . x0into . xt: 
.qσ(xt|x0) = N(xt; x0, σ2tI),(3.19) where . σ is a sequence of positive noise scales satisfying .σ1< ... < σt< ... < σT. 
. σ1is small enough so that.qσ(x1|x0) ≈ p(x0) which is the data distribution, and . σT
is large enough so that .qσ(xT|x0) ≈ N(0, σ2TI). We can obtain the score.∇xlog qσ(xt|x0) in training in an analytic form, and train a noise conditional score network [42].sθ(xt, σt) to match the score. ∇xlog qσ(xt|x0)
using the following loss function: 
.arg min
θ
�Tt=1σ2tEp(x0)Eqσ(xt|x0)||sθ(xt,σt)−∇xlog qσ(xt|x0)||22.(3.20) After obtaining the score function.sθ(xt, σt), we can sample data using Langevin dynamics (Markov chain Monte Carlo) [42]: 
.xmt= xm−1t+ �t∗ sθ(xm−1t, σt) +�2�tzmt, m = 1, 2, ..., M,(3.21) where . �tis the step size and .zmtis standard Gaussian. The sampling process is repeated from .t = N, N − 1, ..., 1, with .x0N∼ N(0, σ2TI) and .x0t= xM
t+1when 
. t < N. As.M → ∞ and .�t→ 0, .XM1∼ p(x0). We can further extend the discrete time steps to the continuous time variable t. In this way, we use.x(t) and.σ(t) to denote. xtand. σt..σ(t) has an infinite number of noise scales, and the distribution of the perturbed data.x(t) ∼ qσ(x(t)|x(0)) evolves as a stochastic differential equation (SDE) [43]: 
.dx = f (x, t)dt + g(t)dw,(3.22) where w is the standard Brownian motion, .f (·, t) is called the drift coefficient of .x(t), and .g(t) is the diffusion coefficient of .x(t). Similarly, we can obtain a corresponding SDE for the reverse/denoising process [43, 56]: 
.dx = [f (x, t) − g(t)2∇xlog pt(x)]dt + g(t)d ¯w,(3.23) where . ¯w is a standard Brownian motion as the time flows from T to 0. According to [43], for all diffusion and denoising processes in stochastic versions as introduced above, there is a corresponding deterministic process whose diffusion and denoising trajectories have the same marginal probability densities 
.{pt(x)}Tt=o. This deterministic process can be formulated as a probability flow
ordinary differential equation (ODE) [43]: 
.dx = [f (x, t) − 12g(t)2∇xlog pt(x)]dt.(3.24) For the details of how to train the score function to estimate the score . ∇xlog pt(x)
and how to sample data according to the reverse SDE and ODE processes, readers can refer to [43].


### 3.3.6.Generative Adversarial Networks

> Generative adversarial networks (GANs) [45] is a kind of generative model that generate data from a random vector sampled from a simple distribution, and are widely used in many data generation tasks, such as image generation [45, 57, 58], speech and audio generation [59–61], text generation [62], etc. Before introducing the mathematical formulation of GANs, we introduce the intuitive idea behind GANs. Generating data can be regarded as a process to generate an N-dimensional vector that follows the distribution of the data. One way is to transform a simple N-dimensional random vector into this desired vector using a complex function, i.e., to generate a vector following a specific data distribution given a random vector. Due to the complexity of data distribution, we cannot explicitly express this transformation function, and instead, formulate it as a deep neural network (a generator) and learn it from data. The key is how to train the generator to generate data that matches the true data distribution. There are different methods to train the generators. One straightforward way is to compare the distribution generated by the generator and the true data distribution directly based on samples, and backpropagate the distribution matching error to adjust the weights of the generator. However, it is difficult to directly compare two probability distributions based on data samples. Therefore, some works propose indirect ways to match the two distributions, where GANs are one of the most prominent methods. GANs introduce a discriminator upon the generator to form an adversarial game: the discriminator takes the generated data and true data and tries to classify them as well as possible; the generator tries to fool the discriminator as much as possible, i.e., generating data that can be classified as true by the discriminator. GANs do not estimate the data density explicitly like that in autoregressive models or normalizing flows. Instead, they learn the transformation parameterized by the generator from a simple distribution (e.g. standard Gaussian) to the training data distribution. During inference, they sample a random noise from the standard Gaussian distribution and use the generator to transform it into a data point in the training distribution. The loss function of GANs can be formulated as follows 
Ex∼pdatalog D(x; φ) + Ex∼pzlog(1 − D(G(z; θ); φ)),(3.25) where. θ and. φ denote the parameter of generator and discriminator respectively, and 
.pdataand . pzdenote the true data distribution and standard Gaussian distribution.

### 3.3.7.Comparisons of Deep Generative Models

> Data generation based on deep generative models can be regarded as a process to transform a random variable z from a simple prior distribution.p(z) (usually a simple distribution like standard Gaussian distribution) into the desired complex data x following a distribution of .p(x).4 We illustrate different deep generative models to achieve this data transformation in Fig. 3.4. For example, GAN achieves this by its generator, VAE achieves this by its decoder, Flow achieves this by a series of inverse mapping .f−1k...f−11, and Diffusion achieves this by the iterative inverse process .xT→ · · · → xt→ · · · → x0. We further explain why we need these sophisticated designs in different generative models. Let us first check a naive way to learn a generative model to achieve this data transformation: we can randomly sample a latent variable z from .p(z) and randomly pick a data x from.p(x), and try to optimize the generative model to align the two data by.x = f (z; θ). However, this simple learning and optimization will never work since there is no consistent relationship between latent z and data x. For example, for a data point x optimized in different training iterations, different z may be sampled. A reasonable way is to assign each data point x with a consistent latent variable z that would not change over the training iterations. In this way, the generative model will try its best to learn (overfit) the mapping between z and x. However, since the random variables are randomly assigned to each data point, the learned generative model will not have a generalization ability. It is because we may sample two close latent variables z for two data points that are very different, or the latent variables are very different for two similar data points. To ensure the generative models can be optimized and generalized well, we need to associate the data point x to the random variable z in a consistent and generalized way. Thus, we need an inverse mapping function from data to latent .z = f (x; φ). We introduce how the inverse mapping function is implemented in different deep generative models. (1) In VAE, the encoder maps the data point x into latent variable .z ∼ q(z|x; φ) and reconstructs the data point x based on this z. To achieve data generation from latent variable z randomly sampled from prior distribution .p(z), the posterior distribution .q(z|x; φ) is regularized by the prior distribution. (2) In Flow, the data x is transformed by the forward mapping . f (·) into .z ∼ p(z). Since the forward mapping function is invertible, the generation from z to x can be achieved by the inverse mapping .f−1(·). (3) In Diffusion, the data x is destroyed into random Gaussian noise z with a series of diffusion steps .q(xt|xt−1). The data generation from z to x is achieved by the inverse/denoising process .pθ(xt−1|xt), which is optimized by maximizing the evidence lower bound of the log-likelihood. (4) GAN solves this problem from a very different perspective: its discriminator learns to distinguish the data generated by its generator from the real data, and the generator and discriminator play an adversarial game. In this way, GAN just randomly samples a latent variable z and guides the generation of x using a distribution loss with a discriminator, instead of a point-wise loss as in VAE. Thus, the adversarial game can automatically guarantee that the data point x and the random variable z are associated in a consistent and generalized way. We list the characteristics of different kinds of generative models, as shown in Table 3.3.5 We summarize as follows:

> - High-Quality Generation. Normalizing flows have to ensure the invertibility of the mapping function to enable exact likelihood estimation and data sampling, with a sacrifice of model capacity and thus sampling quality. VAEs usually adopt L1/L2 loss for data reconstruction and thus result in blurred generation results. Other generative models can usually generate high-quality samples.
> - Fast Sampling. Autoregressive models generate samples token by token, which suffer from slow generation speed. Other iterative-based models such as diffusion models, score matching with Langevin dynamics (SMLD), SDEs, and ODEs also suffer from slow inference speed due to multiple time steps used to guarantee high sampling quality. VAEs and GANs support parallel/non-autoregressive generation, with fast sampling speed. For flow-based models, they either leverage bipartite factorization that requires stacking multiple mapping steps or autoregressive factorization that requires autoregressive training/inference, both of which have slow inference speeds.
> - Model Diversity. All these models except GANs can generate samples with diverse modes. GANs usually suffer from model collapse problems where the model can only capture and generate a part of data distribution.
> - Density Estimation. Autoregressive models use the chain rule to decompose the probability of data into a product of multiple single-step distributions and thus can estimate the probability in an exact way. Normalizing flows perform tractable density estimation of the data points and can give exact data probability. Unlike autoregressive models and normalizing flows, both VAEs and diffusion models can only give a lower bound of the data probability since they maximize the evidence lower bound of the likelihood (the true likelihood. p(x) =�p(x|z)p(z)dz is very hard to be computed since it is intractable to calculate all possible values of the latent variable z). For SMLD and SDEs, they only care the score function instead of the data density, and thus cannot estimate the data probability. For probability flow ODEs, they can estimate the data probability in an exact way. GANs model the probability of data samples in an implicit way, and thus they cannot estimate the data probability.
> - Latent Manipulation. Except for autoregressive models, all generative models can support latent manipulations to some extent. Some GAN-based models used in TTS do not take random Gaussian noise as model input and thus cannot support latent manipulation.
> - Error Propagation. Iterative-based methods, such as autoregressive models, diffusion models, SMLD, SDEs, and ODEs suffer from error propagation, as they typically use the ground-truth intermediate data for training (i.e., teacher-forcing) while the generated data for inference.
> - Stable Training. The training of VAEs requires the tradeoff between the data reconstruction loss and the KL loss, which is not stable if loss weights are not properly used. GANs also suffer from notoriously unstable training due to the tradeoff between the generator and the discriminator.


## References

1. Goodfellow I, Bengio Y, Courville A (2016) Deep learning. MIT Press 
2. Bishop CM (2006) Pattern recognition and machine learning. Springer 
3. Mitchell TM, Mitchell TM (1997) Machine learning, vol 1. McGraw-hill, New York 
4. Xu R, Wunsch D (2005) Survey of clustering algorithms. IEEE Trans Neural Netw 16(3):645– 
678 
5. Chandola V, Banerjee A, Kumar V (2009) Anomaly detection: A survey. ACM Comput Surv 
(CSUR) 41(3):1–58 
6. Van Der Maaten L, Postma E, Van den Herik J, et al (2009) Dimensionality reduction: a comparative review. J Mach Learn Res 10(66–71):13 
7. Bishop CM (1998) Latent variable models. In: Learning in graphical models. Springer, pp 
371–403 
8. Sutton RS, Barto AG (2018) Reinforcement learning: an introduction. MIT Press 
9. Devlin J, Chang MW, Lee K, Toutanova K (2018) BERT: Pre-training of deep bidirectional Transformers for language understanding. Preprint. arXiv:1810.04805 
10. Radford A, Narasimhan K, Salimans T, Sutskever I, et al (2018) Improving language understanding by generative pre-training 
11. Radford A, Wu J, Child R, Luan D, Amodei D, Sutskever I, et al (2019) Language models are unsupervised multitask learners. OpenAI Blog 1(8):9 
12. Brown T, Mann B, Ryder N, Subbiah M, Kaplan JD, Dhariwal P, Neelakantan A, Shyam P, Sastry G, Askell A, et al (2020) Language models are few-shot learners. Adv Neural Inf Process Syst 33:1877–1901 
13. Song K, Tan X, Qin T, Lu J, Liu TY (2019) Mass: masked sequence to sequence pre-training for language generation. In: International conference on machine learning. PMLR, pp 5926– 
5936 
14. Schneider S, Baevski A, Collobert R, Auli M (2019) wav2vec: unsupervised pre-training for speech recognition. In: Proc Interspeech 2019, pp 3465–3469 
15. Baevski A, Zhou Y, Mohamed A, Auli M (2020) wav2vec 2.0: A framework for self-supervised learning of speech representations. Adv Neural Inf Process Syst 33:12449–12460 
16. Chen T, Kornblith S, Norouzi M, Hinton G (2020) A simple framework for contrastive learning of visual representations. In: International conference on machine learning. PMLR, pp 1597– 
1607 
17. He K, Chen X, Xie S, Li Y, Dollár P, Girshick R (2022) Masked autoencoders are scalable vision learners. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp 16000–16009 
18. Bengio Y, Louradour J, Collobert R, Weston J (2009) Curriculum learning. In: Proceedings of the 26th annual international conference on machine learning, pp 41–48
19. Settles B (2009) Active learning literature survey. University of Wisconsin-Madison Department of Computer Sciences 
20. Parisi GI, Kemker R, Part JL, Kanan C, Wermter S (2019) Continual lifelong learning with neural networks: a review. Neural Netw 113:54–71 
21. Li H (2012) Statistical learning methods. Tsinghua University Press, pp 95–115 
22. Kleinbaum DG, Dietz K, Gail M, Klein M, Klein M (2002) Logistic regression. Springer 
23. Bottou L (2012) Stochastic gradient descent tricks. In: Neural networks: tricks of the trade. Springer, pp 421–436 
24. Cho K, van Merriënboer B, Bahdanau D, Bengio Y (2014) On the properties of neural machine translation: encoder–decoder approaches. In: Proceedings of SSST-8, eighth workshop on syntax, semantics and structure in statistical translation, pp 103–111 
25. Hochreiter S, Schmidhuber J (1997) Long short-term memory. Neural Comput 9(8):1735–1780 
26. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser Ł, Polosukhin I 
(2017) Attention is all you need. In: Advances in neural information processing systems, pp 
5998–6008 
27. LeCun Y, Bottou L, Bengio Y, Haffner P (1998) Gradient-based learning applied to document recognition. Proc IEEE 86(11):2278–2324 
28. van den Oord A, Dieleman S, Zen H, Simonyan K, Vinyals O, Graves A, Kalchbrenner N, Senior A, Kavukcuoglu K (2016) WaveNet: A generative model for raw audio. Preprint. arXiv:1609.03499 
29. Kalchbrenner N, Espeholt L, Simonyan K, Oord Avd, Graves A, Kavukcuoglu K (2016) Neural machine translation in linear time. Preprint. arXiv:1610.10099 
30. Cho K, van Merrienboer B, Gülçehre Ç, Bahdanau D, Bougares F, Schwenk H, Bengio Y 
(2014) Learning phrase representations using RNN encoder-decoder for statistical machine translation. In: EMNLP 
31. Sutskever I, Vinyals O, Le QV (2014) Sequence to sequence learning with neural networks. In: Ghahramani Z, Welling M, Cortes C, Lawrence ND, Weinberger KQ (eds) Advances in neural information processing systems 27: Annual conference on neural information processing systems 2014, December 8–13 2014, Montreal, Quebec, Canada, pp 3104–3112. https:// proceedings.neurips.cc/paper/2014/hash/a14ac55a4f27472c5d894ec1c3c743d2-Abstract.html 
32. Bahdanau D, Cho KH, Bengio Y (2015) Neural machine translation by jointly learning to align and translate. In: 3rd International Conference on Learning Representations, ICLR 2015 
33. Dinh L, Krueger D, Bengio Y (2014) NICE: Non-linear independent components estimation. Preprint. arXiv:1410.8516 
34. Dinh L, Sohl-Dickstein J, Bengio S (2016) Density estimation using Real NVP. Preprint. arXiv:1605.08803 
35. Rezende D, Mohamed S (2015) Variational inference with normalizing flows. In: International conference on machine learning. PMLR, pp 1530–1538 
36. Kingma DP, Salimans T, Jozefowicz R, Chen X, Sutskever I, Welling M (2016) Improved variational inference with inverse autoregressive flow. Adv Neural Inf Process Syst 29:4743– 
4751 
37. Kingma DP, Dhariwal P (2018) Glow: generative flow with invertible 1×1 convolutions. In: Proceedings of the 32nd international conference on neural information processing systems, pp 10236–10245 
38. Kingma DP, Welling M (2013) Auto-encoding variational Bayes. Preprint. arXiv:1312.6114 
39. Sohl-Dickstein J, Weiss E, Maheswaranathan N, Ganguli S (2015) Deep unsupervised learning using nonequilibrium thermodynamics. In: International conference on machine learning. PMLR, pp 2256–2265 
40. Ho J, Jain A, Abbeel P (2020) Denoising diffusion probabilistic models. Preprint. arXiv:2006.11239 
41. LeCun Y, Chopra S, Hadsell R, Ranzato MA, Huang FJ (2006) A tutorial on energy-based learning. To appear in Predict Struct Data 1(0) 
42. Song Y, Ermon S (2019) Generative modeling by estimating gradients of the data distribution. In: Wallach HM, Larochelle H, Beygelzimer A, d’Alché-Buc F, Fox EB, Garnett R (eds) Advances in neural information processing systems 32: Annual conference on neural information processing systems 2019, NeurIPS 2019, December 8–14, 2019, Vancouver, BC, Canada, pp 11895–11907. https://proceedings.neurips.cc/paper/2019/hash/ 
3001ef257407d5a371a96dcd947c7d93-Abstract.html 
43. Song Y, Sohl-Dickstein J, Kingma DP, Kumar A, Ermon S, Poole B (2020) Score-based generative modeling through stochastic differential equations. In: International conference on learning representations 
44. Song J, Meng C, Ermon S (2020) Denoising diffusion implicit models. Preprint. arXiv:2010.02502 
45. Goodfellow IJ, Pouget-Abadie J, Mirza M, Xu B, Warde-Farley D, Ozair S, Courville A, Bengio Y (2014) Generative adversarial nets. In: NIPS 
46. van den Oord A, Kalchbrenner N, Kavukcuoglu K (2016) Pixel recurrent neural networks. In: International conference on machine learning. PMLR, pp 1747–1756 
47. van den Oord A, Kalchbrenner N, Vinyals O, Espeholt L, Graves A, Kavukcuoglu K 
(2016) Conditional image generation with PixelCNN decoders. In: Proceedings of the 30th international conference on neural information processing systems, pp 4797–4805 
48. van der Berg R, Hasenclever L, Tomczak JM, Welling M (2018) Sylvester normalizing flows for variational inference. Preprint. arXiv:180305649 
49. Huang CW, Krueger D, Lacoste A, Courville A (2018) Neural autoregressive flows. In: International conference on machine learning. PMLR, pp 2078–2087 
50. Papamakarios G, Pavlakou T, Murray I (2017) Masked autoregressive flow for density estimation. In: Proceedings of the 31st international conference on neural information processing systems, pp 2335–2344 
51. Grathwohl W, Chen RT, Bettencourt J, Sutskever I, Duvenaud D (2018) FFJORD: Free-form continuous dynamics for scalable reversible generative models. In: International conference on learning representations 
52. Chen RTQ, Behrmann J, Duvenaud D, Jacobsen J-H (2019) Residual flows for invertible generative modeling. In: Proceedings of the 33rd International Conference on Neural Information Processing Systems, pp 9916–9926 
53. Ping W, Peng K, Zhao K, Song Z (2020) WaveFlow: a compact flow-based model for raw audio. In: International conference on machine learning. PMLR, pp 7706–7716 
54. Blei DM, Kucukelbir A, McAuliffe JD (2017) Variational inference: A review for statisticians. J Am Stat Assoc 112(518):859–877 
55. Jensen JLWV (1906) Sur les fonctions convexes et les inégalités entre les valeurs moyennes. Acta Math 30(1):175–193 
56. Anderson BD (1982) Reverse-time diffusion equation models. Stoch Process Appl 12(3):313– 
326 
57. Zhu JY, Park T, Isola P, Efros AA (2017) Unpaired image-to-image translation using cycle-consistent adversarial networks. In: Proceedings of the IEEE international conference on computer vision, pp 2223–2232 
58. Karras T, Laine S, Aila T (2019) A style-based generator architecture for generative adversarial networks. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp 4401–4410 
59. Donahue C, McAuley J, Puckette M (2018) Adversarial audio synthesis. In: International conference on learning representations 
60. Kumar K, Kumar R, de Boissiere T, Gestin L, Teoh WZ, Sotelo J, de Brébisson A, Bengio Y, Courville A (2019) MelGAN: Generative adversarial networks for conditional waveform synthesis. In: NeurIPS 
61. Kong J, Kim J, Bae J (2020) HiFi-GAN: Generative adversarial networks for efficient and high fidelity speech synthesis. Adv Neural Inf Process Syst 33:17022 
62. Yu L, Zhang W, Wang J, Yu Y (2017) SeGAN: Sequence generative adversarial nets with policy gradient. In: Proceedings of the AAAI conference on artificial intelligence, vol 31