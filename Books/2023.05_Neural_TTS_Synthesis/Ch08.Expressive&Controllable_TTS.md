# 8.Expressive and Controllable TTS

## Abstract

> The goal of text-to-speech synthesis is to generate intelligible and natural speech, where the naturalness largely depends on the expressiveness of the synthesized voice.
> Generally speaking, expressiveness is determined by multiple characteristics, such as content, timbre, emotion, and style, etc.
> A key for expressive speech synthesis is to handle the problem of one-to-many mapping, which refers to that there are multiple speech variations corresponding to the same text, in terms of duration, pitch, sound volume, speaker style, emotion, etc.
> Modeling the one-to-many mapping under the regular L1 loss [123, 124] without enough input information will cause over-smoothing mel-spectrogram prediction [125, 126], e,g., predicting the average mel-spectrograms in the dataset instead of capturing the expressiveness of every single speech utterance, which leads to low-quality and less expressive speech.
> Therefore, providing this variation information as input and better modeling this variation information is important to alleviate this problem and improve the expressiveness of synthesized speech.

> Furthermore, by providing variation information as input, we can achieve controllable speech synthesis.
> Specifically, we can disentangle, control, and transfer the variation information as follows: (1) by adjusting this variation information (any specific speaker timbre, style, accent, speaking rate, etc) in inference, we can control the synthesized speech; (2) by providing the variation information corresponding to another style, we can transfer the voice to this style; (3) in order to achieve fine-grained voice control and transfer, we need to disentangle different variation information, such as content and prosody, timbre and noise, etc. 

> Therefore, expressive and controllable TTS covers broad topics including modeling, disentangling, controlling, and transferring the content, timbre, style, and emotion, etc.
> In this chapter, we first conduct a comprehensive analysis of this variation information (Sect. 8.1), and then introduce some technologies for expressive (Sect. 8.2) and controllable (Sect. 8.3) TTS, including modeling, disentangling, controlling, and transferring this variation information.

## 8.1.Categorization of Variation Information in Speech

> We first categorize the information needed for speech synthesis into four aspects as follows.

### 8.1.1.Text/Content Information

> Text information can be characters or phonemes, and represents the content of the synthesized speech (i.e., what to say).
> Some works improve the representation learning of text through enhanced word embeddings or text pre-training [89, 90, 127–129], aiming to improve the quality and expressiveness of synthesized speech.

### 8.1.2.Speaker/Timbre Information

> Speaker or timbre information, which represents the characteristics of speakers (i.e., who to say).
> Some multi-speaker TTS systems explicitly model the speaker representations through a speaker lookup table or speaker encoder [39, 43, 116, 130, 131].

### 8.1.3.Style/Emotion Information

> Style and emotion information, which is determined by the prosody, intonation, stress, and rhythm of speech and represents how to say the text [132, 133].
> Style and emotion are important to improve the expressiveness of speech and the vast majority of works on expressive TTS focus on improving the style/emotion of speech [1, 2, 15, 22, 134, 135].

### 8.1.4.Recording Devices or Noise Environments

> Recording devices or noise environments, which are the channels to convey speech, and are not related to the content/speaker/prosody of speech, but will affect speech quality.
> Research works in this area focus on disentangling, controlling, and denoising for clean speech synthesis [3, 5, 19].

## 8.2.Modeling Variation Information for Expressive Synthesis

> Many methods have been proposed to model different types of variation information in different granularities, as shown in Table 8.1.
> We introduce them in the following subsections.

### 8.2.1.Explicit or Implicit Modeling

> We can categorize the works according to the types of information being modeled: 
> (1) explicit information, where we can explicitly get the labels of this variation information; 
> (2) implicit information, where we can only implicitly obtain this variation information.

> For explicit information, we directly use them as input to enhance the models for expressive synthesis.
> We can obtain this information through different ways: 
> (1) Get the language ID, speaker ID, style, and prosody from labeling data [35, 39, 136, 137].
> For example, the prosody information can be labeled according to some annotation schemas, such as ToBI [155], AuToBI [156], Tilt [157], INTSINT [158], and SLAM [159]. 
> (2) Extract the pitch and energy information from speech and extract duration from paired text and speech data [45, 58, 139–142].

> In some situations, there are no explicit labels available, or explicit labeling usually causes much human effort and cannot cover specific or fine-grained variation information.
> Thus, we can model the variation information implicitly from data, usually with the help of latent variables.
> From the perspective of latent variables, typical implicit modeling methods include: 
> - Reference encoder [1–3, 8, 9, 20, 116, 117], which extracts latent information from reference input as the latent variables.
> Skerry-Ryan et al. [2] define the prosody as the variation in speech signals that remains after removing variation due to text content, speaker timbre, and channel effects, and model prosody through a reference encoder, which does not require explicit annotations.
> Specifically, it extracts prosody embeddings from reference audio and uses it as the input of the decoder.
> During training, ground-truth reference audio is used, and during inference, another refer audio is used to synthesize speech with similar prosody.
> Wang et al. [1] extract embeddings from reference audio and use them as the query to attend (through Q/K/V based attention [160]) a bank of style tokens, and the attention results are used as the prosody condition of TTS models for expressive speech synthesis.
> The style tokens can increase the capacity and variation of TTS models to learn different kinds of styles, and enable knowledge sharing across data samples in the dataset.
> Each token in the style token bank can learn different prosody representations, such as different speaking rates and emotions.
> During inference, it can use reference audio to attend and extract prosody representations, or simply pick one or some style tokens to synthesize speech. 
> - Variational autoencoder [4, 5, 10, 12, 14–16, 143] is a typical latent variable model.
> Zhang et al. [10] leverage VAE to model the variance information in the latent space with Gaussian prior as a regularization, which can enable expressive modeling and control on synthesized styles.
> Some works [5, 12, 143, 161, 162] also leverage the VAE framework to better model the variance information for expressive synthesis. 
> - Advanced generative models [6, 7, 11, 13, 17, 18, 20, 59].
> One way to alleviate the one-to-many mapping problem and combat over-smoothing prediction is to use advanced generative models (such as GAN, Flow, and Diffusion introduced in Sect. 3.3), which introduce latent variables (z in GAN/Flow,. xtin Diffusion) to implicitly learn the variation information and model the multi-modal distribution.

### 8.2.2.Modeling in Different Granularities

> Variation information can be modeled in different granularities.
> We describe this information from coarse-grained to fine-grained levels: 
> - Language level and speaker level [39, 136, 137], where multilingual and multi-speaker TTS systems use language ID or speaker ID to differentiate languages and speakers. 
> - Paragraph level [144–146], where a TTS model needs to consider the connections between utterances/sentences for long-form reading. 
> - Utterance level [1–3, 33, 116, 134], where a single hidden vector is extracted from the reference speech to represent the timber/prosody of this utterance. 
> - Word/syllable level [16, 149–151], which can model the fine-grained style/prosody information that cannot be covered by utterance level information. 
> - Character/phoneme level [3, 15, 16, 150, 152–154], such as duration, pitch or prosody information. 
> - Frame level [8, 19, 140, 152], the most fine-grained information. 
>
> Some corresponding works on different granularities can be found in Table 8.1. 

> Furthermore, modeling the variance information with a hierarchical structure that covers different granularities is helpful for expressive synthesis.
> Suni et al. [163] demonstrate that hierarchical structures of prosody intrinsically exist in spoken languages.
> Kenter et al. [140] predict prosody features from the frame and phoneme levels to syllable level, and concatenate with the word- and sentence-level features.
> Hono et al. [149] leverage a multi-grained VAE to obtain different time-resolution latent variables and sample finer-level latent variables from coarser-level ones (e.g., from utterance level to phrase level and then to word level).
> Sun et al. [16] use VAE  to model variance information on both phoneme and word levels and combine them together to feed into the decoder.
> Chien and Lee [150] study on prosody prediction and propose a hierarchical structure from the word to phoneme level to improve the prosody prediction.

## 8.3.Modeling Variation Information for Controllable Synthesis

> In this subsection, we introduce technologies on disentangling [5, 20, 21], controlling [22–28], and transferring [29, 30, 164, 165] variation information, as shown in Table 8.2.

### 8.3.1.Disentangling for Control

#### Disentangling with Adversarial Training 

> When multiple styles or prosody information are entangled together, it is necessary to disentangle them during training for better expressive speech synthesis and control.
> Ma et al. [20] enhance the content-style disentanglement ability and controllability with adversarial and collaborative games.
> Hsu et al. [5] leverage the VAE framework with adversarial training to disentangle noise from speaker information.
> Qian et al. [21] propose speechflow to disentangle the rhythm, pitch, content, and timbre using three bottleneck reconstructions.
> Zhang et al. [19] propose to disentangle noise from speakers with frame-level noise modeling and adversarial training. 

#### Disentangling with Semi-Supervised Learning 

> Some attributes used to control the speech include pitch, duration, energy, prosody, emotion, speaker, noise, etc.
> If we have the label for each attribute, we can easily control the synthesized speech, by using the tag as input for model training and using the corresponding tag to control the synthesized speech in inference.
> However, when there is no tag/label available, or only a part is available, how to disentangle and control these attributes is challenging.
> When the partial label is available, [14] propose a semi-supervised learning method to learn the latent of the VAE model, in order to control attributes such as affect or speaking rate.
> When no label is available, [4] propose Gaussian mixture VAE models to disentangle different attributes, and [5, 19] leverage gradient reversal or adversarial training to disentangle speaker timbre from noise in order to synthesize clean speech for noisy speakers.

### 8.3.2.Improving Controllability

> When providing variance information such as style tags as input, the TTS models are supposed to synthesize speech with the corresponding style.
> However, if no constraint is added, the TTS models tend to ignore the variance information and the synthesized speech that does not follow the style.
> To enhance the controllability of the TTS models, some works propose to use cycle consistency or feedback loss to encourage the synthesized speech to contain the variance information in the input.
> Li et al. [35] conduct controllable emotional transfer by adding an emotion style classifier with a feedback cycle, where the classifier encourages the TTS model to synthesize speech with a specific emotion.
> Whitehill et al. [32] use style classifier to provide the feedback loss to encourage the speech synthesis of a given style.
> Meanwhile, it incorporates adversarial learning between different style classifiers to ensure the preservation of different styles from multiple reference audios.
> Liu et al. [31] use ASR to provide the feedback loss to train the unmatched text and speech, which aims to reduce the mismatch between training and inference since randomly chosen audio is used as the reference in inference.
> Other works [33, 34, 164, 165, 168, 169] leverage the feedback loss to ensure the controllability on style and speaker embeddings, etc.

### 8.3.3.Transferring with Control

> We can transfer the style of synthesized speech by changing the variation information to different styles.
> If the variation information is provided in the labeled tag, we can use the speech and the corresponding tag in training, and transfer the style with corresponding tags in inference [35, 39, 136, 137].
> Alternatively, if we do not have a labeled tag for the variation information, we can get the variation information from speech during training, no matter through explicit or implicit modeling as introduced above: Pitch, duration, and energy can be explicitly extracted from speech, and some latent representations can be implicitly extracted by reference encoder or VAE.
> In this way, in order to achieve style transfer in inference, we can obtain the variation information in three ways: (1) extracting from reference speech [1–3, 8, 10, 116, 164, 165]; (2) predicting from text [3, 15, 45, 58, 134, 153, 167]; (3) obtaining by sampling from the latent space [1, 4, 10].

## References

1. Wang Y, Stanton D, Zhang Y, Skerry-Ryan R, Battenberg E, Shor J, Xiao Y, Jia Y, Ren F, 
Saurous RA (2018) Style tokens: unsupervised style modeling, control and transfer in end-to-end speech synthesis. In: International conference on machine learning (PMLR), pp 5180– 
5189 
2. Skerry-Ryan R, Battenberg E, Xiao Y, Wang Y, Stanton D, Shor J, Weiss R, Clark R, 
Saurous RA (2018) Towards end-to-end prosody transfer for expressive speech synthesis with 
Tacotron. In: International conference on machine learning (PMLR), pp 4693–4702 
3. Chen M, Tan X, Li B, Liu Y, Qin T, sheng zhao, Liu TY (2021) AdaSpeech: adaptive text to speech for custom voice. In: International conference on learning representations. https:// openreview.net/forum?id=Drynvt7gg4L 
4. Hsu WN, Zhang Y, Weiss RJ, Zen H, Wu Y, Wang Y, Cao Y, Jia Y, Chen Z, Shen J et al 
(2018) Hierarchical generative modeling for controllable speech synthesis. In: International conference on learning representations
5. Hsu WN, Zhang Y, Weiss RJ, Chung YA, Wang Y, Wu Y, Glass J (2019) Disentangling correlated speaker and noise for speech synthesis via data augmentation and adversarial factorization. In: ICASSP 2019-2019 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 5901–5905 
6. Lee SH, Yoon HW, Noh HR, Kim JH, Lee SW (2020) Multi-SpectroGAN: high-diversity and high-fidelity spectrogram generation with adversarial style combination for speech synthesis. 
Preprint. arXiv:2012.07267 
7. Lee S-G, Kim H, Shin C, Tan X, Liu C, Meng Q, Qin T, Chen W, Yoon S, Liu TY (2021) 
PriorGrad: improving conditional denoising diffusion models with data-driven adaptive prior. 
Preprint. arXiv:2106.06406 
8. Choi S, Han S, Kim D, Ha S (2020) Attentron: Few-shot text-to-speech utilizing attention-based variable-length embedding. In: Proceedigs of the Interspeech 2020, pp 2007–2011 
9. Gururani S, Gupta K, Shah D, Shakeri Z, Pinto J (2019) Prosody transfer in neural text to speech using global pitch and loudness features. Preprint, arXiv:1911.09645 
10. Zhang YJ, Pan S, He L, Ling ZH (2019) Learning latent representations for style control and transfer in end-to-end speech synthesis. In: ICASSP 2019-2019 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 6945–6949 
11. Kim J, Kim S, Kong J, Yoon S (2020) Glow-TTS: a generative flow for text-to-speech via monotonic alignment search. Adv Neural Inf Process Syst 33 
12. Akuzawa K, Iwasawa Y, Matsuo Y (2018) Expressive speech synthesis via modeling expressions with variational autoencoder. In: Proceedings of the Interspeech 2018, pp 3067– 
3071 
13. Valle R, Shih K, Prenger R, Catanzaro B (2020) Flowtron: an autoregressive flow-based generative network for text-to-speech synthesis. Preprint. arXiv:2005.05957 
14. Habib R, Mariooryad S, Shannon M, Battenberg E, Skerry-Ryan R, Stanton D, Kao D, 
Bagby T (2019) Semi-supervised generative modeling for controllable speech synthesis. In: 
International conference on learning representations 
15. Sun G, Zhang Y, Weiss RJ, Cao Y, Zen H, Rosenberg A, Ramabhadran B, Wu Y (2020) 
Generating diverse and natural text-to-speech samples using a quantized fine-grained VAE and autoregressive prosody prior. In: ICASSP 2020-2020 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 6699–6703 
16. Sun G, Zhang Y, Weiss RJ, Cao Y, Zen H, Wu Y (2020) Fully-hierarchical fine-grained prosody modeling for interpretable speech synthesis. In: ICASSP 2020-2020 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 6264–6268 
17. Du C, Yu K (2021) Mixture density network for phone-level prosody modelling in speech synthesis. Preprint. arXiv:2102.00851 
18. Jeong M, Kim H, Cheon SJ, Choi BJ, Kim NS (2021) Diff-TTS: a denoising diffusion model for text-to-speech. Preprint. arXiv:2104.01409 
19. Zhang C, Ren Y, Tan X, Liu J, Zhang K, Qin T, Zhao S, Liu TY (2021) DenoiSpeech: denoising text to speech with frame-level noise modeling. In: 2021 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE 
20. Ma S, Mcduff D, Song Y (2018) Neural TTS stylization with adversarial and collaborative games. In: International conference on learning representations 
21. Qian K, Zhang Y, Chang S, Hasegawa-Johnson M, Cox D (2020) Unsupervised speech decomposition via triple information bottleneck. In: International conference on machine learning (PMLR), pp 7836–7846 
22. Um S-Y, Oh S, Byun K, Jang I, Ahn C, Kang HG (2020) Emotional speech synthesis with rich and granularized control. In: ICASSP 2020-2020 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 7254–7258 
23. Lee K, Park K, Kim D (2021) Styler: style modeling with rapidity and robustness via speech decomposition for expressive and controllable neural text to speech. Preprint, arXiv:2103.09474 
24. Neekhara P, Hussain S, Dubnov S, Koushanfar F, McAuley J (2021) Expressive neural voice cloning. Preprint. arXiv:2102.00151
25. Bae J-S, Bae H, Joo YS, Lee J, Lee GH, Cho HY (2020) Speaking speed control of end-to-end speech synthesis using sentence-level conditioning. In: Proceedings of the Interspeech 2020, pp 4402–4406 
26. Polyak A, Adi Y, Copet J, Kharitonov E, Lakhotia K, Hsu WN, Mohamed A, Dupoux 
E (2021) Speech resynthesis from discrete disentangled self-supervised representations. 
Preprint. arXiv:2104.00355 
27. Tits N, Haddad KE, Dutoit T (2021) Analysis and assessment of controllability of an expressive deep learning-based TTS system. Preprint. arXiv:2103.04097 
28. Li X, Song C, Li J, Wu Z, Jia J, Meng H (2021) Towards multi-scale style control for expressive speech synthesis. Preprint. arXiv:2104.03521 
29. Karlapati S, Moinet A, Joly A, Klimkov V, Sáez-Trigueros D, Drugman T (2020) Copycat: many-to-many fine-grained prosody transfer for neural text-to-speech. In: Proceedings of the 
Interspeech 2020, pp 4387–4391 
30. Inoue K, Hara S, Abe M, Hojo N, Ijima Y (2021) Model architectures to extrapolate emotional expressions in DNN-based text-to-speech. Speech Commun 126:35–43 
31. Liu DR, Yang CY, Wu SL, Lee HY (2018) Improving unsupervised style transfer in end-to-end speech synthesis with end-to-end speech recognition. In: 2018 IEEE spoken language technology workshop (SLT). IEEE, pp 640–647 
32. Whitehill M, Ma S, McDuff D, Song Y (2020) Multi-reference neural TTS stylization with adversarial cycle consistency. In: Proceedings of the Interspeech 2020, pp 4442–4446 
33. Liu R, Sisman B, Gao G, Li H (2020) Expressive TTS training with frame and style reconstruction loss. Preprint. arXiv:2008.01490 
34. Cai Z, Zhang C, Li M (2020) From speaker verification to multispeaker speech synthesis, deep transfer with feedback constraint. In: Proceedings of the Interspeech 2020, pp 3974–3978 
35. Li T, Yang S, Xue L, Xie L (2021) Controllable emotion transfer for end-to-end speech synthesis. In: 2021 12th international symposium on chinese spoken language processing 
(ISCSLP). IEEE, pp 1–5 
36. Wang Y, Skerry-Ryan R, Stanton D, Wu Y, Weiss RJ, Jaitly N, Yang Z, Xiao Y, Chen Z, 
Bengio S et al (2017) Tacotron: towards end-to-end speech synthesis. In: Proceedings of the 
Interspeech 2017, pp 4006–4010 
37. Tachibana H, Uenoyama K, Aihara S (2018) Efficiently trainable text-to-speech system based on deep convolutional networks with guided attention. In: 2018 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 4784–4788 
38. He M, Deng Y, He L (2019) Robust sequence-to-sequence acoustic modeling with stepwise monotonic attention for neural TTS. In: Proceedings of the Interspeech 2019, pp 1293–1297 
39. Chen M, Tan X, Ren Y, Xu J, Sun H, Zhao S, Qin T (2020) MultiSpeech: multi-speaker text to speech with Transformer. In: INTERSPEECH, pp 4024–4028 
40. Sotelo J, Mehri S, Kumar K, Santos JF, Kastner K, Courville AC, Bengio Y (2017) Char2wav: end-to-end speech synthesis. In: 5th international conference on learning representations, 
ICLR 2017, Toulon, France, April 24–26, 2017, Workshop Track Proceedings. OpenReview.net. https://openreview.net/forum?id=B1VWyySKx 
41. Shen J, Pang R, Weiss RJ, Schuster M, Jaitly N, Yang Z, Chen Z, Zhang Y, Wang 
Y, Skerry-Ryan R et al (2018) Natural TTS synthesis by conditioning WaveNet on mel spectrogram predictions. In: 2018 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 4779–4783 
42. Zhang JX, Ling ZH, Dai LR (2018) Forward attention in sequence-to-sequence acoustic modeling for speech synthesis. In: 2018 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 4789–4793 
43. Ping W, Peng K, Gibiansky A, Arik SO, Kannan A, Narang S, Raiman J, Miller J (2018) Deep voice 3: 2000-speaker neural text-to-speech. In: Proceedings of the international conference on learning representations, pp 214–217 
44. Peng K, Ping W, Song Z, Zhao K (2020) Non-autoregressive neural text-to-speech. In: 
International conference on machine learning (PMLR), pp 7586–7598
45. Ren Y, Ruan Y, Tan X, Qin T, Zhao S, Zhao Z, Liu TY (2019) FastSpeech: fast, robust and controllable text to speech. In: NeurIPS 
46. Yu C, Lu H, Hu N, Yu M, Weng C, Xu K, Liu P, Tuo D, Kang S, Lei G et al (2020) DurIAN: duration informed attention network for speech synthesis. In: Proceedings of the Interspeech 
2020, pp 2027–2031 
47. Donahue J, Dieleman S, Bi´nkowski M, Elsen E, Simonyan K (2021) End-to-end adversarial text-to-speech. In: International conference on learning representations 
48. Li N, Liu Y, Wu Y, Liu S, Zhao S, Liu M (2020) RobuTrans: a robust Transformer-based text-to-speech model. In: Proceedings of the AAAI conference on artificial intelligence, vol 34, pp 8228–8235 
49. Beliaev S, Rebryk Y, Ginsburg B (2020) TalkNet: fully-convolutional non-autoregressive speech synthesis model. Preprint. arXiv:2005.05514 
50. Vainer J, Dušek O (2020) SpeedySpeech: efficient neural speech synthesis. In: Proceedings of the Interspeech 2020, pp 3575–3579 
51. Zeng Z, Wang J, Cheng N, Xia T, Xiao J (2020) AlignTTS: efficient feed-forward text-to-speech system without explicit alignment. In: ICASSP 2020-2020 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 6714–6718 
52. Elias I, Zen H, Shen J, Zhang Y, Ye J, Skerry-Ryan R, Wu Y (2021) Parallel Tacotron 
2: a non-autoregressive neural TTS model with differentiable duration modeling. Preprint. arXiv:2103.14574 
53. Shen J, Jia Y, Chrzanowski M, Zhang Y, Elias I, Zen H, Wu Y (2020) Non-attentive Tacotron: robust and controllable neural TTS synthesis including unsupervised duration modeling. 
Preprint. arXiv:2010.04301 
54. Guo H, Soong FK, He L, Xie L (2019) A new GAN-based end-to-end TTS training algorithm. 
In: Proceedings of the Interspeech 2019, pp 1288–1292 
55. Liu R, Yang J, Liu M (2019) A new end-to-end long-time speech synthesis system based on Tacotron2. In: Proceedings of the 2019 international symposium on signal processing systems, pp 46–50 
56. Liu R, Sisman B, Li J, Bao F, Gao G, Li H (2020) Teacher-student training for robust 
Tacotron-based TTS. In: ICASSP 2020-2020 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 6274–6278 
57. Ren Y, Tan X, Qin T, Zhao S, Zhao Z, Liu TY (2019) Almost unsupervised text to speech and automatic speech recognition. In: International conference on machine learning (PMLR), pp 
5410–5419 
58. Ren Y, Hu C, Tan X, Qin T, Zhao S, Zhao Z, Liu TY (2021) FastSpeech 2: fast and high-quality end-to-end text to speech. In: International conference on learning representations. https://openreview.net/forum?id=piLPYqxtWuA 
59. Miao C, Liang S, Chen M, Ma J, Wang S, Xiao J (2020) Flow-TTS: a non-autoregressive network for text to speech based on flow. In: ICASSP 2020-2020 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 7209–7213 
60. Liu P, Cao Y, Liu S, Hu N, Li G, Weng C, Su D (2021) VARA-TTS: non-autoregressive text-to-speech synthesis based on very deep VAE with residual attention. Preprint. arXiv:2102.06431 
61. van der Oord A, Li Y, Babuschkin I, Simonyan K, Vinyals O, Kavukcuoglu K, Driessche 
G, Lockhart E, Cobo L, Stimberg F et al (2018) Parallel WaveNet: fast high-fidelity speech synthesis. In: International conference on machine learning (PMLR), pp 3918–3926 
62. Prenger R, Valle R, Catanzaro B (2019) WaveGlow: a flow-based generative network for speech synthesis. In: ICASSP 2019-2019 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 3617–3621 
63. Kim S, Lee SG, Song J, Kim J, Yoon S (2019) FloWaveNet: a generative flow for raw audio. 
In: International conference on machine learning (PMLR), pp 3370–3378 
64. Kumar K, Kumar R, de Boissiere T, Gestin L, Teoh WZ, Sotelo J, de Brébisson A, Bengio 
Y, Courville A (2019) MelGAN: generative adversarial networks for conditional waveform synthesis. In: NeurIPS
65. Kong J, Kim J, Bae J (2020) HiFi-GAN: generative adversarial networks for efficient and high fidelity speech synthesis. Adv Neural Inf Process Syst 33 
66. Kong Z, Ping W, Huang J, Zhao K, Catanzaro B (2021) DiffWave: a versatile diffusion model for audio synthesis. In: International conference on learning representations 
67. Chen N, Zhang Y, Zen H, Weiss RJ, Norouzi M, Chan W (2021) WaveGrad: estimating gradients for waveform generation. In: International conference on learning representations 
68. Yamamoto R, Song E, Kim JM (2020) Parallel WaveGAN: a fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram. In: 
ICASSP 2020-2020 IEEE international conference on acoustics, speech and signal processing 
(ICASSP). IEEE, pp 6199–6203 
69. Kim J, Kong J, Son J (2021) Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech. Preprint. arXiv:2106.06103 
70. Kalchbrenner N, Elsen E, Simonyan K, Noury S, Casagrande N, Lockhart E, Stimberg F, van den Oord A, Dieleman S, Kavukcuoglu K (2018) Efficient neural audio synthesis. In: 
International conference on machine learning (PMLR), pp 2410–2419 
71. Luo R, Tan X, Wang R, Qin T, Li J, Zhao S, Chen E, Liu TY (2021) LightSpeech: lightweight and fast text to speech with neural architecture search. In: 2021 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE 
72. Zhai B, Gao T, Xue F, Rothchild D, Wu B, Gonzalez JE, Keutzer K (2020) SqueezeWave: extremely lightweight vocoders for on-device speech synthesis. Preprint. arXiv:2001.05685 
73. Kanagawa H, Ijima Y (2020) Lightweight LPCNet-based neural vocoder with tensor decomposition. In: Proceedings of the Interspeech 2020, pp 205–209 
74. Hsu Pc, Lee Hy (2020) WG-WaveNet: real-time high-fidelity speech synthesis without gpu. 
In: Proceedings of the Interspeech 2020, pp 210–214 
75. Huang Z, Li H, Lei M (2020) DeviceTTS: a small-footprint, fast, stable network for on-device text-to-speech. Preprint. arXiv:2010.15311 
76. Zeng Z, Wang J, Cheng N, Xiao J (2021) Lvcnet: efficient condition-dependent modeling network for waveform generation. Preprint. arXiv:2102.10815 
77. Valin JM, Skoglund J (2019) LPCNet: improving neural speech synthesis through linear prediction. In: ICASSP 2019-2019 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 5891–5895 
78. Yang G, Yang S, Liu K, Fang P, Chen W, Xie L (2020) Multi-band MelGAN: faster waveform generation for high-quality text-to-speech. Preprint. arXiv:2005.05106 
79. Okamoto T, Tachibana K, Toda T, Shiga Y, Kawai H (2018) An investigation of subband 
WaveNet vocoder covering entire audible frequency range with limited acoustic features. In: 
2018 IEEE international conference on acoustics, speech and signal processing (ICASSP). 
IEEE, pp 5654–5658 
80. Cui Y, Wang X, He L, Soong FK (2020) An efficient subband linear prediction for LPCNet-based neural synthesis. In: INTERSPEECH, pp 3555–3559 
81. Jin Z, Finkelstein A, Mysore GJ, Lu J (2018) FFTNet: a real-time speaker-dependent neural vocoder. In: 2018 IEEE international conference on acoustics, speech and signal processing 
(ICASSP). IEEE, pp 2251–2255 
82. Ellinas N, Vamvoukakis G, Markopoulos K, Chalamandaris A, Maniati G, Kakoulidis P, 
Raptis S, Sung JS, Park H, Tsiakoulis P (2020) High quality streaming speech synthesis with low, sentence-length-independent latency. In: Proceedings of the Interspeech 2020, pp 2022– 
2026 
83. Ma M, Zheng B, Liu K, Zheng R, Liu H, Peng K, Church K, Huang L (2020) Incremental text-to-speech synthesis with prefix-to-prefix framework. In: Proceedings of the 2020 conference on empirical methods in natural language processing: findings, pp 3886–3896 
84. Stephenson B, Besacier L, Girin L, Hueber T (2020) What the future brings: investigating the impact of lookahead for incremental neural TTS. In: Proceedings of the Interspeech 2020, pp 
215–219
85. Yanagita T, Sakti S, Nakamura S (2019) Neural iTTS: toward synthesizing speech in real-time with end-to-end neural text-to-speech framework. In: Proceedings of the 10th ISCA speech synthesis workshop, pp 183–188 
86. Chung YA, Wang Y, Hsu WN, Zhang Y, Skerry-Ryan R (2019) Semi-supervised training for improving data efficiency in end-to-end speech synthesis. In: ICASSP 2019-2019 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 
6940–6944 
87. Wang P, Qian Y, Soong FK, He L, Zhao H (2015) Word embedding for recurrent neural network based TTS synthesis. In: 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 4879–4883 
88. Zhang M, Wang X, Fang F, Li H, Yamagishi J (2019) Joint training framework for text-to-speech and voice conversion using multi-source Tacotron and WaveNet. In: Proceedings of the Interspeech 2019, pp 1298–1302 
89. Fang W, Chung YA, Glass J (2019) Towards transfer learning for end-to-end speech synthesis from deep pre-trained language models. Preprint. arXiv:1906.07307 
90. Jia Y, Zen H, Shen J, Zhang Y, Wu Y (2021) PnG BERT: augmented BERT on phonemes and graphemes for neural TTS. Preprint. arXiv:2103.15060 
91. Tjandra A, Sisman B, Zhang M, Sakti S, Li H, Nakamura S (2019) VQVAE unsupervised unit discovery and multi-scale Code2Spec inverter for zerospeech challenge 2019. In: Proceedings of the Interspeech 2019, pp 1118–1122 
92. Liu AH, Tu T, Lee Hy, Lee L-S (2020) Towards unsupervised speech recognition and synthesis with quantized speech representation learning. In: ICASSP 2020-2020 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 
7259–7263 
93. Tu T, Chen YJ, Liu AH, Lee Hy (2020) Semi-supervised learning for multi-speaker text-to-speech synthesis using discrete speech representation. In: Proceedings of the Interspeech 
2020, pp 3191–3195 
94. Dunbar E, Algayres R, Karadayi J, Bernard M, Benjumea J, Cao XN, Miskic L, Dugrain C, 
Ondel L, Black AW et al (2019) The zero resource speech challenge 2019: TTS without T. 
In: Proceedings of the Interspeech 2019, pp 1088–1092 
95. Chen L, Deng Y, Wang X, Soong FK, He L (2021) Speech BERT embedding for improving prosody in neural TTS. In: ICASSP 2021-2021 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 6563–6567 
96. Xu J, Tan X, Ren Y, Qin T, Li J, Zhao S, Liu TY (2020) LRSpeech: extremely low-resource speech synthesis and recognition. In: Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery and data mining, pp 2802–2812 
97. Chen Y-J, Tu T, Yeh C-C, Lee H-Y (2019) End-to-end text-to-speech for low-resource languages by cross-lingual transfer learning. In: Proceedings of the Interspeech 2019, pp 
2075–2079 
98. Azizah K, Adriani M, Jatmiko W (2020) Hierarchical transfer learning for multilingual, multi-speaker, and style transfer DNN-based TTS on low-resource languages. IEEE Access 
8:179798–179812 
99. de Korte M, Kim J, Klabbers E (2020) Efficient neural speech synthesis for low-resource languages through multilingual modeling. In: Proceedings of the Interspeech 2020, pp 2967– 
2971 
100. Yang J, He L (2020) Towards universal text-to-speech. In: INTERSPEECH, pp 3171–3175 
101. Prajwal K, Jawahar C (2021) Data-efficient training strategies for neural TTS systems. In: 8th 
ACM IKDD CODS and 26th COMAD, pp 223–227 
102. He M, Yang J, He L (2021) Multilingual Byte2Speech text-to-speech models are few-shot spoken language learners. Preprint. arXiv:2103.03541 
103. Luong HT, Wang X, Yamagishi J, Nishizawa N (2019) Training multi-speaker neural text-to-speech systems using speaker-imbalanced speech corpora. In: Proceedings of the Interspeech 
2019, pp 1303–1307
104. Huybrechts G, Merritt T, Comini G, Perz B, Shah R, Lorenzo-Trueba J (2020) Low-resource expressive text-to-speech using data augmentation. Preprint. arXiv:2011.05707 
105. Dai D, Chen L, Wang Y, Wang M, Xia R, Song X, Wu Z, Wang Y (2020) Noise robust 
TTS for low resource speakers using pre-trained model and speech enhancement. Preprint. arXiv:2005.12531 
106. Tjandra A, Sakti S, Nakamura S (2017) Listening while speaking: Speech chain by deep learning. In: 2017 IEEE automatic speech recognition and understanding workshop (ASRU). 
IEEE, pp 301–308 
107. Tjandra A, Sakti S, Nakamura S (2018) Machine speech chain with one-shot speaker adaptation. In: Proceedings of the Interspeech 2018, pp 887–891 
108. Cooper EL (2019) Text-to-speech synthesis using found data for low-resource languages. 
Ph.D. Thesis, Columbia University 
109. Hu Q, Marchi E, Winarsky D, Stylianou Y, Naik D, Kajarekar S (2019) Neural text-to-speech adaptation from low quality public recordings. In: Speech Synthesis Workshop, vol 10 
110. Cooper E, Wang X, Zhao Y, Yasuda Y, Yamagishi J (2020) Pretraining strategies, waveform model choice, and acoustic configurations for multi-speaker end-to-end speech synthesis. 
Preprint. arXiv:2011.04839 
111. Yan Y, Tan X, Li B, Zhang G, Qin T, Zhao S, Shen Y, Zhang WQ, Liu TY (2021) AdaSpeech 
3: adaptive text to speech for spontaneous style. In: INTERSPEECH 
112. Cooper E, Lai CI, Yasuda Y, Yamagishi J (2020) Can speaker augmentation improve multi-speaker end-to-end TTS? In: Proceedings of the Interspeech 2020, pp 3979–3983 
113. Paul D, Shifas MP, Pantazis Y, Stylianou Y (2020) Enhancing speech intelligibility in text-to-speech synthesis using speaking style conversion. In: Proceedings of the Interspeech 2020, pp 1361–1365 
114. Hu Q, Bleisch T, Petkov P, Raitio T, Marchi E, Lakshminarasimhan V (2021) Whispered and lombard neural speech synthesis. In: 2021 IEEE spoken language technology workshop 
(SLT). IEEE, pp 454–461 
115. Chen M, Chen M, Liang S, Ma J, Chen L, Wang S, Xiao J (2019) Cross-lingual, multi-speaker text-to-speech synthesis using neural speaker embedding. In: Proceedings of the Interspeech 
2019, pp 2105–2109 
116. Jia Y, Zhang Y, Weiss RJ, Wang Q, Shen J, Ren F, Chen Z, Nguyen P, Pang R, Moreno IL et al 
(2018) Transfer learning from speaker verification to multispeaker text-to-speech synthesis. 
In: Proceedings of the 32nd international conference on neural information processing systems, pp 4485–4495 
117. Arık SÖ, Chen J, Peng K, Ping W, Zhou Y (2018) Neural voice cloning with a few samples. In: 
Proceedings of the 32nd international conference on neural information processing systems, pp 10040–10050 
118. Kons Z, Shechtman S, Sorin A, Rabinovitz C, Hoory R (2019) High quality, lightweight and adaptable TTS using LPCNet. In: Proceedings of the Interspeech 2019, pp 176–180 
119. Zhang Z, Tian Q, Lu H, Chen LH, Liu S (2020) AdaDurIAN: few-shot adaptation for neural text-to-speech with durian. Preprint. arXiv:2005.05642 
120. Chen Y, Assael Y, Shillingford B, Budden D, Reed S, Zen H, Wang Q, Cobo LC, Trask A, 
Laurie B et al (2018) Sample efficient adaptive text-to-speech. In: International conference on learning representations 
121. Yan Y, Tan X, Li B, Qin T, Zhao S, Shen Y, Liu TY (2021) AdaSpeech 2: adaptive text to speech with untranscribed data. In: 2021 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE 
122. Wu Y, Tan X, Li B, He L, Zhao S, Song R, Qin T, Liu TY (2022) Adaspeech 4: adaptive text to speech in zero-shot scenarios. In: INTERSPEECH 
123. Gazor S, Zhang W (2003) Speech probability distribution. IEEE Signal Process Lett 
10(7):204–207 
124. Usman M, Zubair M, Shiblee M, Rodrigues P, Jaffar S (2018) Probabilistic modeling of speech in spectral domain using maximum likelihood estimation. Symmetry 10(12):750
125. Toda T, Tokuda K (2007) A speech parameter generation algorithm considering global variance for HMM-based speech synthesis. IEICE Trans Inf Syst 90(5):816–824 
126. Takamichi S, Toda T, Black AW, Neubig G, Sakti S, Nakamura S (2016) Postfilters to modify the modulation spectrum for statistical parametric speech synthesis. IEEE/ACM Trans Audio 
Speech Lang Process 24(4):755–767 
127. Hayashi T, Watanabe S, Toda T, Takeda K, Toshniwal S, Livescu K (2019) Pre-trained text embeddings for enhanced text-to-speech synthesis. In: Proc Interspeech 2019, pp 4430–4434 
128. Xiao Y, He L, Ming H, Soong FK (2020) Improving prosody with linguistic and BERT derived features in multi-speaker based Mandarin Chinese neural TTS. In: ICASSP 2020-
2020 IEEE international conference on acoustics, speech and signal processing (ICASSP). 
IEEE, pp 6704–6708 
129. Zhang G, Song K, Tan X, Tan D, Yan Y, Liu Y, Wang G, Zhou W, Qin T, Lee T et al 
(2022) Mixed-phoneme BERT: improving BERT with mixed phoneme and sup-phoneme representations for text to speech. Preprint. arXiv:2203.17190 
130. Gibiansky A, Arik SÖ, Diamos GF, Miller J, Peng K, Ping W, Raiman J, Zhou Y (2017) 
Deep voice 2: multi-speaker neural text-to-speech. In: Proceedings of the neural information processing systems 
131. Moss HB, Aggarwal V, Prateek N, González J, Barra-Chicote R (2020) BOFFIN TTS: few-shot speaker adaptation by Bayesian optimization. In: ICASSP 2020-2020 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 7639–7643 
132. Wagner M, Watson DG (2010) Experimental and theoretical advances in prosody: a review. 
Lang Cogn Process 25(7–9):905–945 
133. Ladd DR (2008) Intonational phonology. Cambridge University Press, Cambridge 
134. Stanton D, Wang Y, Skerry-Ryan R (2018) Predicting expressive speaking style from text in end-to-end speech synthesis. In: 2018 IEEE spoken language technology workshop (SLT). 
IEEE, pp 595–602 
135. Gao Y, Zheng W, Yang Z, Kohler T, Fuegen C, He Q (2020) Interactive text-to-speech via semi-supervised style transfer learning. Preprint arXiv:2002.06758 
136. Zhang Y, Weiss RJ, Zen H, Wu Y, Chen Z, Skerry-Ryan R, Jia Y, Rosenberg A, Ramabhadran 
B (2019) Learning to speak fluently in a foreign language: multilingual speech synthesis and cross-language voice cloning. In: Proceedings of the Interspeech 2019, pp 2080–2084 
137. Nekvinda T, Dušek O (2020) One model, many languages: meta-learning for multilingual text-to-speech. In: Proceedings of the Interspeech 2020, pp 2972–2976 
138. Kim M, Cheon SJ, Choi BJ, Kim JJ, Kim NS (2021) Expressive text-to-speech using style tag. Preprint arXiv:2104.00436 
139. Ła´ncucki A (2020) FastPitch: parallel text-to-speech with pitch prediction. Preprint. arXiv:2006.06873 
140. Kenter T, Wan V, Chan CA, Clark R, Vit J (2019) Chive: varying prosody in speech synthesis with a linguistically driven dynamic hierarchical conditional variational network. 
In: International conference on machine learning (PMLR), pp 3331–3340 
141. Morrison M, Jin Z, Salamon J, Bryan NJ, Mysore GJ (2020) Controllable neural prosody synthesis. In: Proceedings of the Interspeech 2020, pp 4437–4441 
142. Valle R, Li J, Prenger R, Catanzaro B (2020) Mellotron: multispeaker expressive voice synthesis by conditioning on rhythm, pitch and global style tokens. In: ICASSP 2020-2020 
IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 6189–6193 
143. Elias I, Zen H, Shen J, Zhang Y, Jia Y, Weiss R, Wu Y (2020) Parallel Tacotron: non-autoregressive and controllable TTS. Preprint, arXiv:2010.11439 
144. Aubin A, Cervone A, Watts O, King S (2019) Improving speech synthesis with discourse relations. In: INTERSPEECH, pp 4470–4474 
145. Xu G, Song W, Zhang Z, Zhang C, He X, Zhou B (2020) Improving prosody modelling with cross-utterance BERT embeddings for end-to-end speech synthesis. Preprint. arXiv:2011.05161
146. Wang X, Ming H, He L, Soong FK (2020) s-Transformer: segment-transformer for robust neural speech synthesis. Preprint. arXiv:2011.08480 
147. Liu Y, Xu Z, Wang G, Chen K, Li B, Tan X, Li J, He L, Zhao S (2021) DelightfulTTS: the microsoft speech synthesis system for Blizzard challenge 2021. Preprint. arXiv:2110.12612 
148. Liu Y, Xue R, He L, Tan X, Zhao S (2022) DelightfulTTS 2: end-to-end speech synthesis with adversarial vector-quantized auto-encoders. Preprint. arXiv:2207.04646 
149. Hono Y, Tsuboi K, Sawada K, Hashimoto K, Oura K, Nankaku Y, Tokuda K (2020) 
Hierarchical multi-grained generative model for expressive speech synthesis. In: Proceedings of the Interspeech 2020, pp 3441–3445 
150. Chien CM, Lee Hy (2021) Hierarchical prosody modeling for non-autoregressive speech synthesis. In: 2021 IEEE spoken language technology workshop (SLT). IEEE, pp 446–453 
151. Talman A, Suni A, Celikkanat H, Kakouros S, Tiedemann J, Vainio M et al (2019) Predicting prosodic prominence from text with pre-trained contextualized word representations. In: 22nd nordic conference on computational linguistics (NoDaLiDa) proceedings of the conference. 
Linköping University Electronic Press 
152. Lee Y, Kim T (2019) Robust and fine-grained prosody control of end-to-end speech synthesis. 
In: ICASSP 2019-2019 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 5911–5915 
153. Zeng Z, Wang J, Cheng N, Xiao J (2020) Prosody learning mechanism for speech synthesis system without text length limit. In: Proceedings of the Interspeech 2020, pp 4422–4426 
154. Lei Y, Yang S, Xie L (2021) Fine-grained emotion strength transfer, control and prediction for emotional speech synthesis. In: 2021 IEEE spoken language technology workshop (SLT). 
IEEE, pp 423–430 
155. Silverman K, Beckman M, Pitrelli J, Ostendorf M, Wightman C, Price P, Pierrehumbert J, 
Hirschberg J (1992) ToBI: a standard for labeling English prosody. In: Second international conference on spoken language processing 
156. Rosenberg A (2010) AuToBI-a tool for automatic ToBI annotation. In: Eleventh annual conference of the international speech communication association 
157. Taylor P (1998) The Tilt intonation model. In: Fifth international conference on spoken language processing 
158. Hirst D (2001) Automatic analysis of prosody for multilingual speech corpora. In: Improvements in speech synthesis, pp 320–327 
159. Obin N, Beliao J, Veaux C, Lacheret A (2014) SLAM: automatic stylization and labelling of speech melody. In: Speech prosody, p 246 
160. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser Ł, Polosukhin I 
(2017) Attention is all you need. In: Advances in neural information processing systems, pp 
5998–6008 
161. Aggarwal V, Cotescu M, Prateek N, Lorenzo-Trueba J, Barra-Chicote R (2020) Using 
VAEs and normalizing flows for one-shot text-to-speech synthesis of expressive speech. In: 
ICASSP 2020-2020 ieee international conference on acoustics, speech and signal processing 
(ICASSP). IEEE, pp 6179–6183 
162. Tan X, Chen J, Liu H, Cong J, Zhang C, Liu Y, Wang X, Leng Y, Yi Y, He L et al 
(2022) NaturalSpeech: end-to-end text to speech synthesis with human-level quality. Preprint. arXiv:2205.04421 
163. Suni A, Šimko J, Aalto D, Vainio M (2017) Hierarchical representation and estimation of prosody using continuous wavelet transform. Comput Speech Lang 45:123–136 
164. Xue L, Pan S, He L, Xie L, Soong FK (2021) Cycle consistent network for end-to-end style transfer TTS training. Neural Netw 140:223–236 
165. An X, Soong FK, Xie L (2021) Improving performance of seen and unseen speech style transfer in end-to-end neural TTS. Preprint. arXiv:2106.10003 
166. Shechtman S, Fernandez R, Haws D (2021) Supervised and unsupervised approaches for controlling narrow lexical focus in sequence-to-sequence speech synthesis. In: 2021 IEEE spoken language technology workshop (SLT). IEEE, pp 431–437
167. Guo Z, Leng Y, Wu Y, Zhao S, Tan X (2022) PromptTTS: Controllable text-to-speech with text descriptions. Preprint. arXiv:2211.12171 
168. Nachmani E, Polyak A, Taigman Y, Wolf L (2018) Fitting new speakers based on a short untranscribed sample. In: International conference on machine learning (PMLR), pp 3683– 
3691 
169. Shi Y, Bu H, Xu X, Zhang S, Li M (2020) AISHELL-3: A multi-speaker Mandarin TTS corpus and the baselines. Preprint. arXiv:2010.11567