# 12.Beyond TTS Synthesis

## Abstract

> In this chapter, we briefly introduce other speech tasks that are related to TTS and discuss their relationships.
> The closest task to text-to-speech synthesis is **Singing Voice Synthesis (SVS)** [1–5].
> Actually, SVS can be regarded as a subtask of TTS, as SVS shares similar input/output, model pipeline, and methodology with TTS.
> **Voice Conversion (VC)** [6–8] is also closely related to TTS since they both aim to synthesize speech.
> The difference is that TTS synthesizes speech from the text while VC synthesizes speech from another speech.
> Other related tasks include speech enhancement and separation [9], which separate different signals from source speech (e.g., separate clean signal from noise in speech enhancement).

## 12.1 Singing Voice Synthesis

> **Singing Voice Synthesis (SVS)** is similar to text-to-speech synthesis in that they share similar input/output, modeling pipeline, and methodology.
> They both take text or discrete symbols (music score for singing voice synthesis, which can be roughly regarded as text, pitch, and duration as in TTS) as input, and generate mel-spectrogram or waveform as output.
> Singing voice synthesis nearly adopts the same pipeline as in TTS that leverages either cascaded acoustic models and vocoders or fully end-to-end models.
> However, singing voice synthesis has distinctive characteristics that make it challenging to model.
> We introduce the distinctive challenges in singing voice synthesis and some representative models for singing voice synthesis.

### 12.1.1 Challenges in Singing Voice Synthesis

> We list some challenges in SVS that are unique to TTS: 
> 1. Diverse pitch/duration/energy and text/speaker pairs.
> We can roughly factorize the elements in speech and singing voice into different aspects: linguistic content, pitch, duration, energy, and speaker identity.
> As we can see, in TTS, a word or phoneme for a certain speaker usually has a relatively fixed range of pitch, duration, and energy, while in SVS, a word or phoneme for a certain speaker can be combined with different pitch, duration, and energy, which are pre-defined by the music score.
> As a result, the singing voice has a much large space of compositionality in terms of linguistic content, speaker identity, pitch, duration, and energy, which makes singing voice synthesis challenging. 
> 2. Singing voice is mainly to express emotion while speech voice is mainly to express content.
> Thus, singing voice requires more expressive singing skills and also high-fidelity audio to convey the expressiveness.
> As a result, we need to model different singing skills that lack training data, and also model singing voice using a high sampling rate (e.g., 48 kHz), which is challenging.

### 12.1.2 Representative Models for Singing Voice Synthesis

> We just list some works on singing voice synthesis that address the unique challenges in singing voice synthesis: 
> 1. XiaoiceSing [3] collects a large corpus of singing data with over 70 h, which can largely alleviate the diverse pattern distribution. 
> 2. DeepSinger [4] mines a lot of data from the Web.
> Although the mined data is noisy, it is abundant and helpful to cover a wide range of singing patterns. 
> 3. HiFiSinger [5] models singing voice in 48 kHz sampling rate with sophisticated model designs, which improves the expressiveness of synthesized singing voice.
>
> You are encouraged to read more works in the SVS literature [1, 2, 10–14].

## 12.2 Voice Conversion

> **Voice Conversion (VC)** is to convert the speaker identity of a speech utterance from one to another, without modifying the linguistic content.
> In a broad sense, voice conversion can convert any aspect of a speech, not limited to speaker identity.
> Both voice conversion and text-to-speech can be regarded as a subtask of speech synthesis.

### 12.2.1 Brief Overview of Voice Conversion

> Traditional voice conversion adopts an analysis-mapping-reconstruction pipeline [8], which first converts the source speech into acoustic features (analysis), maps the features from the source speaker to the target speaker (mapping), and then reconstructs the speech waveform from the acoustic features of target speaker (reconstruction).
> With the development of neural networks and deep learning, the voice conversion pipeline is more and more neural-based and end-to-end: 
> 1. The acoustic features are simplified, e.g., using mel-spectrograms. 
> 2. The mapping function is implemented with neural networks. 
> 3. The reconstruction is achieved by neural vocoders instead of traditional statistical vocoders. 
> 4. The whole conversion pipeline can be replaced by a fully end-to-end neural model.

> An important taxonomy of voice conversion is based on the training data: parallel data or non-parallel data.
> Since paired training data is usually hard to collect, achieving voice conversion with non-parallel data is attractive and also challenging.
> Thus, we mainly introduce voice conversion methods based on non-parallel data.

### 12.2.2 Representative Methods for Voice Conversion

> There are different modeling pipelines for voice conversion: 
> 1. PPG-VC, where the source speech is first encoded into a sequence of phonetic posterior grams (PPGs) [15] and then converted into target speech [16]; 
> 2. ASR-TTS, where the source speech is first recognized into text using an **Automatic Speech Recognition (ASR)** model, and then synthesized into target speech using a TTS model [17, 18];  
> 3. Auto-Encoder, where the source speech is first encoded into a sequence of hidden representations, and then reconstructed into the original speech [19, 20].
> To ensure the voice can be converted, the hidden representations are disentangled into speaker-dependent and speaker-independent representations.
> In inference, we concatenate the speaker-independent representations of the source speech and the speaker-dependent representations of the target speaker and use them to generate the target speech; 
> 4. GAN-based methods, such as CycleGAN-VC [21] or StarGAN-VC [22].

> The key to voice conversion is to convert the speaker identity while maintaining the linguistic content.
> Thus, disentangling the speaker identity and linguistic content is critical for voice conversion.
> The above methods achieve disentanglement in different ways: 
> 1. Explicit way, including PPG-VC and ASR-TTS, which explicitly obtain the linguistic content from source speech through PPG extraction or text recognition, and then add the target speaker identity when generating target speech. 
> 2. Implicit way, including Auto-Encoder and GAN-based methods, which use some loss constraints such as adversarial loss or cycle-consistency loss to achieve disentanglement on hidden representations and ensure the target speech does not contain source speaker identity information.

## 12.3 Speech Enhancement/Separation

> Speech enhancement and separation [9] can also be regarded as a general form of speech synthesis: generate clean from noisy speech or generate separated speech from mixed speech.
> Speech enhancement can be regarded as a special case of speech separation tasks since speech enhancement separates clean speech and noise from noisy speech. 

> Different from TTS, SVS, and voice conversion, speech enhancement and separation do not generate speech from scratch or modify the speaker identity.
> They just need to separate different signals apart from the source speech.
> Thus, masking methods are widely used in speech enhancement and separation, where the model predicts a mask, which is added or multiplied on the source speech spectrogram to get the separated speech.

## References

1. Nishimura M, Hashimoto K, Oura K, Nankaku Y, Tokuda K (2016) Singing voice synthesis based on deep neural networks. In: Proceedings of the Interspeech, pp 2478–2482 
2. Lee J, Choi HS, Jeon CB, Koo J, Lee K (2019) Adversarially trained end-to-end korean singing voice synthesis system. In: Proceedings of the Interspeech 2019, pp 2588–2592 
3. Lu P, Wu J, Luan J, Tan X, Zhou L (2020) XiaoiceSing: a high-quality and integrated singing voice synthesis system. In: Proceedings of the Interspeech, 2020 pp 1306–1310 
4. Ren Y, Tan X, Qin T, Luan J, Zhao Z, Liu TY (2020) Deepsinger: singing voice synthesis with data mined from the web. In: Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery and data mining, pp 1979–1989 
5. Chen J, Tan X, Luan J, Qin T, Liu TY (2020) HiFiSinger: towards high-fidelity neural singing voice synthesis. Preprint. arXiv:2009.01776 
6. Stylianou Y (2009) Voice transformation: a survey. In: 2009 IEEE international conference on acoustics, speech and signal processing. IEEE, pp 3585–3588 
7. Mohammadi SH, Kain A (2017) An overview of voice conversion systems. Speech Commun 
88:65–82
8. Sisman B, Yamagishi J, King S, Li H (2020) An overview of voice conversion and its challenges: from statistical modeling to deep learning. IEEE/ACM Trans Audio Speech Lang Process 29:132–157 
9. Wang D, Chen J (2018) Supervised speech separation based on deep learning: an overview. IEEE/ACM Trans Audio Speech Lang Process 26(10):1702–1726 
10. Chandna P, Blaauw M, Bonada J, Gómez E (2019) Wgansing: a multi-voice singing voice synthesizer based on the wasserstein-gan. In: 2019 27th European signal processing conference 
(EUSIPCO). IEEE, pp 1–5 
11. Gu Y, Yin X, Rao Y, Wan Y, Tang B, Zhang Y, Chen J, Wang Y, Ma Z (2021) ByteSing: a Chinese singing voice synthesis system using duration allocated encoder-decoder acoustic models and WaveRNN vocoders. In: 2021 12th international symposium on chinese spoken language processing (ISCSLP). IEEE, pp 1–5 
12. Zhang Y, Cong J, Xue H, Xie L, Zhu P, Bi M (2022) VISinger: variational inference with adversarial learning for end-to-end singing voice synthesis. In: ICASSP 2022-2022 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 7237– 
7241 
13. Liu J, Li C, Ren Y, Chen F, Zhao Z (2022) DiffSinger: singing voice synthesis via shallow diffusion mechanism. In: Proceedings of the AAAI conference on artificial intelligence, vol 36, pp 11020–11028 
14. Zhang Z, Zheng Y, Li X, Lu L (2022) WeSinger 2: fully parallel singing voice synthesis via multi-singer conditional adversarial training. Preprint. arXiv:2207.01886 
15. Sun L, Li K, Wang H, Kang S, Meng H (2016) Phonetic posteriorgrams for many-to-one voice conversion without parallel data training. In: 2016 IEEE international conference on multimedia and expo (ICME). IEEE, pp 1–6 
16. Tian X, Chng ES, Li H (2019) A speaker-dependent WaveNet for voice conversion with non-parallel data. In: Proceedings of the Interspeech, pp 201–205 
17. Miyoshi H, Saito Y, Takamichi S, Saruwatari H (2017) Voice conversion using sequence-to-sequence learning of context posterior probabilities. In: Proceedings of the Interspeech 2017, pp 1268–1272 
18. Biadsy F, Weiss RJ, Moreno PJ, Kanvesky D, Jia Y (2019) Parrotron: an end-to-end speech-to-speech conversion model and its applications to hearing-impaired speech and speech separation. In: Proceedings of the Interspeech 2019, pp 4115–4119 
19. Mohammadi SH, Kain A (2014) Voice conversion using deep neural networks with speaker-independent pre-training. In: 2014 IEEE spoken language technology workshop (SLT). IEEE, pp 19–23 
20. Qian K, Zhang Y, Chang S, Yang X, Hasegawa-Johnson M (2019) AutoVC: zero-shot voice style transfer with only autoencoder loss. In: International conference on machine learning 
(PMLR), pp 5210–5219 
21. Kaneko T, Kameoka H (2018) CycleGAN-VC: non-parallel voice conversion using cycle-consistent adversarial networks. In: 2018 26th European signal processing conference 
(EUSIPCO). IEEE, pp 2100–2104 
22. Kameoka H, Kaneko T, Tanaka K, Hojo N (2018) StarGAN-VC: non-parallel many-to-many voice conversion using star generative adversarial networks. In: 2018 IEEE spoken language technology workshop (SLT). IEEE, pp 266–273