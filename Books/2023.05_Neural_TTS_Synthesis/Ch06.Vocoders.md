# 6.Vocoders

## Abstract

> In this chapter, we introduce vocoders, which generate waveforms from acoustic features or directly from linguistic features.
> With the development of TTS, different kinds of vocoders have been adopted, including signal processing-based vocoders [1–3], and neural network-based vocoders [4–8].
> We first view vocoders from a historic perspective, and then introduce the vocoders in neural TTS, mainly from the perspective of different deep generative models used. 

> **Prerequisite Knowledge for Reading This Chapter** 
> - Language and speech processing, such as waveform, mel-spectrogram. 
> - Model structures of deep neural networks, such as RNN, and CNN. 
> - Deep generative models, such as Autoregressive Models, Normalizing Flows, Variational Auto-Encoders, Denoising Diffusion Probabilistic Models, and Generative Adversarial Networks.

## 6.1.Vocoders from a Historic Perspective

> Roughly speaking, the development of vocoders can be categorized into two stages: the vocoders using signal processing technologies [1], [2], [3], [9], [10], and the vocoders using neural networks [4], [5], [6], [7], [8].

### 6.1.1.Vocoders in Signal Processing

> Some popular vocoders using signal processing include STRAIGHT [1], WORLD [2], and Griffin-Lim algorithm [11].
> We take the WORLD vocoder as an example, which consists of vocoder analysis and vocoder synthesis steps.
> In vocoder analysis, it analyzes the speech and gets a spectral envelope, aperiodicity, and F0, which can be further post-processed to extract their lower-dimensional representation such as mel-cepstrum [12] and band aperiodicity [13, 14] to be modeled by acoustic models.
> In vocoder synthesis, it generates speech waveforms from these acoustic features.

### 6.1.2.Vocoders in Neural TTS

> Early neural vocoders such as WaveNet [4], Parallel WaveNet [21], WaveRNN [6] directly take linguistic features as input and generate the waveform.
> Later, [7, 8, 27, 28] take mel-spectrograms as input and generate the waveform.
> Since speech waveform is very long, autoregressive waveform generation takes much inference time.
> Thus, other deep generative models (as introduced in Sect. 3.3) such as normalizing flows (Flow) [38–40], generative adversarial networks (GAN) [41], variational auto-encoders (VAE) [42], and denoising diffusion probabilistic model (DDPM or Diffusion for short) [43, 44] are used in waveform generation.
> Accordingly, we divide the neural vocoders into different categories: 
> 1. Autoregressive vocoders, 
> 2. Flow-based vocoders, 
> 3. GAN-based vocoders, 
> 4. VAE-based vocoders,
> 5. Diffusion-based vocoders.
>
> We list some neural vocoders in Table 6.1 and introduce some typical vocoders in the following sections.

## 6.2.Vocoders with Different Generative Models

### 6.2.1.Autoregressive Vocoders (e.g., WaveNet)

> WaveNet [4] is the first neural-based vocoder, which leverages dilated convolution to generate waveform points autoregressively, as shown in Fig. 6.1.
> Unlike the vocoder analysis and synthesis in signal processing [1, 2, 12, 13, 45, 46], WaveNet incorporates almost no prior knowledge about audio signals and purely relies on end-to-end learning.
> The original WaveNet, as well as some following works that leverage WaveNet as vocoder [47, 48], generate speech waveform conditioned on linguistic features, while WaveNet can be easily adapted to condition on linear-spectrograms [48] and mel-spectrograms [49–51].
> Although WaveNet achieves good voice quality, it suffers from slow inference speed.
> Therefore, a lot of works [15, 52, 53] investigate lightweight and fast vocoders.
> SampleRNN [15] leverages a hierarchical recurrent neural network for unconditional waveform generation, and it is further integrated into Char2Wav [5] to generate waveform conditioned on acoustic features.
> Further, WaveRNN [54] is developed for efficient audio synthesis, using a recurrent neural network and leveraging several designs including dual softmax layer, weight pruning, and subscaling techniques to reduce the computation.
> Lorenzo-Trueba et al. [17], Paul et al. [18], and Jiao et al. [55] further improve the robustness and universality of the vocoders.
> LPCNet [16, 56] introduces conventional digital signal processing into neural networks and uses linear prediction coefficients to calculate the next waveform point while leveraging a lightweight RNN to compute the residual.
> LPCNet generates speech waveform conditioned on BFCC (bark-frequency cepstral coefficients) features and can be easily adapted to mel-spectrograms.
> Some following works further improve LPCNet from different perspectives, such as reducing complexity for speedup [57–59], and improving stability for better quality [60].

### 6.2.2.Flow-Based Vocoders (e.g., Parallel WaveNet, WaveGlow)

> Normalizing flows [38–40, 61, 62] are a kind of generative model.
> It transforms a probability density with a sequence of invertible mappings [62].
> Since we can get a standard/normalized probability distribution (e.g., Gaussian) through the sequence of invertible mappings based on the change-of-variables rules, this kind of flow-based generative model is called a normalizing flow.
> During sampling, it generates data from a standard probability distribution through the inverse of these transforms.
> As described in Sect. 3.3.2 and shown in Table 6.2, there are two categories of normalizing flows according to the two different techniques [63]: (1) autoregressive transforms [39] (e.g., inverse autoregressive flow used in Parallel WaveNet [21]), and (2) bipartite transforms (e.g., Glow [40] used in WaveGlow [7], and RealNVP [61] used in FloWaveNet [8]). 

> - Autoregressive transforms, e.g., inverse autoregressive flow (IAF) [39].
> IAF can be regarded as a dual formulation of autoregressive flow (AF) [64, 65].
> The training of AF is parallel while the sampling is sequential.
> In contrast, the sampling in IAF is parallel while the inference for likelihood estimation is sequential.
> Parallel WaveNet [21] leverages probability density distillation to marry the efficient sampling of IAF with the efficient training of AR modeling.
> It uses an autoregressive WaveNet as the teacher network to guide the training of the student network (Parallel WaveNet) to approximate the data likelihood, as shown in Fig. 6.2.
> Similarly, ClariNet [66] uses IAF and teacher distillation and leverages a closed-form KL divergence to simplify and stabilize the distillation process.
> Although Parallel WaveNet and ClariNet can generate speech in parallel, it relies on sophisticated teacher-student training and still requires large computation. 

> - Bipartite transforms, e.g., Glow [40] or RealNVP [61].
> To ensure the transforms are invertible, bipartite transforms leverage the affine coupling layers that ensure the output can be computed from the input and vice versa.
> Some vocoders based on bipartite transforms include WaveGlow [7] and FloWaveNet [8], which achieve high voice quality and fast inference speed.
> An illustration of the affine coupling layer used in WaveGlow is shown in Fig. 6.3, where the .σb(xa; θ) and μb(xa; θ) in the last line of Table 6.2 are implemented with a WaveNet based model structure, denoted as “WN” in Fig. 6.3.

> Both autoregressive and bipartite transforms have their advantages and disadvantages [22]: (1) Autoregressive transforms are more expressive than bipartite transforms by modeling dependency between data distribution x and standard probability distribution z but require teacher distillation that is complicated in training. (2) Bipartite transforms enjoy a much simpler training pipeline, but usually require a larger number of parameters (e.g., deeper layers, larger hidden size) to reach comparable capacities with autoregressive models.
> To combine the advantages of both autoregressive and bipartite transforms, WaveFlow [22] provides a unified view of likelihood-based models for audio data to explicitly trade inference parallelism for model capacity.
> In this way, WaveNet, WaveGlow, and FloWaveNet can be regarded as special cases of WaveFlow.

### 6.2.3.GAN-Based Vocoders (e.g., MelGAN, HiFiGAN)

> **Generative Adversarial Networks (GANs)** [41] have been widely used in data generation tasks, such as image generation [41, 67], text generation [68], and audio generation [24].
> As introduced in Sect. 3.3.6, **GAN** consists of a generator for data generation, and a discriminator to judge the authenticity of the generated data and is optimized with an adversarial loss function, formulated as follows:

$$
    \min_{\theta}\max_{\phi} \mathbb{E}_{x\sim p_{data}}\log D(x;\phi) + \mathbb{E}_{x\sim p_z}\log(1-D(G(z;\theta);\phi)),\tag{6.1}
$$

> where $\theta$ and $\phi$ denote the parameter of generator and discriminator respectively, and $p_{data}$ and $p_z$ denote the true data distribution and standard Gaussian distribution.

> A lot of vocoders leverage **GAN** to ensure the audio generation quality, including WaveGAN [24], GAN-TTS [26], MelGAN [27], Parallel WaveGAN [28], HiFi-GAN [29], and other GAN-based vocoders [69–74].
> We summarize the characteristics according to the generators, discriminators, and losses used in each vocoder in Table 6.3, and introduce them accordingly. 

|GAN|Generator|Discriminator|Loss|
|---|---|---|---|
|WaveGAN [24]|DCGAN [80]|/|WGAN-GP [75] 
|GAN-TTS [26]|/|Random window D |Hinge-loss GAN [76] 
|MelGAN [27]|/|Multi-Scale D|LS-GAN [77] <br>Feature Matching Loss [79] 
|Par.WaveGAN [28] |WaveNet [4]|/|LS-GAN, <br>Multi-STFT Loss 
|HiFi-GAN [29]|Multi-Receptive Field Fusion|Multi-Period D,Multi-Scale D|LS-GAN,<br>STFT Loss, <br>Feature Matching Loss 
|VocGAN [30]|Multi-Scale G|Hierarchical D|LS-GAN, <br>Multi-STFT Loss, <br>Feature Matching Loss 
|GED [31]|/|Random Window D |Hinge-Loss GAN, <br>Repulsive loss

#### Generator

> Most GAN-based vocoders use dilated convolution to increase the receptive field to model the long-dependency in waveform sequence, and transposed convolution to upsample the condition information (e.g., linguistic features or mel-spectrograms) to match the length of waveform sequence.
> Yamamoto et al. [28] choose to upsample the conditional information one time, and then perform dilated convolution to ensure model capacity.
> However, this kind of upsampling increases the sequence length too early, resulting in a larger computation cost.
> Therefore, some vocoders [27, 29] choose to iteratively upsample the condition information and perform dilated convolution, which can avoid too long a sequence in the lower layers.
> Specifically, VocGAN [30] proposes a multi-scale generator that can gradually output waveform sequences at different scales, from coarse-grained to fine-grained.
> HiFi-GAN [29] processes different patterns of various lengths in parallel through a multi-receptive field fusion module and also has the flexibility to trade off between synthesis efficiency and sample quality. 

#### Discriminator

> Some efforts [26, 27, 29, 30] on discriminators focus on how to design models to capture the characteristics of a waveform, in order to provide a better guiding signal for the generators.
> We introduce these efforts as follows: 
> 1. Random window discriminators, proposed in GAN-TTS [26], which use multiple discriminators, where each is feeding with different random windows of waveform with and without conditional information.
> Random window discriminators have several benefits, such as evaluating audios in different complementary ways, simplifying the true/false judgments compared with full audio, and acting as a data augmentation effect, etc. 
> 2. Multi-scale discriminators, proposed in MelGAN [27], which use multiple discriminators to judge audios in different scales (different downsampling ratios compared with original audio).
> The advantage of multi-scale discriminators is that the discriminator in each scale can focus on the characteristics in different frequency ranges. 
> 3. Multi-period discriminators, proposed in HiFi-GAN [29], which leverage multiple discriminators, where each accepts equally spaced samples of input audio with a period.
> Specifically, the 1D waveform sequence with a length of T is reshaped into a 2D data .[p, T /p] where p is the period, and processed by a 2D convolution.
> Multi-period discriminators can capture different implicit structures by looking at different parts of input audio in different periods. 
> 4. Hierarchical discriminators, leveraged in VocGAN [30] to judge the generated waveform in different resolutions from coarse-grained to fine-grained, which can guide the generator to learn the mapping between the acoustic features and waveform in both low and high frequencies. 

#### Loss

> Except for the regular GAN losses such as WGAN-GP [75], hinge-loss GAN [76], and LS-GAN [77], other specific losses such as STFT loss [69, 78] and feature matching loss [79] are also leveraged.
> These additional losses can improve the stability and efficiency of adversarial training [28], and improve the perceptual audio quality.
> Gritsenko et al. [31] propose a generalized energy distance with a repulsive term to better capture the multi-modal waveform distribution.

### 6.2.4.Diffusion-Based Vocoders (e.g., WaveGrad, DiffWave)

> Denoising diffusion probabilistic models (DDPM or Diffusion) [44] are leveraged in vocoders, such as WaveGrad [34], DiffWave [35].
> As introduced in Sect. 3.3.4, the basic idea of the diffusion model is to formulate the mapping between data and latent distributions with a diffusion process and a reverse process, as shown in Fig. 6.4: in the diffusion process, the waveform data sample is gradually added with some random noises and finally becomes Gaussian noise; in the reverse process, the random Gaussian noise is gradually denoised into waveform data sample step by step.
> Diffusion-based vocoders can generate speech with very high voice quality but suffer from slow inference speed due to the long iterative process.
> Thus, a lot of works on diffusion models [36, 37, 81–83] are investigating how to reduce inference time while maintaining generation quality.
> For example, PriorGrad [36] and SpecGrad [37] reduce the number of iterations by providing a more informative prior distribution that is close to the data distribution.
> They have a nice connection between signal processing and neural network-based methods by incorporating signal processing knowledge into the model.
> InferGrad [84] reduces the mismatch between training and inference in the diffusion model (i.e., a large iteration step is used in training while a small iteration step in inference) by taking the inference process into training, which can achieve good voice quality with a small number of iteration steps.

> As shown in Sects. 3.3.4 and 3.3.5, we introduce several kinds of generative models, including denoising diffusion probabilistic models (Diffusion) [43, 44], score matching with Langevin dynamics (SMLG) [85], stochastic differential equation (SDE) [86], and ordinary differential equation (ODE) [86].
> Actually, they have very close connections: 
> 1. Diffusion and SMLG have very similar formulations and only differ in scale in their loss functions [87].
> 2. SDE extends the discrete time step in Diffusion and SMLG to continuous time variables and formulates the diffusion and denoising processes as stochastic differential equations. 
> 3. ODE is a corresponding deterministic process of SDE and has the same marginal probability densities in the diffusion and denoising trajectories.
>
> These generative models have been used in the literature of speech synthesis: DiffWave [35] is based on the formulation of the original diffusion model, while WaveGrad [34] is based on the formulation of both diffusion model and score matching with Langevin dynamics.
> In this chapter, we just use the term “diffusion model” to represent these generative models in speech synthesis.

### 6.2.5.Other Vocoders

> Some works leverage neural-based source-filter model for waveform generation [25, 88–95], aiming to achieve high voice quality while maintaining controllable speech generation.
> Govalkar et al. [96] conduct a comprehensive study on different kinds of vocoders.
> Hsu et al. [97] study the robustness of vocoders by evaluating several common vocoders with comprehensive experiments.

## References

1. Kawahara H (2006) STRAIGHT, exploitation of the other aspect of vocoder: perceptually isomorphic decomposition of speech sounds. Acoust Sci Technol 27(6):349–353 
2. Morise M, Yokomori F, Ozawa K (2016) WORLD: a vocoder-based high-quality speech synthesis system for real-time applications. IEICE Trans Inf Syst 99(7):1877–1884
3. Ai Y, Ling ZH (2020) A neural vocoder with hierarchical generation of amplitude and phase spectra for statistical parametric speech synthesis. IEEE/ACM Trans Audio Speech Lang Process 28:839–851 
4. van den Oord A, Dieleman S, Zen H, Simonyan K, Vinyals O, Graves A, Kalchbrenner N, Senior A, Kavukcuoglu K (2016) WaveNet: a generative model for raw audio. Preprint. arXiv:1609.03499 
5. Sotelo J, Mehri S, Kumar K, Santos JF, Kastner K, Courville A, Bengio Y (2017) Char2wav: end-to-end speech synthesis 
6. Kalchbrenner N, Elsen E, Simonyan K, Noury S, Casagrande N, Lockhart E, Stimberg F, Oord A, Dieleman S, Kavukcuoglu K (2018) Efficient neural audio synthesis. In: International conference on machine learning. PMLR, pp 2410–2419 
7. Prenger R, Valle R, Catanzaro B (2019) WaveGlow: a flow-based generative network for speech synthesis. In: ICASSP 2019-2019 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 3617–3621 
8. Kim S, Lee SG, Song J, Kim J, Yoon S (2019) FloWaveNet: A generative flow for raw audio. In: International conference on machine learning. PMLR, pp 3370–3378 
9. Imai S (1983) Cepstral analysis synthesis on the mel frequency scale. In: ICASSP’83. IEEE international conference on acoustics, speech, and signal processing. IEEE, vol 8, pp 93–96 
10. Yoshimura T, Tokuda K, Masuko T, Kobayashi T, Kitamura T (2001) Mixed excitation for HMM-based speech synthesis. In: Seventh European conference on speech communication and technology 
11. Griffin D, Lim J (1984) Signal estimation from modified short-time Fourier transform. IEEE Trans Acoust Speech Signal Process 32(2):236–243 
12. Fukada T, Tokuda K, Kobayashi T, Imai S (1992) An adaptive algorithm for mel-cepstral analysis of speech. In: Proc. ICASSP, vol 1, pp 137–140 
13. Kawahara H, Masuda-Katsuse I, De Cheveigne A (1999) Restructuring speech representations using a pitch-adaptive time–frequency smoothing and an instantaneous-frequency-based F0 extraction: Possible role of a repetitive structure in sounds. Speech Commun 27(3–4):187–207 
14. Kawahara H, Estill J, Fujimura O (2001) Aperiodicity extraction and control using mixed mode excitation and group delay manipulation for a high quality speech analysis, modification and synthesis system STRAIGHT. In: Second international workshop on models and analysis of vocal emissions for biomedical applications 
15. Mehri S, Kumar K, Gulrajani I, Kumar R, Jain S, Sotelo J, Courville A, Bengio Y (2017) SampleRNN: an unconditional end-to-end neural audio generation model. In: ICLR 
16. Valin JM, Skoglund J (2019) LPCNet: Improving neural speech synthesis through linear prediction. In: ICASSP 2019-2019 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 5891–5895 
17. Lorenzo-Trueba J, Drugman T, Latorre J, Merritt T, Putrycz B, Barra-Chicote R, Moinet A, Aggarwal V (2019) Towards achieving robust universal neural vocoding. In: Proc Interspeech 
2019, pp 181–185 
18. Paul D, Pantazis Y, Stylianou Y (2020) Speaker conditional WaveRNN: towards universal neural vocoder for unseen speaker and recording conditions. In: Proc Interspeech 2020, pp 
235–239 
19. Yu C, Lu H, Hu N, Yu M, Weng C, Xu K, Liu P, Tuo D, Kang S, Lei G, et al (2020) DurIAN: Duration informed attention network for speech synthesis. In: Proc Interspeech 2020, pp 2027– 
2031 
20. Jin Z, Finkelstein A, Mysore GJ, Lu J (2018) FFTNet: A real-time speaker-dependent neural vocoder. In: 2018 IEEE international conference on acoustics, speech and signal processing 
(ICASSP). IEEE, pp 2251–2255 
21. van den Oord A, Li Y, Babuschkin I, Simonyan K, Vinyals O, Kavukcuoglu K, Driessche G, Lockhart E, Cobo L, Stimberg F, et al. (2018) Parallel WaveNet: fast high-fidelity speech synthesis. In: International conference on machine learning. PMLR, pp 3918–3926 
22. Ping W, Peng K, Zhao K, Song Z (2020) WaveFlow: a compact flow-based model for raw audio. In: International conference on machine learning. PMLR, pp 7706–7716
23. Zhai B, Gao T, Xue F, Rothchild D, Wu B, Gonzalez JE, Keutzer K (2020) SqueezeWave: extremely lightweight vocoders for on-device speech synthesis. Preprint. arXiv:200105685 
24. Donahue C, McAuley J, Puckette M (2018) Adversarial audio synthesis. In: International conference on learning representations 
25. Juvela L, Bollepalli B, Yamagishi J, Alku P (2019) GELP: GAN-excited linear prediction for speech synthesis from mel-spectrogram. In: Proc Interspeech 2019, pp 694–698 
26. Bi´nkowski M, Donahue J, Dieleman S, Clark A, Elsen E, Casagrande N, Cobo LC, Simonyan K (2019) High fidelity speech synthesis with adversarial networks. In: International conference on learning representations 
27. Kumar K, Kumar R, de Boissiere T, Gestin L, Teoh WZ, Sotelo J, de Brébisson A, Bengio Y, Courville A (2019) MelGAN: generative adversarial networks for conditional waveform synthesis. In: NeurIPS 
28. Yamamoto R, Song E, Kim JM (2020) Parallel WaveGAN: a fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram. In: ICASSP 2020-
2020 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 6199–6203 
29. Kong J, Kim J, Bae J (2020) HiFi-GAN: generative adversarial networks for efficient and high fidelity speech synthesis. Adv Neural Inf Process Syst 33, 17022 
30. Yang J, Lee J, Kim Y, Cho HY, Kim I (2020) VocGAN: a high-fidelity real-time vocoder with a hierarchically-nested adversarial network. In: Proc Interspeech 2020, pp 200–204 
31. Gritsenko A, Salimans T, van den Berg R, Snoek J, Kalchbrenner N (2020) A spectral energy distance for parallel speech synthesis. Adv Neural Inf Process Syst 33:13062 
32. Kim JH, Lee SH, Lee JH, Lee SW (2021) Fre-GAN: Adversarial frequency-consistent audio synthesis. Preprint. arXiv:2106.02297 
33. Peng K, Ping W, Song Z, Zhao K (2020) Non-autoregressive neural text-to-speech. In: International conference on machine learning. PMLR, pp 7586–7598 
34. Chen N, Zhang Y, Zen H, Weiss RJ, Norouzi M, Chan W (2021) WaveGrad: Estimating gradients for waveform generation. In: ICLR 
35. Kong Z, Ping W, Huang J, Zhao K, Catanzaro B (2021) DiffWave: a versatile diffusion model for audio synthesis. In: ICLR 
36. Lee Sg, Kim H, Shin C, Tan X, Liu C, Meng Q, Qin T, Chen W, Yoon S, Liu TY (2021) PriorGrad: improving conditional denoising diffusion models with data-driven adaptive prior. Preprint. arXiv:2106.06406 
37. Koizumi Y, Zen H, Yatabe K, Chen N, Bacchiani M (2022) SpecGrad: diffusion probabilistic model based neural vocoder with adaptive noise spectral shaping. Preprint. arXiv:2203.16749 
38. Dinh L, Krueger D, Bengio Y (2014) NICE: Non-linear independent components estimation. Preprint. arXiv:14108516 
39. Kingma DP, Salimans T, Jozefowicz R, Chen X, Sutskever I, Welling M (2016) Improved variational inference with inverse autoregressive flow. Adv Neural Inf Process Syst 29:4743– 
4751 
40. Kingma DP, Dhariwal P (2018) Glow: generative flow with invertible 1×1 convolutions. In: Proceedings of the 32nd international conference on neural information processing systems, pp 10236–10245 
41. Goodfellow IJ, Pouget-Abadie J, Mirza M, Xu B, Warde-Farley D, Ozair S, Courville A, Bengio Y (2014) Generative adversarial nets. In: NIPS 
42. Kingma DP, Welling M (2013) Auto-encoding variational bayes. Preprint. arXiv:1312.6114 
43. Sohl-Dickstein J, Weiss E, Maheswaranathan N, Ganguli S (2015) Deep unsupervised learning using nonequilibrium thermodynamics. In: International conference on machine learning. PMLR, pp 2256–2265 
44. Ho J, Jain A, Abbeel P (2020) Denoising diffusion probabilistic models. Preprint. arXiv:2006.11239 
45. Tokuda K, Kobayashi T, Masuko T, Imai S (1994) Mel-generalized cepstral analysis-a unified approach to speech spectral estimation. In: Third international conference on spoken language processing
46. Itakura F (1975) Line spectrum representation of linear predictor coefficients of speech signals. J Acoust Soc Am 57(S1):S35–S35 
47. Arık SÖ, Chrzanowski M, Coates A, Diamos G, Gibiansky A, Kang Y, Li X, Miller J, Ng A, Raiman J, et al (2017) Deep Voice: real-time neural text-to-speech. In: International conference on machine learning. PMLR, pp 195–204 
48. Gibiansky A, Arik SÖ, Diamos GF, Miller J, Peng K, Ping W, Raiman J, Zhou Y (2017) Deep Voice 2: multi-speaker neural text-to-speech. In: NIPS 
49. Tamamori A, Hayashi T, Kobayashi K, Takeda K, Toda T (2017) Speaker-dependent WaveNet vocoder. In: Interspeech, vol 2017, pp 1118–1122 
50. Ping W, Peng K, Gibiansky A, Arik SO, Kannan A, Narang S, Raiman J, Miller J (2018) Deep Voice 3: 2000-speaker neural text-to-speech. In: Proc ICLR, pp 214–217 
51. Shen J, Pang R, Weiss RJ, Schuster M, Jaitly N, Yang Z, Chen Z, Zhang Y, Wang Y, Skerry-Ryan R, et al (2018) Natural TTS synthesis by conditioning WaveNet on mel spectrogram predictions. In: 2018 IEEE international conference on acoustics, speech and signal processing 
(ICASSP). IEEE, pp 4779–4783 
52. Paine TL, Khorrami P, Chang S, Zhang Y, Ramachandran P, Hasegawa-Johnson MA, Huang TS (2016) Fast WaveNet generation algorithm. Preprint. arXiv:1611.09482 
53. Hsu Pc, Lee Hy (2020) WG-WaveNet: real-time high-fidelity speech synthesis without GPU. In: Proc Interspeech 2020, pp 210–214 
54. Zhang ZR, Chu M, Chang E (2002) An efficient way to learn rules for grapheme-to-phoneme conversion in Chinese. In: International symposium on Chinese spoken language processing 
55. Jiao Y, Gabrys A, Tinchev G, Putrycz B, Korzekwa D, Klimkov V (2021) Universal neural vocoding with parallel WaveNet. Preprint. arXiv:2102.01106 
56. Valin JM, Skoglund J (2019) A real-time wideband neural vocoder at 1.6 kb/s using LPCNet. In: Proc Interspeech 2019, pp 3406–3410 
57. Vipperla R, Park S, Choo K, Ishtiaq S, Min K, Bhattacharya S, Mehrotra A, Ramos AGC, Lane ND (2020) Bunched LPCNet: vocoder for low-cost neural text-to-speech systems. In: Proc Interspeech 2020, pp 3565–3569 
58. Popov V, Kudinov M, Sadekova T (2020) Gaussian LPCNet for multisample speech synthesis. In: ICASSP 2020-2020 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 6204–6208 
59. Kanagawa H, Ijima Y (2020) Lightweight LPCNet-based neural vocoder with tensor decomposition. In: Proc Interspeech 2020, pp 205–209 
60. Hwang MJ, Song E, Yamamoto R, Soong F, Kang HG (2020) Improving LPCNet-based text-to-speech with linear prediction-structured mixture density network. In: ICASSP 2020-2020 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 
7219–7223 
61. Dinh L, Sohl-Dickstein J, Bengio S (2016) Density estimation using real NVP. Preprint. arXiv:160508803 
62. Rezende D, Mohamed S (2015) Variational inference with normalizing flows. In: International conference on machine learning. PMLR, pp 1530–1538 
63. Papamakarios G, Nalisnick E, Rezende DJ, Mohamed S, Lakshminarayanan B (2019) Normalizing flows for probabilistic modeling and inference. Preprint. arXiv:1912.02762 
64. Papamakarios G, Pavlakou T, Murray I (2017) Masked autoregressive flow for density estimation. In: Proceedings of the 31st international conference on neural information processing systems, pp 2335–2344 
65. Huang CW, Krueger D, Lacoste A, Courville A (2018) Neural autoregressive flows. In: International conference on machine learning. PMLR, pp 2078–2087 
66. Ping W, Peng K, Chen J (2018) ClariNet: Parallel wave generation in end-to-end text-to-speech. In: International conference on learning representations 
67. Zhu JY, Park T, Isola P, Efros AA (2017) Unpaired image-to-image translation using cycle-consistent adversarial networks. In: Proceedings of the IEEE international conference on computer vision, pp 2223–2232
68. Yu L, Zhang W, Wang J, Yu Y (2017) SeGAN: sequence generative adversarial nets with policy gradient. In: Proceedings of the AAAI conference on artificial intelligence, vol 31 
69. Yamamoto R, Song E, Kim JM (2019) Probability density distillation with generative adversarial networks for high-quality parallel waveform generation. In: Proc Interspeech 2019, pp 699–703 
70. Wu YC, Hayashi T, Okamoto T, Kawai H, Toda T (2020) Quasi-periodic parallel WaveGAN vocoder: a non-autoregressive pitch-dependent dilated convolution model for parametric speech generation. In: Proc Interspeech 2020, pp 3535–3539 
71. Song E, Yamamoto R, Hwang MJ, Kim JS, Kwon O, Kim JM (2021) Improved parallel WaveGAN vocoder with perceptually weighted spectrogram loss. In: 2021 IEEE spoken language technology workshop (SLT). IEEE, pp 470–476 
72. You J, Kim D, Nam G, Hwang G, Chae G (2021) GAN vocoder: multi-resolution discriminator is all you need. Preprint. arXiv:2103.05236 
73. Wang C, Chen Y, Wang B, Shi Y (2021) Improve GAN-based neural vocoder using pointwise relativistic leastsquare GAN. Preprint. arXiv:210314245 
74. Jang W, Lim D, Yoon J (2020) Universal MelGAN: a robust neural vocoder for high-fidelity waveform generation in multiple domains. Preprint. arXiv:2011.09631 
75. Gulrajani I, Ahmed F, Arjovsky M, Dumoulin V, Courville A (2017) Improved training of wasserstein GANs. In: Proceedings of the 31st international conference on neural information processing systems, pp 5769–5779 
76. Lim JH, Ye JC (2017) Geometric GAN. Preprint. arXiv:1705.02894 
77. Mao X, Li Q, Xie H, Lau RY, Wang Z, Paul Smolley S (2017) Least squares generative adversarial networks. In: Proceedings of the IEEE international conference on computer vision, pp 2794–2802 
78. Arık SÖ, Jun H, Diamos G (2018) Fast spectrogram inversion using multi-head convolutional neural networks. IEEE Signal Process Lett 26(1):94–98 
79. Larsen ABL, Sønderby SK, Larochelle H, Winther O (2016) Autoencoding beyond pixels using a learned similarity metric. In: International conference on machine learning. PMLR, pp 1558–1566 
80. Radford A, Metz L, Chintala S (2015) Unsupervised representation learning with deep convolutional generative adversarial networks. Preprint. arXiv:1511.06434 
81. Song J, Meng C, Ermon S (2020) Denoising diffusion implicit models. Preprint. arXiv:201002502 
82. Watson D, Ho J, Norouzi M, Chan W (2021) Learning to efficiently sample from diffusion probabilistic models. Preprint. arXiv:210603802 
83. Kong Z, Ping W (2021) On fast sampling of diffusion probabilistic models. Preprint. arXiv:2106.00132 
84. Chen Z, Tan X, Wang K, Pan S, Mandic D, He L, Zhao S (2022) InferGrad: Improving diffusion models for vocoder by considering inference in training. In: ICASSP 2022-2022 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 8432–8436 
85. Song Y, Ermon S (2019) Generative modeling by estimating gradients of the data distribution. In: Wallach HM, Larochelle H, Beygelzimer A, d’Alché-Buc F, Fox EB, Garnett R (eds) Advances in neural information processing systems 32: Annual conference on neural information processing systems 2019, NeurIPS 2019, December 8–14, 2019, Vancouver, BC, Canada, pp 11895–11907. https://proceedings.neurips.cc/paper/2019/hash/ 
3001ef257407d5a371a96dcd947c7d93-Abstract.html 
86. Song Y, Sohl-Dickstein J, Kingma DP, Kumar A, Ermon S, Poole B (2020) Score-based generative modeling through stochastic differential equations. In: International conference on learning representations 
87. Luo C (2022) Understanding diffusion models: a unified perspective. Preprint. arXiv:2208.11970
88. Wang X, Takaki S, Yamagishi J (2019) Neural source-filter-based waveform model for statistical parametric speech synthesis. In: ICASSP 2019-2019 IEEE International conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 5916–5920 
89. Wang X, Takaki S, Yamagishi J (2019) Neural source-filter waveform models for statistical parametric speech synthesis. IEEE/ACM Trans Audio Speech Lang Process 28:402–415 
90. Wang X, Yamagishi J (2019) Neural harmonic-plus-noise waveform model with trainable maximum voice frequency for text-to-speech synthesis. In: Proc. 10th ISCA Speech Synthesis Workshop, pp 1–6 
91. Liu Z, Chen K, Yu K (2020) Neural homomorphic vocoder. In: Proc Interspeech 2020, pp 
240–244 
92. Juvela L, Bollepalli B, Tsiaras V, Alku P (2019) GlotNet – a raw waveform model for the glottal excitation in statistical parametric speech synthesis. IEEE/ACM Trans Audio Speech Lang Process 27(6):1019–1030 
93. Engel J, Gu C, Roberts A, et al (2019) DDSP: differentiable digital signal processing. In: International conference on learning representations 
94. Song E, Hwang MJ, Yamamoto R, Kim JS, Kwon O, Kim JM (2020) Neural text-to-speech with a modeling-by-generation excitation vocoder. In: Proc Interspeech 2020, pp 3570–3574 
95. Yoneyama R, Wu YC, Toda T (2021) Unified source-filter GAN: unified source-filter network based on factorization of quasi-periodic parallel WaveGAN. Preprint. arXiv:210404668 
96. Govalkar P, Fischer J, Zalkow F, Dittmar C (2019) A comparison of recent neural vocoders for speech signal reconstruction. In: Proc. 10th ISCA speech synthesis workshop, pp 7–12 
97. Hsu Pc, Wang Ch, Liu AT, Lee Hy (2019) Towards robust neural vocoding for speech generation: a survey. Preprint. arXiv:191202461