# 5.Acoustic Models.

In this chapter, we introduce acoustic models, which generate acoustic features from 
linguistic features or directly from phonemes or characters. With the development 
of TTS, different kinds of acoustic models have been adopted, including the early 
hidden Markov models and deep neural networks in statistical parametric speech 
synthesis (SPSS) [1–6], and then the sequence-to-sequence models based on an 
encoder-attention-decoder framework (including RNN, CNN, and Transformer) [7– 
10], and the latest feed-forward networks (CNN or Transformer) [11, 12] and 
advanced generative models (GAN, Flow, VAE, and Diffusion) [13–17].

Prerequisite Knowledge for Reading This Chapter
•Language and speech processing, such as waveform, mel-spectrogram, phoneme, 
pitch, duration.
•Model structures of deep neural networks, such as RNN, CNN, and Transformer.
•Generative models, such as Autoregressive Models, Normalizing Flows, Varia-
tional Auto-Encoders, Denoising Diffusion Probabilistic Models, and Generative 
Adversarial Networks.

## 5.1.Acoustic Models from a Historic Perspective

Acoustic models aim to generate acoustic features, which are further converted into 
waveforms using vocoders. The choice of acoustic features largely determines the 
types of TTS pipelines. Different kinds of acoustic features have been tried, such 
as mel-cepstral coefficients (MCC) [18], mel-generalized coefficients (MGC) [19], 
band aperiodicity (BAP) [20, 21], fundamental frequency (F0), voiced/unvoiced 
(V/UV), bark-frequency cepstral coefficients (BFCC), and the most widely used 
mel-spectrograms. Accordingly, we can divide the acoustic models into two periods: 
(1) acoustic models in SPSS, which typically predict acoustic features such as MGC, 
BAP, and F0 from linguistic features, and (2) acoustic models in neural TTS, which 
predict acoustic features such as mel-spectrograms from phonemes or characters.

### 5.1.1.Acoustic Models in SPSS

In statistical parametric speech synthesis (SPSS) [22, 23], statistical models such as 
HMM [1, 2], DNN [3, 4] or RNN [5, 6] are leveraged to generate acoustic features 
(speech parameters) from linguistic features, where the generated speech parameters 
are converted into speech waveform using a vocoder such as STRAIGHT [24] 
or WORLD [25]. The developments of these acoustic models are driven by 
several requirements: (1) taking more context information as input; (2) modeling 
the correlation between output frames; (3) better combating the over-smoothing 
prediction problem [22] since the mapping from linguistic features to acoustic 
features is one-to-many. We briefly review some works as follows. 
HMM [26] is leveraged to generate speech parameters in [1, 2], where the 
observation vectors of HMM consist of spectral parameter vectors such as mel-
cepstral coefficients (MCC) and F0. Compared to concatenative speech synthesis, 
HMM-based parametric synthesis is more flexible in changing speaker identities, 
emotions, and speaking styles [2]. Readers can refer to [22, 23, 27] for some  
analyses on the advantages and drawbacks of HMM-based SPSS. One major 
drawback of HMM-based SPSS is that the quality of the synthesized speech is 
not good enough [22, 23], mainly due to two reasons: (1) the accuracy of acoustic 
models is not good enough, and the predicted acoustic features are over-smoothing 
and a lack of details, and (2) the vocoding techniques are not good enough. The 
first reason is mainly due to a lack of modeling capacity in HMM. Thus, DNN-
based acoustic models [3] are proposed in SPSS, which improve the synthesized 
quality of HMM-based models. Later, in order to better model the long time span 
contextual effect in a speech utterance, LSTM-based recurrent neural networks [5] 
are leveraged to better model the’ context dependency. As the development of deep 
learning, some advanced network structures such as CBHG [7] are leveraged to 
better predict acoustic features [28]. VoiceLoop [29] adopts a working memory 
called phonological loop to generate acoustic features (e.g., F0, MGC, BAP) from 
phoneme sequence, and then uses a WORLD [25] vocoder to synthesize waveform
from this acoustic features. Yang et al. [30] leverage GAN [31] to improve the 
generation quality of acoustic features. Wang et al. [32] explore a more end-to-
end way that leverages an attention-based recurrent sequence transducer model 
to directly generate acoustic features from phoneme sequence, which can avoid 
the frame-by-frame alignments required in previous neural network-based acoustic 
models. Wang et al. [33] conduct thorough experimental studies on different 
acoustic models. Some acoustic models in SPSS are summarized in Table 5.1.

### 5.1.2.Acoustic Models in Neural TTS

Acoustic models enhanced with deep neural networks have several advantages 
compared to those in SPSS: (1) Conventional acoustic models require alignments 
between linguistic and acoustic features, while sequence-to-sequence-based neural 
models implicitly learn the alignments through attention or predict the duration 
jointly, which are more end-to-end and require less preprocessing. (2) As the 
increasing modeling power of neural networks, the linguistic features are simplified 
into only character or phoneme sequence, and the acoustic features have changed 
from low-dimensional and condensed cepstrum (e.g., MGC) to high-dimensional 
mel-spectrograms or even more high-dimensional linear-spectrograms. In the fol-
lowing paragraphs, we introduce some representative acoustic models in neural 
TTS,1 and provide a comprehensive list of acoustic models in Table 5.1.

## 5.2.Acoustic Models with Different Structures

Different model structures have been adopted in acoustic models, such as recurrent 
neural networks (RNNs), convolutional neural networks (CNNs), and self-attention 
in Transformer. In this section, we introduce typical acoustic models with different 
model structures.

### 5.2.1.RNN-Based Models (e.g., Tacotron Series)

The inductive bias to use RNNs is that speech signals are inherently in sequen-
tial order. We introduce representative RNN-based acoustic models, such as the 
Tacotron series [7, 8].

#### Tacotron 
Tacotron [7] leverages an encoder-attention-decoder framework and takes char-
acters (with text normalization) as input2 and outputs linear-spectrograms, and 
uses Griffin-Lim algorithm [69] to generate the waveform. The model structure 
of Tacotron is shown in Fig. 5.1. The encoder consists of a Pre-net that pre-
processes the character embedding into a sequence of hidden representations, and a 
CBHG module [7, 70] that transforms the hidden representations into the encoder 
output representations. Specifically, the CBHG module consists of (1) several 1D 
convolution filters with different kernel sizes k to emphasize k-gram in phoneme 
sequence, (2) a highway network [71] to extract high-level features, and (3) a 
bidirectional gated recurrent unit (GRU) [72] to generate final representations 
of the character sequence. The decoder consists of (1) a Pre-net to pre-process 
the mel-spectrogram sequence, (2) an attention RNN to attend to the encoder 
representations, (3) a decoder RNN to generate the mel-spectrogram sequence, and 
(4) a post-processing network using CBHG module to predict linear-spectrograms 
from mel-spectrograms. Finally, Tacotron leverages the Griffin-Lim algorithm [69] 
to synthesize waveform from linear-spectrograms. A useful trick is to predict 
multiple frames at each autoregressive step, which helps the learning of attention 
alignment [7].

#### Tacotron 2 

Since the Griffin-Lim algorithm used in Tacotron leads to artifacts and worse quality 
than neural-based vocoders such as WaveNet[73], Tacotron 2 [8] is proposed to 
generate mel-spectrograms and convert mel-spectrograms into a waveform using 
WaveNet [73] model. As shown in Fig. 5.2, Tacotron 2 adopts an encoder-attention-
decoder framework. The encoder consists of a Pre-net with 3 convolution layers to 
convert the character embeddings into hidden representations, and a bidirectional 
LSTM to generate the hidden representations of the encoder. The decoder consists 
of a Pre-net to pre-process mel-spectrograms and a 2-layer LSTM with a location-
sensitive attention [74] to generate mel-spectrograms in an autoregressive way, 
with a Post-net to further enhance the quality of the predicted mel-spectrograms. 
Tacotron 2 greatly improves the voice quality over previous methods including 
concatenative TTS, parametric TTS, and neural TTS such as Tacotron. 

#### Other Tacotron Related Acoustic Models 

A lot of works improve Tacotron 1/2 from different aspects: (1) Using a reference 
encoder and style tokens to enhance the expressiveness of the speech synthesis, 
such as GST-Tacotron [75] and Ref-Tacotron [76]. (2) Removing the attention 
mechanism in Tacotron, and instead using a duration predictor for autoregressive 
prediction, such as DurIAN [35] and Non-Attentive Tacotron [36]. (3) Changing 
the autoregressive generation in Tacotron to non-autoregressive generation, such 
as Parallel Tacotron 1/2 [61, 62] which do not use RNN structure anymore. (4)
Building end-to-end text-to-waveform models based on Tacotron, such as Wave-
Tacotron [77]. 
RNN-based acoustic models can well model the text and speech sequence into 
a sequential nature but suffer from both slow training and slow inference. To this 
end, parallel structures such as CNN and self-attention-based models are further 
leveraged in acoustic models.

### 5.2.2.CNN-Based Models (e.g., DeepVoice Series).

The inductive bias to use CNNs is that speech signals are locally dependent, i.e., a 
speech frame is mostly influenced by its adjacent speech frames. 
DeepVoice [38] is actually an SPSS system enhanced with convolutional neural 
networks. After obtaining linguistic features through neural networks, DeepVoice 
leverages a WaveNet [73] based vocoder to generate the waveform. DeepVoice 
2 [39] follows the basic data conversion flow of DeepVoice and enhances DeepVoice 
with improved network structures and multi-speaker modeling. Furthermore, Deep-
Voice 2 also adopts a Tacotron + WaveNet model pipeline, which first generates 
linear-spectrograms using Tacotron and then generates waveform using WaveNet. 
DeepVoice 3 [9] leverages a fully-convolutional network structure for speech 
synthesis as shown in Fig. 5.3, which generates mel-spectrograms from characters 
and can scale up to real-word multi-speaker datasets. DeepVoice 3 improves over 
previous DeepVoice 1/2 systems by using a more compact sequence-to-sequence 
model and directly predicting mel-spectrograms instead of complex linguistic 
features. 
Besides the DeepVoice series, there are several other CNN-based acoustic 
models: (1) Based on DeepVoice 3, ClariNet [78] is proposed to generate waveform
from a text in a fully end-to-end way. (2) ParaNet [12] is a fully convolutional-
based non-autoregressive model that can speed up the mel-spectrogram generation 
and obtain reasonably good speech quality. (3) DCTTS [40] shares a similar 
data conversion pipeline with Tacotron and leverages a fully convolutional-based 
encoder-attention-decoder network to generate mel-spectrograms from character 
sequences. It then uses a spectrogram super-resolution network to obtain linear-
spectrograms and synthesizes waveform using Griffin-Lim [69].

### 5.2.3.Transformer-Based Models (e.g., FastSpeech Series)

Transformer [79] removes the sequential or local bias in RNNs and CNNs and 
models the sequence through the self-attention mechanism, where the attention 
weights are automatically learned from data. Transformer demonstrates strong 
capacity in sequence modeling. 

#### TransformerTTS 

TransformerTTS [10] (Fig. 5.4) leverages Transformer [79] based encoder-
attention-decoder architecture to generate mel-spectrograms from phonemes. The 
authors in TransformerTTS [10] argue that RNN-based encoder-attention-decoder 
models like Tacotron 2 suffer from the following two issues: (1) Due to the recurrent 
nature, both the RNN-based encoder and decoder cannot be trained in parallel, and 
the RNN-based encoder cannot be parallel in inference, which affects the efficiency 
both in training and inference. (2) Since the text and speech sequences are usually 
very long, RNNs are not good at modeling the long dependency in these sequences. 
TransformerTTS adopts the basic model structure of Transformer and absorbs some 
designs from Tacotron 2 such as decoder Pre-net/Post-net and stop token prediction. 
It achieves similar voice quality to Tacotron 2 but enjoys faster training time. 
However, compared with RNN-based models such as Tacotron that leverage stable 
attention mechanisms such as location-sensitive attention, the encoder-decoder 
attention in Transformer is not robust due to parallel computation. Thus, some 
works propose to enhance the robustness of Transformer-based acoustic models. 
For example, MultiSpeech [44] improves the robustness of the attention mechanism 
through several technologies including encoder normalization, decoder bottleneck, 
and diagonal attention constraint, and RobuTrans [80] leverages duration prediction 
to enhance the robustness in an autoregressive generation. 

#### FastSpeech 

Previous neural-based acoustic models such as Tacotron 1/2 [7, 8], DeepVoice 
3 [9], and TransformerTTS [10] all adopt autoregressive generation, which suffer
from several issues: (1) Slow inference speed. The autoregressive mel-spectrogram 
generation is slow, especially for long speech sequences (e.g., for a 5 s speech, 
there are nearly 500 frames of mel-spectrograms if hop size is 10 ms, which is 
a long sequence). (2) Robust issues. The generated speech usually has a lot of 
word skipping and repeating issues, which are mainly caused by the inaccurate 
attention alignments between text and mel-spectrogram sequences in an encoder-
attention-decoder-based autoregressive generation. Thus, FastSpeech [11] (Fig. 5.5) 
is proposed to solve these issues: (1) It adopts a feed-forward Transformer network 
to generate mel-spectrograms in parallel, which can greatly speed up inference. (2) 
It removes the attention mechanism between text and speech to avoid word skipping 
and repeating issues and improve robustness. Instead, it uses a length regulator to 
bridge the length mismatch between the phoneme and mel-spectrogram sequences. 
The length regulator leverages a duration predictor3 to predict the duration of each 
phoneme and expands the phoneme hidden sequence according to the phoneme 
duration, where the expanded phoneme hidden sequence can match the length of 
the mel-spectrogram sequence and facilitate the parallel generation.
FastSpeech enjoys several advantages [11]:4 (1) extremely fast inference speed 
(e.g., .270× inference speedup on mel-spectrogram generation, .38× speedup on 
waveform generation), (2) robust speech synthesis without word skipping and 
repeating issues, and (3) on par or even better voice quality with previous autore-
gressive models. 

#### FastSpeech 2 

FastSpeech 2 [45] (Fig. 5.6) is proposed to further enhance FastSpeech, mainly from 
two aspects: (1) Using ground-truth mel-spectrograms as training targets, instead of 
the distilled mel-spectrograms from an autoregressive teacher model in FastSpeech. 
This simplifies the two-stage teacher-student distillation pipeline in FastSpeech 
and also avoids the information loss in target mel-spectrograms after distillation. 
(2) Providing more variance information such as pitch, duration, and energy as 
decoder input, which eases the one-to-many mapping problem [7, 81–83] in text-
to-speech synthesis.5 FastSpeech 2 achieves better voice quality than FastSpeech 
and maintains the advantages of fast, robust, and controllable speech synthesis in 
FastSpeech.6 
To provide an overall comparison between different acoustic models, we sum-
marize each component in the encoder and decoder in acoustic models as well as 
the vocoder used in each acoustic model in Table 5.2, and also summarize the time 
complexity of different model structures in training and inference in Table 5.3.

### 5.2.4.Advanced Generative Models (GAN/Flow/VAE/Diffusion)

> Acoustic models learn the mapping between linguistic features (e.g., phoneme 
sequence) and acoustic features (e.g., mel-spectrogram sequence). The mapping 
between mel-spectrogram and phoneme sequences are one-to-many since multiple 
mel-spectrogram variations (e.g., different speed, pitches, volumes) can correspond 
to the same phoneme sequence. Thus, the mel-spectrogram data conditioned on
phoneme is distributional-wise, instead of point-wise, and are usually modeled 
by generative models, such as autoregressive models, normalizing flows (Flow), 
variational auto-encoders (VAE), generative adversarial networks (GAN), and 
denoising diffusion probabilistic models (Diffusion). In this section, besides the 
basic generative models introduced in the previous subsections, we introduce some 
advanced generative models used in acoustic models, such as Flow, GAN, VAE, and 
Diffusion.7 We just give a brief overview of the application of generative models
in acoustic models in this chapter. For the detailed formulation of deep generative 
models, please refer to Sect. 3.3. 

#### GAN-Based Models 

GANs have been widely used in acoustic models, such as GAN exposure [63], TTS-
Stylization [64], and Multi-SpectroGAN [13]. Since simple L1/L2 loss can help 
the acoustic models to predict mel-spectrograms in a reasonable quality, GANs are 
usually used as an auxiliary loss to compensate for the L1/L2 mel-spectrogram loss 
to achieve better prediction quality. 

#### Flow-Based Models 

Normalizing flows have been leveraged in acoustic models to better model the 
conditional mel-spectrogram distributions. As introduced in Sect. 3.3.2, normalizing 
flows can be divided into two categories: autoregressive flows and bipartite flows. 
Accordingly, both types of flows are applied to acoustic models. Flowtron [57] 
is an autoregressive flow-based mel-spectrogram generation model, while Flow-
TTS [55] and Glow-TTS [56] leverage bipartite flows for a non-autoregressive 
mel-spectrogram generation. 

#### VAE-Based Models 

Early work [75, 76] leverage reference encoders and style tokens to model the 
variance information from a reference speech, which can be regarded as an auto-
encoder, and here the reference encoder is the encoder of the auto-encoder. Later, 
variational auto-encoders (VAEs) [84] are leveraged in acoustic models, such 
as GMVAE-Tacotron [15], VAE-TTS [59], BVAE-TTS [60], and Para. Tacotron 
1/2 [61, 62]. Different from the standard unconditional VAEs as formulated in 
Eq. 3.10, the VAEs used in acoustic models are conditioned on source text or 
phoneme sequence, which are formulated as follows: 
.L(x; θ, φ) = −Ez∼q(z|x;φ)log p(x|z, y; θ) + KL(q(z|x; φ)||p(z)),(5.1) 
where y represents the text or phoneme sequence, . θ represents the parameters of 
both the phoneme encoder and mel-spectrogram decoder, . φ represents the parame-
ters of the VAE encoder (posterior encoder), and.p(z) is the prior distribution, which 
is chosen as standard Gaussian distribution.
There is also another formulation of conditional VAEs for acoustic models. 
Instead of choosing standard Gaussian as the prior distribution, we leverage a prior 
encoder.θprito predict the prior distribution from the conditional phoneme sequence 
y. Thus, this kind of conditional VAE can be formulated as follows: 
. L(x; θdec, θpri, φ) = −Ez∼q(z|x;φ)log p(x|z; θdec) + KL(q(z|x; φ)||p(z|y; θpri)),
(5.2) 
where .θdecrepresents the parameters of the mel-spectrogram decoder (VAE 
decoder), .θprirepresents the parameters of the prior encoder, . φ represents the 
parameters of the VAE encoder (posterior encoder), and .p(z|y; θpri) is the prior 
distribution predicted from the phoneme sequence, instead of standard Gaussian 
distribution. Some models such as VITS [85] and NaturalSpeech [86] leverage this 
kind of conditional VAEs, which are introduced in Chap. 7. 

#### Diffusion-Based Models 

After diffusion models [87, 88] are first leveraged in vocoders [16, 17], a lot of 
works have applied diffusion models into acoustic models, such as Diff-TTS [65], 
Grad-TTS [66], and PriorGrad [67]. Diffusion models are good at generating high-
quality mel-spectrograms with many fine-grained details, but at the cost of slow 
inference speed due to a large number of iteration steps. Thus, a lot of work has 
tried to reduce the number of iteration steps to speed up inference. For example, 
PriorGrad [67] provides a more informative prior distribution which is calculated 
from the conditional phoneme sequence and is closer to the mel-spectrogram 
distribution, which can reduce the difficulty of the data generation and thus result in 
faster training and inference.