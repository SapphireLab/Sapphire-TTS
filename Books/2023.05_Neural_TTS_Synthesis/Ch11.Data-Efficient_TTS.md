# 11.Data-Efficient TTS

## Abstract

> Deep learning-based TTS models require a large amount of training data to learn the mapping patterns between text and speech as well as the complicated distributions in speech.
> However, in many scenarios, the training data is usually limited, which hinders the effective learning of neural TTS models.
> In this chapter, we introduce data-efficient methods to solve the low data resource issue in TTS. 

> Generally speaking, there are different scenarios with low data resources: (1) language level, where there is a lack of training data when we want to build TTS models for a language, and (2) speaker level, where there is a lack of training data when we want to build TTS models for a speaker.
> Thus, we mainly introduce data-efficient TTS methods from the two scenarios in Sects. 11.1 and 11.2.

## 11.1.Language-Level Data-Efficient TTS

> Building a high-quality TTS system for a new language usually requires a large amount of high-quality paired text and speech data.
> However, there are more than 7000 languages in the world,1 and most languages lack training data for developing TTS systems.
> As a result, popular commercialized speech services2 can only support dozens of (or one hundred) languages for TTS.
> Supporting TTS for low-resource languages can not only have business value but is also beneficial for social good.
> Thus, a lot of research works build TTS systems under low data resource scenarios.
> We summarize some representative techniques for low-resource TTS in Table 11.1 and introduce these techniques as follows.

### 11.1.1.Self-Supervised Training

> Although paired text and speech data is hard to collect, unpaired speech and text data (especially text data) are relatively easy to obtain.
> Self-supervised pre-training methods can be leveraged to enhance language understanding or speech generation capabilities [1–4].
> For example, the text encoder in TTS can be enhanced by the pre-trained BERT models [1, 4, 5], and the speech decoder in TTS can be pretrained through autoregressive mel-spectrogram prediction [1] or jointly trained with voice conversion task [3].
> Besides, speech can be quantized into a discrete token sequence to resemble the phoneme or character sequence [6].
> In this way, the quantized discrete tokens and the speech can be regarded as pseudo-paired data to pre-train a TTS model, which is then fine-tuned on a few truly paired text and speech data [7, 8, 26].

### 11.1.2.Cross-Lingual Transfer

> Although paired text and speech data is scarce in low-resource languages, it is abundant in rich-resource languages.
> Since human languages share similar vocal organs, pronunciations [27], and semantic structures [28], pre-training the TTS models on rich-resource languages can help the mapping between text and speech in low-resource languages [10–14, 29–33].
> Usually, there are different phoneme sets between rich- and low-resource languages.
> Thus, [10] proposes to map the embeddings between the phoneme sets from different languages, and LRSpeech [11] discards the pre-trained phoneme embeddings and initializes the phoneme embeddings from scratch for low-resource languages.
> International phonetic alphabet (IPA) [34] or byte representation [16] is adopted to support arbitrary texts in multiple languages.
> Besides, language similarity [28] can also be considered when conducting the cross-lingual transfer.

### 11.1.3.Semi-Supervised Training

> Text-to-speech (TTS) and automatic speech recognition (ASR) are two dual tasks [35] and can be leveraged together to improve each other. 
> Techniques like speech chain [18, 19] and back transformation [11, 17, 36] leverage additional unpaired text and speech data to boost the performance of TTS and ASR.

### 11.1.4.Mining Dataset in the Wild

> In some scenarios, there may exist some low-quality paired text and speech data on the Web.
> Cooper [20] and Hu et al. [21] propose to mine this kind of data and develop sophisticated techniques to train a TTS model.
> Some techniques such as speech enhancement [37], denoising [38], and disentangling [39, 40] can be leveraged to improve the quality of the speech data mined in the wild.

### 11.1.5.Purely Unsupervised Learning

> There are some methods [23–25] to build TTS systems without any paired text/speech data, but only unpaired text/speech data.
> They typically leverage an unsupervised speech recognizer with good speech representation learned in a self-supervised way to recognize pseudo transcripts, which is guided to match the distribution of true transcripts by the generator-discriminator framework [41].
> After that, they leverage the pseudo transcripts and the corresponding speech to train a TTS model.
> This unsupervised learning method relies on the progress in large-scale unsupervised learning on speech representation, which can provide good pseudo transcripts as the paired text/speech data.

## 11.2 Speaker-Level Data-Efficient TTS

> When a speaker lacks training data, we usually adapt a TTS model that is trained on multiple speakers to this speaker, i.e., adaptive TTS.
> Adaptive TTS3 is an important feature of TTS that can synthesize voice for any user.
> It is known by different terms in academia and industry, such as voice adaptation [42], voice cloning [43], custom voice [44], etc.
> Adaptive TTS has been a hot research topic, e,g., a lot of works in statistical parametric speech synthesis have studied voice adaptation [45–52], and the voice cloning challenge also attracts a lot of participants [53–56].
> In an adaptive TTS scenario, the data from other speakers can be leveraged to improve the synthesis quality of this speaker.
> This can be achieved by converting the voice of other speakers into this target voice through voice conversion to increase the training data [57], or by adapting the TTS models trained on a multi-speaker speech dataset to this target speaker through adaptation or cloning [42, 44].
> We mainly introduce the latter since the former involves another voice conversion model.
> Generally speaking, when considering adaptive TTS, more adaptation data and parameters will result in better voice quality but incur high data collection cost and model deployment costs.
> In practice, we aim to adapt as few data and parameters as possible (the ideal case is no adaptation data and parameters are needed) while achieving high adaptation voice quality.
> To achieve this, on the one hand, we should improve the generalization of the source TTS model and take different adaptation domains into consideration.
> On the other hand, we should consider the methods with few or no adaptation data and parameters.
> Accordingly, we introduce adaptive TTS from several perspectives: 
> (1) Improving generalization for adaptation, which enhances the generalization of the source TTS model to support new speakers. 
> (2) Cross-domain adaption, which adapts the source TTS models to different acoustic conditions, styles, and languages. 
> (3) Few-data adaptation, which uses few data to adapt to a target speaker. 
> (4) Few-parameter adaption, which uses few model parameters to adapt to a target speaker. 
> (5) Zero-shot adaptation, which generalizes the source TTS model to a target speaker without any adaptation data or parameters.
> We summarize the works in each perspective in Table 11.2 and introduce them in the following subsections.

### 11.2.1.Improving Generalization

> A key factor in speaker-level data-efficient TTS is to improve the generalization of the source TTS model.
> In source model training, the source text does not contain enough acoustic information such as prosody, speaker timbre, and recording environments to generate target speech.
> As a result, the TTS model is prone to overfit on the training data and has poor generalization for new speakers in adaptation.
> Chen et al. [44] propose acoustic condition modeling to provide necessary acoustic information as model input to learn the text-to-speech mapping with better generalization instead of memorizing.
> Another way to improve the generalization of the source TTS model is to increase the amount and diversity of training data.
> Cooper et al. [58] leverage speaker augmentation to increase the number of speakers when training source TTS model, which can generalize well to unseen speakers in adaptation.
> Yang and He [13] train a universal TTS model with multiple speakers in 50 language locales, which increases the generalization when adapting to a new speaker.

### 11.2.2.Cross-Domain Adaptation

> In adaptive TTS, an important factor is that the adaptation speech has different acoustic conditions or styles with the speech data used to train the source TTS model.
> In this way, special designs need to be considered to improve the generalization of the source TTS model and support the styles of target speakers.
> AdaSpeech [44] designs acoustic condition modeling to better model the acoustic conditions such as recording devices, environment noise, accents, speaker rates, speaker timbre, etc.
> In this way, the model tends to generalize instead of memorizing the acoustic conditions and can be well adapted to the speech data with different acoustic conditions.
> AdaSpeech 3 [60] adapts a reading-style TTS model to spontaneous style, by designing specific filled pauses adaptation,rhythm adaptation, and timbre adaptation.
> Some other works [61, 62] consider the adaptation across different speaking styles, such as Lombard [61] or whisper [62].
> Some works [34, 63–65, 77–81] propose to transfer voices across languages, e.g., synthesize Mandarin speech using an English speaker, where the English speaker does not have any Mandarin speech data.

### 11.2.3.Few-Data Adaptation

> The adaptation data is usually limited for a target speaker, and sometimes there are even only untranscribed speech data (without text transcripts) available.
> Accordingly, we introduce the methods for few-data adaptation, under the transcribed and untranscribed data settings.
> Some works [42–44, 56, 66–70] conduct few-shot adaptation that only uses few paired text and speech data, varying from several minutes to several seconds.
> Chien et al. [56] explore different speaker embeddings for few-shot adaptation.
> Yue et al. [82] leverage speech chain [18] for few-shot adaptation.
> Chen et al. [44] and Arık et al. [43] compare the voice quality with different amounts of adaptation data and find that voice quality improves quickly with the increase of adaptation data when data size is small (less than 20 sentences) and improves slowly with dozens of adaptation sentences.
> In many scenarios, only speech data can be collected such as in conversions or online meetings, without the corresponding transcripts.
> AdaSpeech 2 [71] leverages untranscribed speech data for voice adaptation, with the help of speech reconstruction and latent alignments [73].
> Inoue et al. [72] use an ASR model to transcribe the speech data and use the transcribed paired data for voice adaptation.

### 11.2.4.Few-Parameter Adaptation

> For adaptation parameters, the whole TTS model [42, 66], or part of the model (e.g., decoder) [67, 68], or only speaker embedding [42–44] can be fine-tuned.
> Similarly, fine-tuning more parameters will result in good voice quality but increase memory and deployment costs.
> To support many users/customers, the adaptation parameters need to be small enough for each target speaker to reduce memory usage while maintaining high voice quality.
> For example, if each user/voice consumes 100 MB parameters, the total memory storage equals to 100 PB for 1M users, which is a huge memory cost.
> Some works propose to reduce the number of adaptation parameters to as few as possible while maintaining the adaptation quality.
> AdaSpeech [44] proposes conditional layer normalization to generate the scale and bias parameters in layer normalization from the speaker embeddings based on contextual parameter generation [83] and only fine-tune the parameters related to the conditional layer normalization and speaker embeddings to achieve good adaptation quality.
> Moss et al. [67] propose a fine-tuning method that selects different model hyperparameters for different speakers based on the Bayesian optimization, which achieves the goal of synthesizing the voice of a specific speaker with only a small number of speech samples.

### 11.2.5.Zero-Shot Adaptation

> Some works [42, 43, 74–76, 84] conduct zero-shot adaptation, which leverage a speaker encoder to extract speaker embeddings given reference audio.
> This scenario is quite appealing since no adaptation data and parameters are needed.
> However, the adaptation quality is not good enough especially when the target speaker is very different from the source speakers.

## References

1. Chung YA, Wang Y, Hsu WN, Zhang Y, Skerry-Ryan R (2019) Semi-supervised training for improving data efficiency in end-to-end speech synthesis. In: ICASSP 2019-2019 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 6940– 
6944 
2. Wang P, Qian Y, Soong FK, He L, Zhao H (2015) Word embedding for recurrent neural network based TTS synthesis. In: 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 4879–4883 
3. Zhang M, Wang X, Fang F, Li H, Yamagishi J (2019) Joint training framework for text-to-speech and voice conversion using multi-source Tacotron and WaveNet. In: Proceedings of the Interspeech 2019, pp 1298–1302 
4. Fang W, Chung YA, Glass J (2019) Towards transfer learning for end-to-end speech synthesis from deep pre-trained language models. Preprint. arXiv:1906.07307 
5. Jia Y, Zen H, Shen J, Zhang Y, Wu Y (2021) PnG BERT: augmented BERT on phonemes and graphemes for neural TTS. Preprint. arXiv:2103.15060 
6. Tjandra A, Sisman B, Zhang M, Sakti S, Li H, Nakamura S (2019) VQVAE unsupervised unit discovery and multi-scale Code2Spec inverter for zerospeech challenge 2019. In: Proceedings of the Interspeech 2019, pp 1118–1122 
7. Liu AH, Tu T, Lee H-Y, Lee L-S (2020) Towards unsupervised speech recognition and synthesis with quantized speech representation learning. In: ICASSP 2020-2020 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 7259–7263 
8. Tu T, Chen YJ, Liu AH, Lee Hy (2020) Semi-supervised learning for multi-speaker text-to-speech synthesis using discrete speech representation. In: Proceedings of the Interspeech 2020, pp 3191–3195 
9. Dunbar E, Algayres R, Karadayi J, Bernard M, Benjumea J, Cao XN, Miskic L, Dugrain C, Ondel L, Black AW et al (2019) The zero resource speech challenge 2019: TTS without T. In: Proceedings of the Interspeech 2019, pp 1088–1092 
10. Chen YJ, Tu T, Yeh C-C, Lee HY (2019) End-to-end text-to-speech for low-resource languages by cross-lingual transfer learning. In: Proceedings of the Interspeech 2019, pp 2075–2079 
11. Xu J, Tan X, Ren Y, Qin T, Li J, Zhao S, Liu TY (2020) LRSpeech: extremely low-resource speech synthesis and recognition. In: Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery and data mining, pp 2802–2812
12. Azizah K, Adriani M, Jatmiko W (2020) Hierarchical transfer learning for multilingual, multi-speaker, and style transfer DNN-based TTS on low-resource languages. IEEE Access 
8:179798–179812 
13. Yang J, He L (2020) Towards universal text-to-speech. In: INTERSPEECH, pp 3171–3175 
14. de Korte M, Kim J, Klabbers E (2020) Efficient neural speech synthesis for low-resource languages through multilingual modeling. In: Proceedings of the Interspeech 2020, pp 2967– 
2971 
15. Prajwal K, Jawahar C (2021) Data-efficient training strategies for neural TTS systems. In: 8th ACM IKDD CODS and 26th COMAD, pp 223–227 
16. He M, Yang J, He L (2021) Multilingual Byte2Speech text-to-speech models are few-shot spoken language learners. Preprint. arXiv:2103.03541 
17. Ren Y, Tan X, Qin T, Zhao S, Zhao Z, Liu TY (2019) Almost unsupervised text to speech and automatic speech recognition. In: International conference on machine learning (PMLR), pp 
5410–5419 
18. Tjandra A, Sakti S, Nakamura S (2017) Listening while speaking: speech chain by deep learning. In: 2017 IEEE automatic speech recognition and understanding workshop (ASRU). IEEE, pp 301–308 
19. Tjandra A, Sakti S, Nakamura S (2018) Machine speech chain with one-shot speaker adaptation. In: Proceedings of the Interspeech 2018, pp 887–891 
20. Cooper EL (2019) Text-to-speech synthesis using found data for low-resource languages. Ph.D Thesis, Columbia University 
21. Hu Q, Marchi E, Winarsky D, Stylianou Y, Naik D, Kajarekar S (2019) Neural text-to-speech adaptation from low quality public recordings. In: Speech synthesis workshop, vol 10 
22. Cooper E, Wang X, Zhao Y, Yasuda Y, Yamagishi J (2020) Pretraining strategies, waveform model choice, and acoustic configurations for multi-speaker end-to-end speech synthesis. Preprint. arXiv:2011.04839 
23. Liu AH, Lai CIJ, Hsu WN, Auli M, Baevskiv A, Glass J (2022) Simple and effective unsupervised speech synthesis. Preprint. arXiv:2204.02524 
24. Ni J, Wang L, Gao H, Qian K, Zhang Y, Chang S, Hasegawa-Johnson M (2022) Unsupervised text-to-speech synthesis by unsupervised automatic speech recognition. Preprint. arXiv:2203.15796 
25. Lian J, Zhang C, Anumanchipalli GK, Yu D (2022) UTTS: Unsupervised TTS with conditional disentangled sequential variational auto-encoder. Preprint. arXiv:2206.02512 
26. Zhang H, Lin Y (2020) Unsupervised learning for sequence-to-sequence text-to-speech for low-resource languages. In: Proceedings of the Interspeech 2020, pp 3161–3165 
27. Wind J (1989) The evolutionary history of the human speech organs. Stud Lang Origins 1:173– 
197 
28. Tan X, Chen J, He D, Xia Y, Tao Q, Liu TY (2019) Multilingual neural machine translation with language clustering. In: Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing 
(EMNLP-IJCNLP), pp 962–972 
29. Guo W, Yang H, Gan Z (2018) A DNN-based Mandarin-Tibetan cross-lingual speech synthesis. In: 2018 Asia-Pacific signal and information processing association annual summit and conference (APSIPA ASC). IEEE, pp 1702–1707 
30. Tan X, Leng Y, Chen J, Ren Y, Qin T, Liu TY (2019) A study of multilingual neural machine translation. Preprint. arXiv:1912.11625 
31. Zhang W, Yang H, Bu X, Wang L (2019) Deep learning for Mandarin-Tibetan cross-lingual speech synthesis. IEEE Access 7:167884–167894 
32. Nekvinda T, Dušek O (2020) One model, many languages: meta-learning for multilingual text-to-speech. In: Proceedings of the Interspeech 2020 pp 2972–2976 
33. Zhang C, Tan X, Ren Y, Qin T, Zhang K, Liu TY (2021) UWSpeech: speech to speech translation for unwritten languages. In: AAAI association for the advancement of artificial intelligence
34. Hemati H, Borth D (2020) Using IPA-based Tacotron for data efficient cross-lingual speaker adaptation and pronunciation enhancement. Preprint. arXiv:2011.06392 
35. Qin T (2020) Dual learning. Springer 
36. Chen J, Tan X, Leng Y, Xu J, Wen G, Qin T, Liu TY (2021) Speech-T: transducer for text to speech and beyond. Adv Neural Inf Process Syst 34:6621–6633 
37. Valentini-Botinhao C, Yamagishi J (2018) Speech enhancement of noisy and reverberant speech for text-to-speech. IEEE/ACM Trans Audio Speech Lang Process 26(8):1420–1433 
38. Zhang C, Ren Y, Tan X, Liu J, Zhang K, Qin T, Zhao S, Liu TY (2021) DenoiSpeech: denoising text to speech with frame-level noise modeling. In: 2021 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE 
39. Wang Y, Stanton D, Zhang Y, Skerry-Ryan R, Battenberg E, Shor J, Xiao Y, Jia Y, Ren F, Saurous RA (2018) Style tokens: unsupervised style modeling, control and transfer in end-to-end speech synthesis. In: International conference on machine learning (PMLR), pp 5180–5189 
40. Hsu WN, Zhang Y, Weiss RJ, Chung YA, Wang Y, Wu Y, Glass J (2019) Disentangling correlated speaker and noise for speech synthesis via data augmentation and adversarial factorization. In: ICASSP 2019-2019 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 5901–5905 
41. Goodfellow IJ, Pouget-Abadie J, Mirza M, Xu B, Warde-Farley D, Ozair S, Courville A, Bengio Y (2014) Generative adversarial nets. In: NIPS 
42. Chen Y, Assael Y, Shillingford B, Budden D, Reed S, Zen H, Wang Q, Cobo LC, Trask A, Laurie B et al (2018) Sample efficient adaptive text-to-speech. In: International conference on learning representations 
43. Arık SÖ, Chen J, Peng K, Ping W, Zhou Y (2018) Neural voice cloning with a few samples. In: Proceedings of the 32nd international conference on neural information processing systems, pp 
10040–10050 
44. Chen M, Tan X, Li B, Liu Y, Qin T, Zhao S, Liu TY (2021) AdaSpeech: Adaptive text to speech for custom voice. In: International conference on learning representations. https://openreview. net/forum?id=Drynvt7gg4L 
45. Tamura M, Masuko T, Tokuda K, Kobayashi T (1998) Speaker adaptation for HMM-based speech synthesis system using MLLR. In: The third ESCA/COCOSDA workshop (ETRW) on speech synthesis 
46. Yamagishi J, Nose T, Zen H, Ling ZH, Toda T, Tokuda K, King S, Renals S (2009) Robust speaker-adaptive HMM-based text-to-speech synthesis. IEEE Trans Audio Speech Lang Process 17(6):1208–1230 
47. Fan Y, Qian Y, Soong FK, He L (2015) Multi-speaker modeling and speaker adaptation for DNN-based TTS synthesis. In: 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 4475–4479 
48. Wu Z, Swietojanski P, Veaux C, Renals S, King S (2015) A study of speaker adaptation for DNN-based speech synthesis. In: Sixteenth annual conference of the international speech communication association 
49. Zhao Y, Saito D, Minematsu N (2016) Speaker representations for speaker adaptation in multiple speakers BLSTM-RNN-based speech synthesis. Space 5(6):7 
50. Fan Y, Qian Y, Soong FK, He L (2016) Speaker and language factorization in DNN-based TTS synthesis. In: 2016 IEEE International conference on acoustics, speech and signal processing 
(ICASSP). IEEE, pp 5540–5544 
51. Doddipatla R, Braunschweiler N, Maia R (2017) Speaker adaptation in DNN-based speech synthesis using d-vectors. In: INTERSPEECH, pp 3404–3408 
52. Huang Z, Lu H, Lei M, Yan Z (2018) Linear networks based speaker adaptation for speech synthesis. In: 2018 IEEE international conference on acoustics, speech and signal processing 
(ICASSP). IEEE, pp 5319–5323 
53. Xie Q, Tian X, Liu G, Song K, Xie L, Wu Z, Li H, Shi S, Li H, Hong F et al (2021) The multi-speaker multi-style voice cloning challenge 2021. Preprint. arXiv:2104.01818 
54. Hu CH, Wu YC, Huang WC, Peng YH, Chen YW, Ku PJ, Toda T, Tsao Y, Wang HM (2021) The AS-NU system for the M2VoC challenge. Preprint. arXiv:2104.03009
55. Tan D, Huang H, Zhang G, Lee T (2021) CUHK-EE voice cloning system for ICASSP 2021 M2VoC challenge. Preprint. arXiv:2103.04699 
56. Chien CM, Lin JH, Huang C-Y, Hsu P-C, Lee H-Y (2021) Investigating on incorporating pretrained and learnable speaker representations for multi-speaker multi-style text-to-speech. Preprint. arXiv:2103.04088 
57. Huybrechts G, Merritt T, Comini G, Perz B, Shah R, Lorenzo-Trueba J (2020) Low-resource expressive text-to-speech using data augmentation. Preprint. arXiv:2011.05707 
58. Cooper E, Lai CI, Yasuda Y, Yamagishi J (2020) Can speaker augmentation improve multi-speaker end-to-end TTS? In: Proceedings of the Interspeech 2020, pp 3979–3983 
59. Cong J, Yang S, Xie L, Yu G, Wan G (2020) Data efficient voice cloning from noisy samples with domain adversarial training. In: Proceedings of the Interspeech 2020, pp 811–815 
60. Yan Y, Tan X, Li B, Zhang G, Qin T, Zhao S, Shen Y, Zhang WQ, Liu TY (2021) AdaSpeech 
3: adaptive text to speech for spontaneous style. In: INTERSPEECH 
61. Paul D, Shifas MP, Pantazis Y, Stylianou Y (2020) Enhancing speech intelligibility in text-to-speech synthesis using speaking style conversion. In: Proceedings of the Interspeech 2020, pp 
1361–1365 
62. Hu Q, Bleisch T, Petkov P, Raitio T, Marchi E, Lakshminarasimhan V (2021) Whispered and lombard neural speech synthesis. In: 2021 IEEE spoken language technology workshop (SLT). IEEE, pp 454–461 
63. Zhang Y, Weiss RJ, Zen H, Wu Y, Chen Z, Skerry-Ryan R, Jia Y, Rosenberg A, Ramabhadran B (2019) Learning to speak fluently in a foreign language: multilingual speech synthesis and cross-language voice cloning. In: Proceedings of the Interspeech 2019, pp 2080–2084 
64. Chen M, Chen M, Liang S, Ma J, Chen L, Wang S, Xiao J (2019) Cross-lingual, multi-speaker text-to-speech synthesis using neural speaker embedding. In: Proceedings of the Interspeech 
2019, pp 2105–2109 
65. Liu Z, Mak B (2019) Cross-lingual multi-speaker text-to-speech synthesis for voice cloning without using parallel corpus for unseen speakers. Preprint. arXiv:1911.11601 
66. Kons Z, Shechtman S, Sorin A, Rabinovitz C, Hoory R (2019) High quality, lightweight and adaptable TTS using LPCNet. In: Proceedings of the Interspeech 2019, pp 176–180 
67. Moss HB, Aggarwal V, Prateek N, González J, Barra-Chicote R (2020) BOFFIN TTS: few-shot speaker adaptation by Bayesian optimization. In: ICASSP 2020-2020 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 7639–7643 
68. Zhang Z, Tian Q, Lu H, Chen LH, Liu S (2020) AdaDurIAN: few-shot adaptation for neural text-to-speech with durian. Preprint. arXiv:2005.05642 
69. Choi S, Han S, Kim D, Ha S (2020) Attentron: few-shot text-to-speech utilizing attention-based variable-length embedding. In: Proceedings of the Interspeech 2020, pp 2007–2011 
70. Min D, Lee DB, Yang E, Hwang SJ (2021) Meta-StyleSpeech: multi-speaker adaptive text-to-speech generation. Preprint. arXiv:2106.03153 
71. Yan Y, Tan X, Li B, Qin T, Zhao S, Shen Y, Liu TY (2021) AdaSpeech 2: adaptive text to speech with untranscribed data. In: 2021 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE 
72. Inoue K, Hara S, Abe M, Hayashi T, Yamamoto R, Watanabe S (2020) Semi-supervised speaker adaptation for end-to-end speech synthesis with pretrained models. In: ICASSP 2020-
2020 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 7634–7638 
73. Luong HT, Yamagishi J (2020) Nautilus: a versatile voice cloning system. IEEE/ACM Trans Audio Speech Lang Process 28:2967–2981 
74. Jia Y, Zhang Y, Weiss RJ, Wang Q, Shen J, Ren F, Chen Z, Nguyen P, Pang R, Moreno IL et al 
(2018) Transfer learning from speaker verification to multispeaker text-to-speech synthesis. In: Proceedings of the 32nd international conference on neural information processing systems, pp 
4485–4495
75. Cooper E, Lai CI, Yasuda Y, Fang F, Wang X, Chen N, Yamagishi J (2020) Zero-shot multi-speaker text-to-speech with state-of-the-art neural speaker embeddings. In: ICASSP 2020-2020 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 
6184–6188 
76. Wu Y, Tan X, Li B, He L, Zhao S, Song R, Qin T, Liu TY (2022) Adaspeech 4: adaptive text to speech in zero-shot scenarios. In: INTERSPEECH 
77. Zhao S, Nguyen TH, Wang H, Ma B (2020) Towards natural bilingual and code-switched speech synthesis based on mix of monolingual recordings and cross-lingual voice conversion. In: Proceedings of the Interspeech 2020 pp 2927–2931 
78. Himawan I, Aryal S, Ouyang I, Kang S, Lanchantin P, King S (2020) Speaker adaptation of a multilingual acoustic model for cross-language synthesis. In: ICASSP 2020-2020 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 7629–7633 
79. Staib M, Teh TH, Torresquintero A, Mohan DSR, Foglianti L, Lenain R, Gao J (2020) Phono-logical features for 0-shot multilingual speech synthesis. In: Proceedings of the Interspeech 
2020, pp 2942–2946 
80. Maiti S, Marchi E, Conkie A (2020) Generating multilingual voices using speaker space translation based on bilingual speaker data. In: ICASSP 2020-2020 IEEE international conference on acoustics, speech and signal processing (ICASSP), IEEE, pp 7624–7628 
81. Zhou X, Tian X, Lee G, Das RK, Li H (2020) End-to-end code-switching TTS with cross-lingual language model. In: ICASSP 2020-2020 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 7614–7618 
82. Yue F, Deng Y, He L, Ko T (2021) Exploring machine speech chain for domain adaptation and few-shot speaker adaptation. Preprint. arXiv:2104.03815 
83. Platanios EA, Sachan M, Neubig G, Mitchell T (2018) Contextual parameter generation for universal neural machine translation. In: Proceedings of the 2018 conference on empirical methods in natural language processing, pp 425–435 
84. Casanova E, Shulby C, Gölge E, Müller NM, de Oliveira FS, Junior AC, da Silva Soares A, Aluisio SM, Ponti MA (2021) SC-GlowTTS: an efficient zero-shot multi-speaker text-to-speech model. Preprint. arXiv:2104.05557