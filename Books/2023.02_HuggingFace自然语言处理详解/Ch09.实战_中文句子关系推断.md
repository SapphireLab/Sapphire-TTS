# 第09章.实战：中文句子关系推断

[代码文件](PythonFiles/Ch09.SentenceRelation.py)

注意: 本章内容经过重排

从人类的角度讲, 阅读两个句子, 很容易就能判断出两个句子是相连的还是无关的.
本章尝试使用神经网络来实现这个功能.

本任务仍然使用预训练的 BERT 模型 `bert-base-chinese` 作为 Backbone 部分以提取两个句子的文本数据特征, 通过下游模型来判断句子相连还是无关. BERT 本身的训练子任务之一就是判断两个句子的关系, 所以用 BERT 来完成这个任务是十分合适的.

注: 同样不进行 Backbone 部分的训练.

## 9.1.数据集准备

定义设备:
```python
device = "cuda" if torch.cuda.is_available() else "cpu"
```

定义分词器:
```python
tokenizer: BertTokenizer = BertTokenizer.from_pretrained("bert-base-chinese")
print("加载分词器".center(50, "="))
print(tokenizer)
运行结果:
======================加载分词器=======================
BertTokenizer(name_or_path='bert-base-chinese', vocab_size=21128, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={
	0: AddedToken("[PAD]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	100: AddedToken("[UNK]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	101: AddedToken("[CLS]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	102: AddedToken("[SEP]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	103: AddedToken("[MASK]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
```

定义数据集:
```python
class PairDataset(Dataset):

    def __init__(self, split):
        super().__init__()
        dataset = load_dataset(dataset_path, split=split)
        def cut(data):
            return len(data['text']) > 40
        self.dataset = dataset.filter(cut)

    def __len__(self):
        return len(self.dataset)
    
    def __getitem__(self, index):
        text = self.dataset[index]['text']
        sentence1 = text[:20]
        sentence2 = text[20:40]
        label = random.randint(0, 1)
        if label == 1:
            j = random.randint(0, len(self.dataset)-1)
            sentence2 = self.dataset[j]['text'][20:40]
        return sentence1, sentence2, label

print("定义数据集".center(50, "="))
train_dataset = PairDataset('train')
print("查看数据集样例".center(50, "="))
s1, s2, label = train_dataset[7]
print(f"数据集长度: {len(train_dataset)}")
print(f"句子 1: {s1}")
print(f"句子 2: {s2}")
print(f"标签  : {label}")
运行结果:
======================定义数据集=======================
=====================查看数据集样例======================
数据集长度: 8001
句子 1: 地理位置佳，在市中心。酒店服务好、早餐品
句子 2: 不了系统，郁闷啊。硬盘嘎吱响，怀疑有问题
标签  : 1
```

定义整理函数:
```python
def collate_fn(data):
    sentences = [i[:2] for i in data]
    labels = [i[2] for i in data]
    data = tokenizer.batch_encode_plus(
        batch_text_or_text_pairs = sentences,
        truncation               = True,
        padding                  = "max_length",
        max_length               = 45,
        return_tensors           = "pt",
        return_length            = True,
        add_special_tokens       = True)
    
    input_ids = data['input_ids'].to(device)
    attention_mask = data['attention_mask'].to(device)
    token_type_ids = data['token_type_ids'].to(device)
    labels = torch.LongTensor(labels).to(device)
    return input_ids, attention_mask, token_type_ids, labels

print("定义整理函数".center(50, "="))

tmp_data = [
    ("酒店还是非常的不错，我预定的是套件，服务", "非常好，随叫随到，结账非常快。", 0),
    ("外观很漂亮，性价比感觉还不错，功能简", "单，适合出差携带。蓝牙摄像头都有了。", 0),
    ("《穆斯林的葬礼》我已闻名已久，只是一直没", "怎能享受4星的服务，连空调都不能用的。", 1),
]

print("试运行".center(50, "="))
input_ids, attention_mask, token_type_ids, labels = collate_fn(tmp_data)
print(tokenizer.decode(input_ids[0]))
print(f"{input_ids.shape=}")
print(f"{attention_mask.shape=}")
print(f"{token_type_ids.shape=}")
print(f"{labels=}")
运行结果:
======================定义整理函数======================
=======================试运行========================
[CLS] 酒 店 还 是 非 常 的 不 错 ， 我 预 定 的 是 套 件 ， 服 务 [SEP] 非 常 好 ， 随 叫 随 到 ， 结 账 非 常 快 。 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]
input_ids.shape=torch.Size([3, 45])
attention_mask.shape=torch.Size([3, 45])
token_type_ids.shape=torch.Size([3, 45])
labels=tensor([0, 0, 1], device='cuda:0')
```

注意: 
- 在制作数据集时, 每个句子各有 20 个字, 但经过编码时, 每个字并不一定会被编码成一个词, 此外在编码时还会往句子中插入一些特殊符号, 所以并不能确定是 40 个词, 因此编码时需要留下容差, 即长度为 45. 如果有多余的位置需要进行填充.

定义数据加载器:
```python
loader = DataLoader(
    dataset     = train_dataset,
    batch_size  = 8,
    collate_fn  = collate_fn,
    shuffle     = True,
    drop_last   = True,
)
print("定义数据加载器".center(50, "="))
print(f"{len(loader)=}")
print("查看数据样例".center(50, "="))
for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(loader):
    break
print(tokenizer.decode(input_ids[0]))
print(f"{input_ids.shape=}")
print(f"{attention_mask.shape=}")
print(f"{token_type_ids.shape=}")
print(f"{labels=}")
运行结果:
=====================定义数据加载器======================
len(loader)=1000
======================查看数据样例======================
[CLS] aggerssive 书 中 却 拼 成 了 agg [SEP] resive 书 的 正 文 和 背 面 都 有 这 种 错 误 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]
input_ids.shape=torch.Size([8, 45])
attention_mask.shape=torch.Size([8, 45])
token_type_ids.shape=torch.Size([8, 45])
labels=tensor([0, 0, 1, 1, 1, 0, 1, 1], device='cuda:0')
```

## 9.2.模型定义

定义预训练模型:
```python
pretrained = BertModel.from_pretrained("bert-base-chinese")
print("加载预训练模型".center(50, "="))
print("模型参数:", sum(p.numel() for p in pretrained.parameters()))
for name, param in pretrained.named_parameters():
    param.requires_grad_(False)
print("模型可训练参数:", sum(p.numel() for p in pretrained.parameters() if p.requires_grad))

print("试运行".center(50, "="))
pretrained.to(device)
out = pretrained(
    input_ids      = input_ids,
    attention_mask = attention_mask,
    token_type_ids = token_type_ids,
)
print(f"{out.last_hidden_state.shape=}")
运行结果:
=====================加载预训练模型======================
模型参数: 102267648
=======================试运行========================
out.last_hidden_state.shape=torch.Size([16, 30, 768])
```

定义下游任务模型:
```python
class Model(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.fc = torch.nn.Linear(768, 2)
    
    def forward(self, input_ids, attention_mask, token_type_ids):
        with torch.no_grad():
            out = pretrained(
                input_ids      = input_ids,
                attention_mask = attention_mask,
                token_type_ids = token_type_ids,
            )
        out = self.fc(out.last_hidden_state[:,0])
        out = out.softmax(dim=1)
        return out
    
model = Model().to(device)
print("定义下游模型".center(50, "="))
print("模型可训练参数:", sum(p.numel() for p in model.parameters() if p.requires_grad))

print("试运行".center(50, "="))
out = model(input_ids, attention_mask, token_type_ids)
print(f"{out.shape=}")
运行结果:
======================定义下游模型======================
模型可训练参数: 1538
=======================试运行========================
out.shape=torch.Size([8, 2])
```

注意: 此处的特征选择第 1 个词对应的特征向量, 对应编码结果的 `[CLS]`.

此处之所以只使用第一个词的特征进行后续任务, 这与 BERT 的训练方法有关, 参阅 [Ch14.手动实现BERT](Ch14.手动实现BERT.md).

## 9.3.训练与测试

训练代码如下:
```python
def train():
    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4)
    criterion = torch.nn.CrossEntropyLoss()
    scheduler = get_scheduler(
        name               = "linear",
        num_warmup_steps   = 0,
        num_training_steps = len(loader),
        optimizer          = optimizer,
    )
    model.train()

    for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(loader):
        out = model(input_ids, attention_mask, token_type_ids)
        loss = criterion(out, labels)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        scheduler.step()
        if i % 20 == 0:
            out = out.argmax(dim=1)
            accuracy = (out == labels).sum().item() / len(labels)
            lr = optimizer.state_dict()['param_groups'][0]['lr']
            print(f"Step={i:3d} loss={loss.item():6.4e} accuracy={accuracy*100:>6.2f}% {lr=:6.4e}")

print("训练".center(50, "="))
train()
```

训练过程:
```
========================训练========================
Step=  0 loss=7.9643e-01 accuracy= 37.50% lr=4.9950e-04
Step= 20 loss=4.2383e-01 accuracy=100.00% lr=4.8950e-04
Step= 40 loss=4.7196e-01 accuracy= 87.50% lr=4.7950e-04
Step= 60 loss=3.3282e-01 accuracy=100.00% lr=4.6950e-04
Step= 80 loss=3.5214e-01 accuracy=100.00% lr=4.5950e-04
Step=100 loss=5.4041e-01 accuracy= 87.50% lr=4.4950e-04
Step=120 loss=5.4353e-01 accuracy= 62.50% lr=4.3950e-04
Step=140 loss=4.1782e-01 accuracy= 87.50% lr=4.2950e-04
Step=160 loss=3.4400e-01 accuracy=100.00% lr=4.1950e-04
Step=180 loss=4.3956e-01 accuracy= 87.50% lr=4.0950e-04
Step=200 loss=4.8552e-01 accuracy= 75.00% lr=3.9950e-04
Step=220 loss=4.5481e-01 accuracy=100.00% lr=3.8950e-04
Step=240 loss=3.1621e-01 accuracy=100.00% lr=3.7950e-04
Step=260 loss=4.8273e-01 accuracy= 75.00% lr=3.6950e-04
Step=280 loss=4.2328e-01 accuracy=100.00% lr=3.5950e-04
Step=300 loss=4.3405e-01 accuracy= 87.50% lr=3.4950e-04
Step=320 loss=3.2600e-01 accuracy=100.00% lr=3.3950e-04
Step=340 loss=3.3160e-01 accuracy=100.00% lr=3.2950e-04
Step=360 loss=3.8996e-01 accuracy= 87.50% lr=3.1950e-04
Step=380 loss=3.8844e-01 accuracy=100.00% lr=3.0950e-04
Step=400 loss=3.6529e-01 accuracy=100.00% lr=2.9950e-04
Step=420 loss=4.9560e-01 accuracy= 87.50% lr=2.8950e-04
Step=440 loss=4.9633e-01 accuracy= 87.50% lr=2.7950e-04
Step=460 loss=3.1931e-01 accuracy=100.00% lr=2.6950e-04
Step=480 loss=3.1865e-01 accuracy=100.00% lr=2.5950e-04
Step=500 loss=5.1855e-01 accuracy= 75.00% lr=2.4950e-04
Step=520 loss=3.6378e-01 accuracy=100.00% lr=2.3950e-04
Step=540 loss=4.4013e-01 accuracy= 87.50% lr=2.2950e-04
Step=560 loss=3.2378e-01 accuracy=100.00% lr=2.1950e-04
Step=580 loss=4.8616e-01 accuracy= 75.00% lr=2.0950e-04
Step=600 loss=3.1574e-01 accuracy=100.00% lr=1.9950e-04
Step=620 loss=4.5746e-01 accuracy= 87.50% lr=1.8950e-04
Step=640 loss=4.1350e-01 accuracy= 87.50% lr=1.7950e-04
Step=660 loss=5.0576e-01 accuracy= 75.00% lr=1.6950e-04
Step=680 loss=4.5429e-01 accuracy= 87.50% lr=1.5950e-04
Step=700 loss=5.6000e-01 accuracy= 75.00% lr=1.4950e-04
Step=720 loss=4.8624e-01 accuracy= 75.00% lr=1.3950e-04
Step=740 loss=3.1870e-01 accuracy=100.00% lr=1.2950e-04
Step=760 loss=3.5559e-01 accuracy=100.00% lr=1.1950e-04
Step=780 loss=5.8373e-01 accuracy= 62.50% lr=1.0950e-04
Step=800 loss=3.8880e-01 accuracy= 87.50% lr=9.9500e-05
Step=820 loss=5.0176e-01 accuracy= 75.00% lr=8.9500e-05
Step=840 loss=3.1456e-01 accuracy=100.00% lr=7.9500e-05
Step=860 loss=4.1514e-01 accuracy= 87.50% lr=6.9500e-05
Step=880 loss=3.3541e-01 accuracy=100.00% lr=5.9500e-05
Step=900 loss=3.8147e-01 accuracy= 87.50% lr=4.9500e-05
Step=920 loss=3.2249e-01 accuracy=100.00% lr=3.9500e-05
Step=940 loss=5.3220e-01 accuracy= 75.00% lr=2.9500e-05
Step=960 loss=5.1475e-01 accuracy= 75.00% lr=1.9500e-05
Step=980 loss=4.1872e-01 accuracy= 87.50% lr=9.5000e-06
```

测试代码如下:
```python
def test():
    test_loader = torch.utils.data.DataLoader(
        dataset     = PairDataset('test'),
        batch_size  = 32,
        collate_fn  = collate_fn,
        shuffle     = True,
        drop_last   = True)
    model.eval()
    correct = 0
    total = 0
    for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(test_loader):
        with torch.no_grad():
            out = model(input_ids, attention_mask, token_type_ids)
        out = out.argmax(dim=1)
        correct += (out == labels).sum().item()
        total += len(labels)

    accuracy = correct / total
    print(f"accuracy={accuracy*100:4.2f}%")

print("测试".center(50, "="))
test()
```

测试结果:
```python
========================测试========================
accuracy=88.44%
```

## 总结

本章通过中文句子关系推断任务讲解了如何使用预训练 BERT 模型抽取句子对的数据特征.
句子关系推断也是 BERT 模型本身在训练时的一个子任务, 所以使用 BERT 模型能够有效地解决句子关系推断任务.