# 第07章 实战：中文情感分类

[代码文件](PythonFiles/Ch07.EmotionClassification.py)

注意: 本章内容经过重排

分类任务是大多数机器学习任务中最基础的, 对于自然语言处理也不例外.
本章将讲解一个情感分类的自然语言处理任务.

本章采用 `ChnSentiCorp` 情感分类数据集, 每条数据包括一句对书籍, 酒店, 计算机配件等商品的购物评价, 以及一个表明好评差评的标识.
对于人类而言, 不给予标识也能大致判断出好评还是差评;
对于神经网络, 也通过这个任务来验证其有效性.

在 BERT, GPT, Transformers 模型被提出之前, 被广泛使用的自然语言处理网络结构是 RNN.
RNN 的主要功能是将自然语言的句子抽取成特征向量, 之后将特征向量输入到全连接神经网络进行分类或回归.
从这个角度看 RNN 将自然语言处理任务转换成全连接神经网络任务.
对于这类能够将抽象数据类型转换成具体特征向量的网络层被称为 Backbone. 译为特征抽取层.
自 BERT, GPT, Transformers 模型被提出之后, 它们被广泛应用于任务中的 Backbone.

相对于 Backbone, 后续处理的神经网络被称为下游任务模型, 往往会根据 Backbone 输出的特征向量进行再计算, 得到想要的计算结果, 如分类或回归.

对于应用了预训练 Backbone 的网络, 训练时可以选择继续训练 Backbone 层, 也可以不训练.
不训练能够达到业务需求也可以节省训练 Backbone 所需的大量计算资源.

## 7.1.数据集准备

定义设备:
```python
device = 'cpu'
if torch.cuda.is_available():
    device = 'cuda'
print("使用设备：", device)
```

定义数据集:
```python
class ClassificationDataset(Dataset):

    def __init__(self, split):
        data_dir = 'D:\Speech\_Datasets\Seamew_ChnSentiCorp'
        self.dataset = load_dataset(data_dir)[split]
    
    def __len__(self):
        return len(self.dataset)
    
    def __getitem__(self, index):
        text  = self.dataset[index]['text']
        label = self.dataset[index]['label']
        return text, label

print("加载数据集".center(50, "="))
train_dataset = ClassificationDataset(split='train')
print("训练集大小：", len(train_dataset))
print("训练样本示例：", train_dataset[20])
test_dataset = ClassificationDataset(split='test')
print("测试集大小：", len(test_dataset))
print("测试样本示例：", test_dataset[20])
运行结果:
======================加载数据集=======================
训练集大小： 9600
训练样本示例： ('非常不错，服务很好，位于市中心区，交通方便，不过价格也高！', 1)
测试集大小： 1200
测试样本示例： ('3999的时候抢购的 运气真好 这个价格还有什么号说的 品牌和价格都很不错', 0)
```

定义整理函数:
```python
# %% 加载分词器
print("加载分词器".center(50, "="))
tokenizer: BertTokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
print(tokenizer)

# %% 定义数据整理函数
print("定义数据整理函数".center(50, "="))
def collate_fn(data):
    sentences = [i[0] for i in data]
    labels = [i[1] for i in data]
    data = tokenizer.batch_encode_plus(
        batch_text_or_text_pairs = sentences,
        truncation               = True,
        padding                  = 'max_length',
        return_length            = True,
        return_tensors           = 'pt'
    )
    input_ids = data['input_ids']
    attention_mask = data['attention_mask']
    token_type_ids = data['token_type_ids']
    labels = torch.LongTensor(labels)
    input_ids = input_ids.to(device)
    attention_mask = attention_mask.to(device)
    token_type_ids = token_type_ids.to(device)
    labels = labels.to(device)
    return input_ids, attention_mask, token_type_ids, labels

print("模拟数据整理运行".center(50, "="))
data = [
    ('你站在桥上看风景', 1),
    ('看风景的人在楼上看你', 0),
    ('明月装饰了你的窗子', 1),
    ('你装饰了别人的梦', 0),
]
input_ids, attention_mask, token_type_ids, labels = collate_fn(data)
print("input_ids 大小：", input_ids.size())
print("attention_mask 大小：", attention_mask.size())
print("token_type_ids 大小：", token_type_ids.size())
print("labels：", labels)
运行结果:
======================加载分词器=======================
BertTokenizer(name_or_path='bert-base-chinese', vocab_size=21128, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={
	0: AddedToken("[PAD]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	100: AddedToken("[UNK]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	101: AddedToken("[CLS]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	102: AddedToken("[SEP]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	103: AddedToken("[MASK]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
=====================定义数据整理函数=====================
=====================模拟数据整理运行=====================
input_ids 大小： torch.Size([4, 512])
attention_mask 大小： torch.Size([4, 512])
token_type_ids 大小： torch.Size([4, 512])
labels： tensor([1, 0, 1, 0], device='cuda:0')
```

注意到 `bert-base-chinese` 预训练模型的最大输入长度为 512, 且词表大小为 21128.

定义的整理函数将数据截断为 500 的长度, 且不足的将被填充.

定义数据加载器:
```python
loader = DataLoader(
    dataset    = train_dataset,
    batch_size = 16,
    collate_fn = collate_fn,
    shuffle    = True,
    drop_last  = True
)
print("数据加载器：", len(loader))

print("查看数据示例".center(50, "="))
for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(loader):
    break
print("input_ids 大小：", input_ids.size())
print("attention_mask 大小：", attention_mask.size())
print("token_type_ids 大小：", token_type_ids.size())
print("labels：", labels)
运行结果:
数据加载器： 600
======================查看数据示例======================
input_ids 大小： torch.Size([16, 512])
attention_mask 大小： torch.Size([16, 512])
token_type_ids 大小： torch.Size([16, 512])
labels： tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0], device='cuda:0')
```

## 7.2.模型定义

加载预训练模型
```python
print("加载预训练模型".center(50, "="))
pretrained = BertModel.from_pretrained('bert-base-chinese')
pretrained.to(device)
print("模型参数", sum(p.numel() for p in pretrained.parameters()) / 10000, "万")

for param in pretrained.parameters():
    param.requires_grad = False
print("模型运行".center(50, "="))
out = pretrained(
    input_ids   = input_ids,
    attention_mask = attention_mask,
    token_type_ids = token_type_ids
)
print(out.last_hidden_state.shape)
运行结果:
=====================加载预训练模型======================
模型参数 10226.7648 万
=======================模型运行=======================
torch.Size([16, 512, 768])
```

可以看到预训练模型参数量为一亿左右, 训练的代价相对高昂, 本章不对其进行训练.
示例输入模型后会得到一个 `BaseModelOutputWithPoolingAndCrossAttentions` 对象, 其中包括:
- last_hidden_state: 输入序列的每个词的隐藏状态, 维度为 `[batch_size, sequence_length, hidden_size]`
- pooler_output: 分类任务的输出, 维度为 `[batch_size, hidden_size]`

只关心 last_hidden_state 输出, 即 16 句话, 每句话包含 500 个词, 每个词被编码为 768 维的向量.
这就是用于下游模型的特征向量.

定义下游模型:
```python
class Model(nn.Module):

    def __init__(self):
        super().__init__()
        self.fc = nn.Linear(768, 2)
    
    def forward(self, input_ids, attention_mask, token_type_ids):
        with torch.no_grad():
            out = pretrained(
                input_ids   = input_ids,
                attention_mask = attention_mask,
                token_type_ids = token_type_ids)
        out = self.fc(out.last_hidden_state[:, 0])
        out = out.softmax(dim=1)
        return out

print("定义模型".center(50, "="))
model = Model()
model.to(device)
print("模型参数", sum(p.numel() for p in model.parameters()))

print("模型运行".center(50, "="))
out = model(
    input_ids   = input_ids,
    attention_mask = attention_mask,
    token_type_ids = token_type_ids)
print("模型输出尺寸:", out.shape)
运行结果:
=======================定义模型=======================
模型参数 0.1538 万
可训练参数 1538
=======================模型运行=======================
模型输出尺寸: torch.Size([16, 2])
```

下游模型只选择了特征向量第二个轴, 词轴的第一个元素作为分类的输入, 尺寸为 `16×768`.
用一个全连接线性层, 尺寸为 `768×2`, 将 768 维转化为 2 维度的类别输出, 对应编码结果的 `[CLS]`.

注: 此处之所以只使用第一个词的特征进行后续任务, 这与 BERT 的训练方法有关, 参阅 [Ch14.手动实现BERT](Ch14.手动实现BERT.md).

## 7.3.训练与测试

训练代码如下:
```python
def train():
    optimizer = AdamW(model.parameters(), lr=5e-4)
    criterion = nn.CrossEntropyLoss()
    scheduler = get_scheduler(
        name               = 'linear',
        num_warmup_steps   = 0,
        num_training_steps = len(loader),
        optimizer          = optimizer)
    model.train()
    for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(loader):
        out = model(
            input_ids   = input_ids,
            attention_mask = attention_mask,
            token_type_ids = token_type_ids)
        loss = criterion(out, labels)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        scheduler.step()
        if i % 10 == 0:
            out = out.argmax(dim=1)
            accuracy = (out == labels).sum().item() / len(labels)
            lr = optimizer.state_dict()['param_groups'][0]['lr']
            print(f"第{i}步，损失：{loss.item():.4f}, 准确率：{accuracy:.4f}, 学习率：{lr:.6f}")

train()
```

需要注意的是, 由于任务为分类任务, 所以采用交叉熵损失函数. 优化器选用 AdamW, 学习率为 5e-4, 用线性学习率调度器进行控制.

训练过程:
```
第  0步，损失：0.6720, 准确率：0.6875, 学习率：0.000499
第 10步，损失：0.6532, 准确率：0.6250, 学习率：0.000491
第 20步，损失：0.6002, 准确率：0.8750, 学习率：0.000483
第 30步，损失：0.5611, 准确率：0.8125, 学习率：0.000474
第 40步，损失：0.6126, 准确率：0.6875, 学习率：0.000466
第 50步，损失：0.6154, 准确率：0.6875, 学习率：0.000458
第 60步，损失：0.4747, 准确率：0.8750, 学习率：0.000449
第 70步，损失：0.4655, 准确率：0.9375, 学习率：0.000441
第 80步，损失：0.4951, 准确率：0.8125, 学习率：0.000432
第 90步，损失：0.4839, 准确率：0.8750, 学习率：0.000424
第100步，损失：0.5196, 准确率：0.8750, 学习率：0.000416
第110步，损失：0.5493, 准确率：0.7500, 学习率：0.000407
第120步，损失：0.5507, 准确率：0.7500, 学习率：0.000399
第130步，损失：0.5125, 准确率：0.8125, 学习率：0.000391
第140步，损失：0.5047, 准确率：0.7500, 学习率：0.000383
第150步，损失：0.5202, 准确率：0.8125, 学习率：0.000374
第160步，损失：0.5202, 准确率：0.8125, 学习率：0.000366
第170步，损失：0.4798, 准确率：0.8125, 学习率：0.000358
第180步，损失：0.4543, 准确率：0.8750, 学习率：0.000349
第190步，损失：0.4984, 准确率：0.8750, 学习率：0.000341
第200步，损失：0.4234, 准确率：0.9375, 学习率：0.000333
第210步，损失：0.4147, 准确率：1.0000, 学习率：0.000324
第220步，损失：0.5210, 准确率：0.7500, 学习率：0.000316
第230步，损失：0.4627, 准确率：0.9375, 学习率：0.000307
第240步，损失：0.4420, 准确率：0.9375, 学习率：0.000299
第250步，损失：0.4787, 准确率：0.9375, 学习率：0.000291
第260步，损失：0.4747, 准确率：0.9375, 学习率：0.000282
第270步，损失：0.4473, 准确率：0.9375, 学习率：0.000274
第280步，损失：0.3647, 准确率：1.0000, 学习率：0.000266
第290步，损失：0.5277, 准确率：0.7500, 学习率：0.000258
第300步，损失：0.4130, 准确率：0.9375, 学习率：0.000249
第310步，损失：0.4507, 准确率：0.9375, 学习率：0.000241
第320步，损失：0.4478, 准确率：0.8750, 学习率：0.000233
第330步，损失：0.3558, 准确率：1.0000, 学习率：0.000224
第340步，损失：0.3630, 准确率：1.0000, 学习率：0.000216
第350步，损失：0.4399, 准确率：0.8750, 学习率：0.000208
第360步，损失：0.4891, 准确率：0.8125, 学习率：0.000199
第370步，损失：0.4045, 准确率：0.9375, 学习率：0.000191
第380步，损失：0.4100, 准确率：0.9375, 学习率：0.000182
第390步，损失：0.4876, 准确率：0.8125, 学习率：0.000174
第400步，损失：0.4057, 准确率：0.8750, 学习率：0.000166
第410步，损失：0.4502, 准确率：0.8750, 学习率：0.000158
第420步，损失：0.4748, 准确率：0.8750, 学习率：0.000149
第430步，损失：0.5783, 准确率：0.6875, 学习率：0.000141
第440步，损失：0.6367, 准确率：0.5625, 学习率：0.000133
第450步，损失：0.5184, 准确率：0.7500, 学习率：0.000124
第460步，损失：0.5192, 准确率：0.8125, 学习率：0.000116
第470步，损失：0.4242, 准确率：0.8750, 学习率：0.000107
第480步，损失：0.4438, 准确率：0.9375, 学习率：0.000099
第490步，损失：0.4995, 准确率：0.8125, 学习率：0.000091
第500步，损失：0.3699, 准确率：1.0000, 学习率：0.000082
第510步，损失：0.5141, 准确率：0.7500, 学习率：0.000074
第520步，损失：0.3977, 准确率：0.9375, 学习率：0.000066
第530步，损失：0.3738, 准确率：1.0000, 学习率：0.000058
第540步，损失：0.3919, 准确率：0.9375, 学习率：0.000049
第550步，损失：0.4754, 准确率：0.8750, 学习率：0.000041
第560步，损失：0.4910, 准确率：0.8125, 学习率：0.000033
第570步，损失：0.3687, 准确率：1.0000, 学习率：0.000024
第580步，损失：0.3846, 准确率：1.0000, 学习率：0.000016
第590步，损失：0.4135, 准确率：0.8750, 学习率：0.000008
```

测试代码如下:
```python
def test():
    loader_test = DataLoader(
        dataset    = test_dataset,
        batch_size = 32,
        collate_fn = collate_fn,
        shuffle    = True,
        drop_last  = True)
    model.eval()
    correct = 0
    total = 0
    for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(loader_test):
        with torch.no_grad():
            out = model(
                input_ids   = input_ids,
                attention_mask = attention_mask,
                token_type_ids = token_type_ids)
        out = out.argmax(dim=1)
        correct += (out == labels).sum().item()
        total += len(labels)
    accuracy = correct / total
    print(f"测试集准确率：{accuracy:.4f}")

test()
```

测试结果 (总共 36 个批次):
```
测试集准确率：0.8818
```

## 总结

本章通过一个情感分类的例子讲解了使用 BERT 预训练模型抽取文本特征数据的方法, 使用 BERT 作为 Backbone, 相对传统 RNN 而言计算量会更大一些, 但 BERT 抽取的信息更完整, 更容易被下游任务模型总结出统计规律, 所以在使用 BERT 作为 Backbone 时可以适当减少下游任务模型的训练量.
此外由于采用的是预训练的 BERT 模型, 可以不训练该部分, 在保证基本效果的同时节约了计算量.