# 第03章.使用数据集工具

[代码文件](PythonFiles/Ch03.Datasets.py)

## 3.1.数据集工具介绍

在以往的自然语言处理任务中会花费大量的时间在数据处理上, 针对不同的数据集往往需要不同的处理过程, 各个数据集的格式差异大, 处理起来复杂又容易出错.

针对以上问题, HuggingFace 提供了统一的数据集处理工具, 让开发者在处理各种不同的数据集时可以通过统一的 API 处理, 大大降低了数据处理的工作量.

登录 [HuggingFace 官网](https://huggingface.co/), 点击顶部菜单的 [Datasets](https://huggingface.co/datasets), 就能看到 HuggingFace 提供的数据集.

界面的左侧可以根据不同的任务类型, 语言, 体积, 使用许可来筛选数据集.
界面的右侧为具体的数据集列表, 其中有经典的 `glue`, `super_glue` 数据集, 问答数据集 `squad`, 情感分类数据集 `imdb`, 纯文本数据集 `wikitext`.

单击具体的某个数据集, 进入数据集的详情页面, 可以看到数据集的概要信息.
以 [`glue` 数据集](https://huggingface.co/datasets/nyu-mll/glue)为例, 在详情页可以看到 `glue` 各个数据子集的概要内容, 每个数据子集的下方可能会有作者写的说明信息.

*PS: 本书重点关注中文数据集. 为了简单起见, 本书只会用几个简单的数据集来完成后续的实战任务.*

## 3.2.使用数据集工具

### 3.2.1.数据集加载和保存

#### 1.在线加载数据集

使用 HuggingFace 数据集工具加载数据往往只需要一行代码.
以加载名为 [`seamew/ChnSentiCorp` 数据集](https://huggingface.co/datasets/seamew/ChnSentiCorp/) 为例, 代码如下:
```python
from datasets import load_dataset

dataset = load_dataset(path='seamew/ChnSentiCorp')
print(dataset)
运行结果:
Generating train split: 9600 examples [00:00, 1621378.69 examples/s]
Generating validation split: 1200 examples [00:00, 601909.21 examples/s]
Generating test split: 1200 examples [00:00, 1203530.56 examples/s]
DatasetDict({
    train: Dataset({
        features: ['label', 'text'],
        num_rows: 9600
    })
    validation: Dataset({
        features: ['label', 'text'],
        num_rows: 1200
    })
    test: Dataset({
        features: ['label', 'text'],
        num_rows: 1200
    })
})
```

注意: 由于 HuggingFace 把数据集存储在谷歌云盘上, 在国内加载时可能会遇到网络问题, 所以本书的配套资源中已经提供了保存好的数据文件, 使用 `load_from_disk()` 函数加载即可.

考虑使用 hf.mirror 镜像下载.

可以看到, 要加载一个数据集是很简单的, 使用 `load_dataset()` 函数, 把数据集的名字作为参数传入即可.

观察运行结果, 可以发现该数据集可以分为三个部分: 训练集, 验证集, 测试集.
每条数据拥有两个字段: 文本 text 和标签 label.
还可以看到三个部分分别的数据量: 训练集 9600 条, 验证集 1200 条, 测试集 1200 条.

此外, `load_dataset()` 函数还有一些其他参数.
例如加载 `glue` 数据集的数据子集 `name='sst2'`, 使用 `split='train'` 加载训练集部分.
```python
dataset = load_dataset('glue', name='sst2', split='train')
运行结果:
Downloading readme: 35.3kB [00:00, 35.4MB/s]
HF google storage unreachable. Downloading and preparing it from source
Downloading data: 100%|██████████| 3.11M/3.11M [00:01<00:00, 1.57MB/s]
Downloading data: 100%|██████████| 72.8k/72.8k [00:00<00:00, 87.3kB/s]
Downloading data: 100%|██████████| 148k/148k [00:00<00:00, 169kB/s]
Generating train split: 100%|██████████| 67349/67349 [00:00<00:00, 835642.26 examples/s]
Generating validation split: 100%|██████████| 872/872 [00:00<00:00, 437649.05 examples/s]
Generating test split: 100%|██████████| 1821/1821 [00:00<00:00, 913615.74 examples/s]
Dataset({
    features: ['sentence', 'label', 'idx'],
    num_rows: 67349
})
```

可以看到 `glue` 数据集的 `sst2` 数据子集拥有 67,349 条数据, 每条数据有三个字段: 文本 sentence, 标签 label, 索引 idx.

#### 2.保存到本地磁盘

加载了数据集后, 可以使用 `save_to_disk()` 函数将数据集保存到本地磁盘.

```python
dataset.save_to_disk("./Datasets/glue_sst2_train")
运行结果:
Saving the dataset (1/1 shards): 100%|██████████| 67349/67349 [00:00<00:00, 3295444.19 examples/s]
```

#### 3.从本地磁盘加载数据集

使用 `load_from_disk()` 函数从本地磁盘加载数据集.
注意: 此方法只能加载之前使用 `save_to_disk()` 保存到本地磁盘的数据集.
```python
dataset = load_from_disk("./Datasets/glue_sst2_train")
print(dataset)
运行结果:
Dataset({
    features: ['sentence', 'label', 'idx'],
    num_rows: 67349
})
```

### 3.2.2.数据集基本操作

#### 1.取出数据部分
```python
train_dataset = dataset['train']
print(train_dataset)
运行结果:
for i in [12, 17, 20, 26, 56]:
    print(train_dataset[i])
```

#### 2.查看数据内容
```python
for i in [12, 17, 20, 26, 56]:
    print(train_dataset[i])
运行结果:
{'label': 1, 'text': '轻便，方便携带，性能也不错，能满足平时的工作需要，对出差人员来说非常不错'}
{'label': 0, 'text': '很好的地理位置，一蹋糊涂的服务，萧条的酒店。'}
{'label': 1, 'text': '非常不错，服务很好，位于市中心区，交通方便，不过价格也高！'}
{'label': 0, 'text': '跟住招待所没什么太大区别。 绝对不会再住第2次的酒店！'}
{'label': 0, 'text': '价格太高，性价比不够好。我觉得今后还是去其他酒店比较好。'}
```

可以看出数据的内容是一份购物和消费评论数据, 字段 `text` 表示消费者的评论, 字段 `label` 表示好评还是差评.

#### 3.数据排序

可以使用 `sort()` 函数让数据按照某个字段排序.
代码如下:
```python
print('排序前'.center(50, '-'))
print(train_dataset['label'][:10])
sorted_dataset = train_dataset.sort('label')
print('排序后'.center(50, '-'))
print(sorted_dataset['label'][:10])
print(sorted_dataset['label'][-10:])
运行结果:
-----------------------排序前------------------------
[1, 1, 0, 0, 1, 0, 0, 0, 1, 1]
-----------------------排序后------------------------
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
```

#### 4.数据打乱

对应的可以使用 `shuffle()` 函数再次打乱数据.
代码如下:
```python
print('打乱数据'.center(50, '-'))
shuffled_dataset = sorted_dataset.shuffle()
print(shuffled_dataset['label'][:10])
运行结果:
-----------------------打乱数据-----------------------
[0, 0, 1, 1, 0, 0, 1, 0, 0, 1]
```

#### 5.数据抽样

使用 `select()` 函数从数据集中选择某些数据.
代码如下:
```python
selected_dataset = train_dataset.select([0,10,20,30,40,50])
print(selected_dataset)
运行结果:
Dataset({
    features: ['label', 'text'],
    num_rows: 6
})
```

选择出的数据会再次组装成一个数据子集, 使用这种方法可以实现数据抽样.

#### 6.数据过滤

使用 `filter()` 函数可以按照自定义规则过滤数据.
代码如下:
```python
def f(data):
    return data['text'].startswith('非常不错')

filtered_dataset = train_dataset.filter(f)
print(filtered_dataset)
运行结果:
Filter: 100%|██████████| 9600/9600 [00:00<00:00, 267561.42 examples/s]
Dataset({
    features: ['label', 'text'],
    num_rows: 13
})
```

`filter()` 函数接受一个函数作为参数, 在该函数中确定过滤数据的条件.

#### 7.数据集拆分

使用 `train_test_split()` 函数将数据集切分为训练集和测试集.
代码如下:
```python
split_dataset= train_dataset.train_test_split(test_size=0.1)
print(split_dataset)
运行结果:
DatasetDict({
    train: Dataset({
        features: ['label', 'text'],
        num_rows: 8640
    })
    test: Dataset({
        features: ['label', 'text'],
        num_rows: 960
    })
})
```

训练集和测试集的比例为 9:1.

#### 8.数据分桶

使用 `shard()` 函数把数据均匀分为 `n` 个部分.
代码如下:
```python
bucket_dataset = train_dataset.shard(num_shards=4, index=0)
print(bucket_dataset)
运行结果:
Dataset({
    features: ['label', 'text'],
    num_rows: 2400
})
```

参数解释:
- `num_shards`: 切分的份数
- `index`: 表示取出第几份数据

#### 9.字段重命名

使用 `rename_column()` 函数重命名字段.
代码如下:
```python
renamed_dataset = train_dataset.rename_column('text', 'text_rename')
print(renamed_dataset)
运行结果:
Dataset({
    features: ['label', 'text_rename'],
    num_rows: 9600
})
```

#### 10.字段删除

使用 `remove_columns()` 函数删除字段.
代码如下:
```python
removed_dataset = train_dataset.remove_columns('text')
print(removed_dataset)
Dataset({
    features: ['label'],
    num_rows: 9600
})
```

#### 11.映射函数

使用 `map()` 函数遍历数据, 并且对每条数据都进行修改.
代码如下:
```python
def f(data):
    data['text'] = 'My sentence:' + data["text"]
    return data
mapped_dataset = train_dataset.map(f)
print(train_dataset['text'][20])
print(mapped_dataset['text'][20])
运行结果:
Map: 100%|██████████| 9600/9600 [00:00<00:00, 44940.07 examples/s]
非常不错，服务很好，位于市中心区，交通方便，不过价格也高！
My sentence:非常不错，服务很好，位于市中心区，交通方便，不过价格也高！
```

`map()` 函数以一个函数作为入参, 在函数中定义对数据的修改, 也可以进行增加字段, 删除字段, 修改数据格式等操作.

#### 12.批处理加速

使用过滤和映射这类操作时需要使用函数遍历数据集的方法, 可以使用批处理减少函数调用的次数, 从而达到加速处理的目的.
默认情况下是不使用批处理的, 由于每条数据都需要调用一次函数, 所以函数调用次数等于数据集行数.
使用批处理能够一批一批地处理数据, 减少函数调用次数.
代码如下:
```python
def f(data):
    text = data['text']
    text = ['My sentence:' + i for i in text]
    data['text'] = text
    return data

mapped_dataset2 = train_dataset.map(
    function   = f,
    batched    = True,
    batch_size = 1000,
    num_proc   = 1)
print(mapped_dataset2['text'][20])
运行结果:
Map: 100%|██████████| 9600/9600 [00:00<00:00, 326407.63 examples/s]
My sentence:非常不错，服务很好，位于市中心区，交通方便，不过价格也高！
```

参数解释:
- `batched`: 是否使用批处理
- `batch_size`: 批处理的大小, 通常 1000 是个合适的数值.
- `num_proc`: 线程数 (#TODO 实验发现超过 1 会出错 ?), 一般设置为 CPU 核心数.

#### 13.数据格式设置

使用 `set_format()` 函数设置数据格式.
代码如下:
```python
train_dataset.set_format(
    type               = 'numpy',
    columns            = ['label'],
    output_all_columns = True
)
print(train_dataset[20])
运行结果:
{'label': 1, 'text': '非常不错，服务很好，位于市中心区，交通方便，不过价格也高！'}
```

参数解释:
- `type`: 数据格式, 目前支持 `numpy`, `pandas`, `torch`, `tensorflow` 等.
- `columns`: 指定需要修改的字段.
- `output_all_columns`: 是否保留其他字段.

注意: `set_format()` 函数无返回值.

### 3.2.3.保存为其他格式

#### 1.保存为 CSV 文件

```
train_dataset.to_csv(path_or_buf='./Datasets/train.csv')
csv_dataset = load_dataset(path='csv', data_files='./Datasets/train.csv', split='train')
print(csv_dataset[20])
运行结果:
Creating CSV from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 289.38ba/s]
Generating train split: 9600 examples [00:00, 265198.27 examples/s]
{'label': 1, 'text': '非常不错，服务很好，位于市中心区，交通方便，不过价格也高！'}
```

(现在是否还有?) 有的文件会多出一个 `Unnamed` 字段, 表示数据序号. 如果不想要这一列可以自行删除.

#### 2.保存为 JSON 文件

```
train_dataset.to_json(path_or_buf='./Datasets/train.json')
json_dataset = load_dataset(path='json', data_files='./Datasets/train.json', split='train')
print(json_dataset[20])
运行结果:
Creating json from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 267.47ba/s]
Generating train split: 9600 examples [00:00, 740920.39 examples/s]
{'label': 1, 'text': '非常不错，服务很好，位于市中心区，交通方便，不过价格也高！'}
```

Json 不存在多列的问题.

## 总结

本章讲解了 HuggingFace 数据集工具的使用, 包括数据集加载, 保存, 查看, 排序, 抽样, 过滤, 拆分, 映射, 列重命名等操作.