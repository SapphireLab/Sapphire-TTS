# 第04章.使用评价指标工具

[代码文件](PythonFiles/Ch04.Metrics.py)

## 4.1.介绍

在训练和测试一个模型时往往需要计算不同的评价指标, 如正确率, 查准率, 查全率, F1 值等等, 具体的指标往往和处理的数据集, 任务类型有关.
HuggingFace 提供了统一的评价指标工具, 能够将具体地计算过程隐藏, 调用者只需提供计算结果, 由评价指标工具给出结果.

## 4.2.使用

### 4.2.1.列出可用指标

`list_metrics()` 函数获取可用的评价指标列表.
代码如下:
```python
from datasets import list_metrics

metrics_list = list_metrics()
print(f"{len(metrics_list)=}")
print(metrics_list[:10])
运行结果:
<ipython-input-1-57908563e8de>:4: FutureWarning: list_metrics is deprecated and will be removed in the next major version of datasets. Use 'evaluate.list_evaluation_modules' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metrics_list = list_metrics()
len(metrics_list)=243
['accuracy', 'bertscore', 'bleu', 'bleurt', 'brier_score', 'cer', 'character', 'charcut_mt', 'chrf', 'code_eval']
```

后续要使用其他方法替代掉 (? 指标数量不一致).

```python
!pip install evaluate
from evaluate import list_evaluation_modules

metrics_list = list_evaluation_modules()
print(f"{len(metrics_list)=}")
print(metrics_list[:10])
运行结果:
len(metrics_list)=167
['lvwerra/test', 'jordyvl/ece', 'angelina-wang/directional_bias_amplification', 'cpllab/syntaxgym', 'lvwerra/bary_score', 'hack/test_metric', 'yzha/ctc_eval', 'codeparrot/apps_metric', 'mfumanelli/geometric_mean', 'daiyizheng/valid'
```

### 4.2.2.加载指标

使用 `load_metric()` 函数加载一个评价指标.
评价指标往往和对应的数据集配套使用, 此处以 `glue` 数据集的 `mrpc` 子集为例.
代码如下:
```python
from datasets import load_metric
metric = load_metric(path='glue', config_name='mrpc')
print(metric)
运行结果:
<ipython-input-4-32635ea1b629>:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. 
Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
    metric = load_metric(path='glue', config_name='mrpc')
~\site-packages\datasets\load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.
  warnings.warn(
Downloading builder script: 5.76kB [00:00, 5.79MB/s]                   
Metric(name: "glue", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: """
Compute GLUE evaluation metric associated to each GLUE dataset.
Args:
    predictions: list of predictions to score.
        Each translation should be tokenized into a list of tokens.
    references: list of lists of references for each translation.
        Each reference should be tokenized into a list of tokens.
Returns: depending on the GLUE subset, one or several of:
    "accuracy": Accuracy
    "f1": F1 score
    "pearson": Pearson Correlation
    "spearmanr": Spearman Correlation
    "matthews_correlation": Matthew Correlation
Examples:

    >>> glue_metric = datasets.load_metric('glue', 'sst2')  # 'sst2' or any of ["mnli", "mnli_mismatched", "mnli_matched", "qnli", "rte", "wnli", "hans"]
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0}

    >>> glue_metric = datasets.load_metric('glue', 'mrpc')  # 'mrpc' or 'qqp'
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0, 'f1': 1.0}

    >>> glue_metric = datasets.load_metric('glue', 'stsb')
    >>> references = [0., 1., 2., 3., 4., 5.]
    >>> predictions = [0., 1., 2., 3., 4., 5.]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print({"pearson": round(results["pearson"], 2), "spearmanr": round(results["spearmanr"], 2)})
    {'pearson': 1.0, 'spearmanr': 1.0}

    >>> glue_metric = datasets.load_metric('glue', 'cola')
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'matthews_correlation': 1.0}
""", stored examples: 0)
```

后续将替换为
```python
from evaluate import load
metric = load(path='glue', config_name='mrpc')
print(metric)
运行结果:
Downloading builder script: 100%|██████████| 5.75k/5.75k [00:00<?, ?B/s]
EvaluationModule(name: "glue", module_type: "metric", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: """
Compute GLUE evaluation metric associated to each GLUE dataset.
Args:
    predictions: list of predictions to score.
        Each translation should be tokenized into a list of tokens.
    references: list of lists of references for each translation.
        Each reference should be tokenized into a list of tokens.
Returns: depending on the GLUE subset, one or several of:
    "accuracy": Accuracy
    "f1": F1 score
    "pearson": Pearson Correlation
    "spearmanr": Spearman Correlation
    "matthews_correlation": Matthew Correlation
Examples:

    >>> glue_metric = evaluate.load('glue', 'sst2')  # 'sst2' or any of ["mnli", "mnli_mismatched", "mnli_matched", "qnli", "rte", "wnli", "hans"]
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0}

    >>> glue_metric = evaluate.load('glue', 'mrpc')  # 'mrpc' or 'qqp'
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0, 'f1': 1.0}

    >>> glue_metric = evaluate.load('glue', 'stsb')
    >>> references = [0., 1., 2., 3., 4., 5.]
    >>> predictions = [0., 1., 2., 3., 4., 5.]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print({"pearson": round(results["pearson"], 2), "spearmanr": round(results["spearmanr"], 2)})
    {'pearson': 1.0, 'spearmanr': 1.0}

    >>> glue_metric = evaluate.load('glue', 'cola')
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'matthews_correlation': 1.0}
""", stored examples: 0)
```

注意: 不是每个数据集都有对应的评价指标, 在实际使用时以满足需要为准则选择合适的评价指标即可.

### 4.2.3.获取说明

```python
print(metric.inputs_description)
```
和上面的输出一致, 包括了对评价指标的介绍, 要求输入格式的说明, 输出指标的说明, 以及部分示例代码.

### 4.2.4.计算指标

按照上面的示例代码, 可以实际计算出评价指标.
代码如下:
```python
predictions = [0,1,0]
references  = [0,1,1]
metric.compute(predictions=predictions, references=references)
运行结果:
{'accuracy': 0.6666666666666666, 'f1': 0.6666666666666666}
```

该指标的输出包括了准确率和 F1 分值.

## 总结

本章讲解了 HuggingFace 评价指标工具的使用, 在实际使用时评价指标工具往往和训练工具一起使用, 能够随着训练步骤进行, 同时监控评价指标, 以确定模型确实正向着一个理想的目标进步.