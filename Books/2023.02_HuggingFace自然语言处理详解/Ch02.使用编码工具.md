# 第02章 使用编码工具

## 2.1.编码工具简介

HuggingFace 提供了一套统一的编码 API, 由每个模型各自提交实现.
由于统一了 API, 所以调用者能够快速地使用不同模型的编码工具.

在学习 HuggingFace 的编码工具之前, 先看一个示例的编码过程, 以理解编码工具的工作过程.

## 2.2.编码工具工作流示意

### 1.定义字典

文字是一个抽象的概念, 不是计算机擅长处理的数据单元, 计算机擅长处理的是数字运算, 所以需要把抽象的文字转换为数字, 让计算机能够做数学运算.

为了把抽象的文字数字化, 需要一个字典把文字或者词对应到某个数字.

例如下方示例的字典, 只有 11 个词, 而在实际项目中字典可能会有成千上万个词.

```python
vocab = {
    '<SOS>': 0, '<EOS>': 1, 'the'  : 2, 'quick': 3, 'brown': 4, 'fox'  : 5,
    'jumps': 6, 'over' : 7, 'a'    : 8, 'lazy' : 9, 'dog'  : 10,
}
```

### 2.句子预处理

在句子被分词之前, 一般会对句子进行一些特殊的操作, 例如把太长的句子截短, 或在句子中添加首尾标识符等.

在示例字典中, 可以注意到除了一般的词之外, 还有一些特殊符号, 例如 `<SOS>` 和 `<EOS>`, 它们分别代表一个句子的开头和结束. 把这两个特殊符号添加到句子上.
代码如下:

```python
sentence = 'the quick brown fox jumps over a lazy dog'
sentence = '<SOS>'+ sentence +'<EOS>'
print(sentence)
运行结果: <SOS> the quick brown fox jumps over a lazy dog<EOS>
```

### 3.分词

现在句子准备好了, 接下来需要把句子分成一个一个的词.
对于中文来说, 这是个复杂的问题, 但是对于英文来说, 这个问题比较容易解决, 因为英文有自然的分词方式, 即以空格来分词.
代码如下:
```python
words = sentence.split()
print(words)
运行结果: ['<SOS>', 'the', 'quick', 'brown', 'fox', 'jumps', 'over', 'a', 'lazy', 'dog', '<EOS>']
```

可以看到, 这个英文的句子已经分成了比较理想的一个一个的单词.

对于中文, 分词的问题比较复杂, 因为中文所有的字是连在一起写的, 不存在一个自然的分隔符号.
有很多成熟的工具能够做中文分词, 例如 Jieba 分词, LTP 分词等, 但是在本书中不会使用这些工具, 因为 HuggingFace 的编码工具已经包括了分词这一步工作, 由各个模型自行实现, 对于调用者来讲这些工作是透明的, 不需要关心具体的实现细节.

### 4.编码

句子已经按照要求添加了首尾标识符, 并且分割成了一个一个单词, 现在需要把这些抽象的单词映射为数字.
因为已经定义好了字典, 所以使用字典就可以把每个单词分别映射为数字.
代码如下:
```python
encode = [vocab[word] for word in words]
print(encode)
运行结果: [0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1]
```

以上是一个示例的编码的工作流程, 经历了定义字典, 句子预处理, 分词, 编码四个步骤.

## 2.3.使用编码工具

经过以上示例, 可以知道编码的过程中要经历哪些工作步骤.
下面介绍如何使用 HuggingFace 提供的编码工具.

### 1.加载编码工具

首先需要加载一个编码工具, 此处使用 `BERT-Base-Chinese` 实现.
代码如下:
```python
from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained(
    pretrained_model_name_or_path='bert-base-chinese',
    cache_dir=None,
    force_download=False,)
```

参数解释:
- `pretrained_model_name_or_path`: 指定要加载的编码工具, 大多数模型会把自己提交的编码工具命名为和模型一样的名字.
  模型和编码工具通常是成对使用的, 不会出现对应错误的情况, 建议调用者也遵循习惯, 成对使用.
- `cache_dir`: 指定编码工具的缓存路径, 默认值为 None, 也可以使用自定义路径.
- `force_download`: 为 True 时, 无论是否有本地缓存, 都会强制重新下载模型, 默认为 False.

### 2.准备实验数据

现在有了一个编码工具, 准备一些句子, 以测试编码工具.
代码如下:
```python
sentences = [
    '你站在桥上看风景',
    '看风景的人在楼上看你',
    '明月装饰了你的窗子',
    '你装饰了别人的梦',
]
```
这是卞之琳的 "断章", 后面会用这些句子进行实验.

### 3.基本编码函数

首先从一个基本的编码方法开始.
代码如下:
```python
out = tokenizer.encode(
    text               = sentences[0],
    text_pair          = sentences[1],
    truncation         = True,
    padding            = 'max_length',
    add_special_tokens = True,
    max_length         = 25,
    return_tensors     = None,
)

print(out)
print(tokenizer.decode(out))
运行结果: 
[101, 872, 4991, 1762, 3441, 677, 4692, 7599, 3250, 102, 4692, 7599, 3250, 4638, 782, 1762, 3517, 677, 4692, 872, 102, 0, 0, 0, 0]
[CLS] 你 站 在 桥 上 看 风 景 [SEP] 看 风 景 的 人 在 楼 上 看 你 [SEP] [PAD] [PAD] [PAD] [PAD]
```

这里调用了编码工具的 `encode()` 函数, 这是最基本的编码函数, 一次编码一个或一对句子, 在这个例子中编码了一对句子.

不是所有编码工具都有编码一对句子的功能, 具体取决于不同模型的实现.
在 BERT 中一般会编码一对句子, 这和 BERT 的训练方式有关系, 具体参阅 [第 14 章 手动实现 BERT](Ch14.手动实现BERT.md)

参数解释:
- `text` 和 `text_pair` 分别为两个句子, 如果只想编码一个句子, 则可以让 `text_pair` 传入 `None`.
- `truncation`: 取 True 时表明当句子长度大于 `max_length` 时, 截断句子.
- `padding`: 取 `max_length` 时表明在长度不足 `max_length` 时, 在句子后面补充 `PAD` 直到 `max_length` 长度.
- `add_special_tokens`: 取 True 时, 表明需要在句子中添加特殊符号.
- `max_length`: 定义 `max_length` 的长度.
- `return_tensors`: 取 `None` 时, 表明返回的数据类型为列表, 也可以赋值为 tf, pt, np, 分别表示 TensorFlow, PyTorch, NumPy 数据格式.

查看运行结果可以看到编码的输出为一个数字的列表, 这里使用了编码工具的 `decode()` 函数把这个列表还原为分词前的句子. 这样就可以看出编码工具对句子做了哪些预处理工作.

从输出可以看出, 编码工具把两个句子前后拼接在一起, 中间使用 `[SEP]` 符号分隔, 在整个句子头部添加符号 `[CLS]`, 在整个句子尾部添加符号 `[SEP]`, 因为句子长度不足 `max_length`, 所以在句子后面补充了四个 `[PAD]` 符号.

另外从空格的情况也能看出, 编码工具把每个字作为一个词. 因为每个字之间都有空格, 表明它们是不同的词, 所以在 BERT 的实现中, 中文分词处理比较简单, 就是把每个字都作为一个词来处理.

### 4.进阶编码函数

完成了上述的基础编码函数后, 现在看一个稍微复杂的编码函数.
代码如下:
```python
out = tokenizer.encode_plus(
    text                       = sentences[0],
    text_pair                  = sentences[1],
    truncation                 = True,
    padding                    = 'max_length',
    add_special_tokens         = True,
    max_length                 = 25,
    return_tensors             = None,
    return_token_type_ids      = True,
    return_attention_mask      = True,
    return_special_tokens_mask = True,
    return_length              = True,
)

for k, v in out.items():
    print(k, ':',  v)
print(tokenizer.decode(out['input_ids']))
运行结果: 
input_ids : [101, 872, 4991, 1762, 3441, 677, 4692, 7599, 3250, 102, 4692, 7599, 3250, 4638, 782, 1762, 3517, 677, 4692, 872, 102, 0, 0, 0, 0]
token_type_ids : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]
special_tokens_mask : [1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]
attention_mask : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]
length : 25
[CLS] 你 站 在 桥 上 看 风 景 [SEP] 看 风 景 的 人 在 楼 上 看 你 [SEP] [PAD] [PAD] [PAD] [PAD]
```

和之前不同, 此处调用了 `encode_plus()` 函数, 这是一个进阶版的编码函数, 它会返回更加复杂的编码结果.
和 `encode()` 函数一样, 它也可以编码一个句子或一对句子.

参数解释:
- `return_token_type_ids`: 第一个句子和特殊符号的位置为 0, 第二个句子的位置为 1.
- `return_attention_mask`: PAD 的位置是 0, 其他位置是 1.
- `return_special_tokens_mask`: 特殊符号的位置是 1, 其他位置是 0.
- `return_length`: 句子的长度.

查看运行结果, 最后一行将编码结果中的 `input_ids` 还原为文字形式, 可以看到经过预处理的文本, 内容和 `encode()` 函数的输出一致.

这次编码的结果和 `encode()` 函数不一样的地方在于这次返回的不是一个简单的列表, 而是四个列表和一个数字.

输出解释:
- `input_ids`: 编码后的词.
- `token_type_ids`: 用于表明编码结果中哪些位置是第一个句子, 哪些位置是第二个句子. 其他位置为 0.
- `special_tokens_mask`: 用于表明编码结果中哪些位置是特殊符号. 特殊符号包括 `[CLS]`, `[SEP]`, `[PAD]`, 等.
- `attention_mask`: 用于表明编码结果中哪些位置是 PAD, 哪些位置是有效的编码结果.
- `length`: 编码结果的长度.

### 5.批量编码函数

以上介绍的函数都是一次编码一对或一个句子, 在实际工程中需要处理的数据往往是成千上万的, 为了提高效率, 可以使用 `batch_encode_plus()` 函数批量地进行输出处理.

代码如下:
```python
out = tokenizer.batch_encode_plus(
    batch_text_or_text_pairs   = [(sentences[0], sentences[1]), (sentences[2], sentences[3])],
    truncation                 = True,
    padding                    = 'max_length',
    add_special_tokens         = True,
    max_length                 = 25,
    return_tensors             = None,
    return_token_type_ids      = True,
    return_attention_mask      = True,
    return_special_tokens_mask = True,
    return_length              = True,
    # return_offsets_mapping     = True
)
for k, v in out.items():
    print(k, ':',  v)
print(tokenizer.decode(out['input_ids'][0]))
运行结果:
input_ids : [[101, 872, 4991, 1762, 3441, 677, 4692, 7599, 3250, 102, 4692, 7599, 3250, 4638, 782, 1762, 3517, 677, 4692, 872, 102, 0, 0, 0, 0], [101, 3209, 3299, 6163, 7652, 749, 872, 4638, 4970, 2094, 102, 872, 6163, 7652, 749, 1166, 782, 4638, 3457, 102, 0, 0, 0, 0, 0]]
token_type_ids : [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]]
special_tokens_mask : [[1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]]
length : [21, 20]
attention_mask : [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]]
[CLS] 你 站 在 桥 上 看 风 景 [SEP] 看 风 景 的 人 在 楼 上 看 你 [SEP] [PAD] [PAD] [PAD] [PAD]
```

参数解释:
- `batch_text_or_text_pairs`: 一个列表, 包含一对或多个句子.
- `return_offsets_mapping`: 取 True 时, 标识每个词地起止位置, 只能在 `BERTTokenizerFast` 使用.

可以看到输出都是二维的列表, 表明这是一个批量编码.
该函数在后续将会经常用到.

### 6.对字典的操作

下面介绍对编码工具的字典进行操作.

查看字典:
```python
vocab = tokenizer.get_vocab()
print(f"{type(vocab)=}, {len(vocab)=}, {'明月' in vocab=}")
运行结果: 
type(vocab)=<class 'dict'>, len(vocab)=21128, '明月' in vocab=False
```

可以看到字典本身是个 Dict 类型的数据.
在 BERT 字典中一共有 21,128 个词, 并且没有包含 '明月' 这个词.

添加新词:
```python
tokenizer.add_tokens(new_tokens=['明月', '装饰', '窗子'])
```

添加特殊符号:
```python
tokenizer.add_special_tokens({'eos_token': '[EOS]'})
```

然后调用 `encode()` 部分的代码, 结果如下:
```python
out = tokenizer.encode(
    text               = sentences[2] + '[EOS]', 
    text_pair          = None,
    truncation         = True,
    padding            = 'max_length',
    add_special_tokens = True,
    max_length         = 10,
    return_tensors     = None,
)

print(out)
print(tokenizer.decode(out))
运行结果:
[101, 21128, 21129, 749, 872, 4638, 21130, 21131, 102, 0]
[CLS] 明月 装饰 了 你 的 窗子 [EOS] [SEP] [PAD]
```

可以看到`明月`, `装饰`, `窗子`和 `[EOS]` 分别编码为 21128, 21129, 21130, 21131, 被识别为单独的词, 而不是单个字.

## 总结

本章讲解了编码的工作流程, 分为定义字典→句子预处理→分词→编码等步骤;
使用 HuggingFace 编码工具的基本编码函数和批量编码函数, 并对编码结果进行了解读;
查看了 HuggingFace 编码工具的字典, 并且能向字典添加新词.
