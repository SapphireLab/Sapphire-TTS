# 第05章.使用管道工具

[代码文件](PythonFiles/Ch05.Pipeline.py)

## 5.1.介绍

HuggingFace 有一个巨大的模型库, 其中一些是已经非常成熟的经典模型, 这些模型即使不进行任何训练也能直接得出比较好的预测结果, 也就是常说的 Zero-Shot Learning.

使用管道工具时, 调用者需要做的就是告知管道工具需要进行的任务类型, 管道工具会自动分配合适的模型, 直接给出预测结果.
如果预测结果对于调用者已经可以满足需求, 则不再需要进行训练.

管道工具的 API 非常简洁, 隐藏了大量复杂的底层代码, 即使是非专业人员也能轻松使用.

## 5.2.使用

### 5.2.1.常见任务演示

#### 1.文本分类

使用管道工具处理文本分类任务.
代码如下:

```python
from transformers import pipeline

classifier = pipeline("sentiment-analysis")
result = classifier("I love you")
print(result)
result = classifier("I hate you")
print(result)
运行结果:
No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://hf-mirror.com/distilbert/distilbert-base-uncased-finetuned-sst-2-english).
Using a pipeline without specifying a model name and revision in production is not recommended.
[{'label': 'POSITIVE', 'score': 0.9998656511306763}]
[{'label': 'NEGATIVE', 'score': 0.9991129040718079}]
```

默认模型 `distilbert/distilbert-base-uncased-finetuned-sst-2-english` 255 MB.

该代码预测了 `I love you` 和 `I hate you` 的情感分类, 分别为 `POSITIVE` 和 `NEGATIVE`, 且分数都非常高.

#### 2.阅读理解

使用管道工具处理阅读理解任务.
代码如下:
```python
question_answerer = pipeline("question-answering")
context =r"""
Extractive Question Answering is the task of extracting an answer from a text given a question.
An example of a question answering dataset is the SQuAD dataset, which is entirely based on that task.
If you would like to fine-tune a model on a SQuAD task, you may leverage the examples/PyTorch/question-answering/run_squad.py script.
"""
result = question_answerer(question="What is extractive question answering?", context=context)
print(result)
result = question_answerer(question="What is a good example of a question answering dataset?", context=context)
print(result)
运行结果:
No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 626af31 (https://hf-mirror.com/distilbert/distilbert-base-cased-distilled-squad).
Using a pipeline without specifying a model name and revision in production is not recommended.
{'score': 0.6149139404296875, 'start': 34, 'end': 95, 'answer': 'the task of extracting an answer from a text given a question'}
{'score': 0.5172930955886841, 'start': 147, 'end': 160, 'answer': 'SQuAD dataset'}
```

默认模型 `distilbert/distilbert-base-cased-distilled-squad` 248 MB.

注意: 问题的答案必须在 `context` 中出现, 因为模型的计算过程是从 `context` 中找出问题的答案.

#### 3.完形填空

使用管道工具处理完形填空任务.
代码如下:
```python
unmasker = pipeline("fill-mask")
sentence = "HuggingFace is creating a <mask> that the community uses to solve NLP tasks."
result = unmasker(sentence)
print(result)
运行结果:
Some weights of the model checkpoint at distilbert/distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[{'score': 0.17927460372447968, 'token': 3944, 'token_str': ' tool', 'sequence': 'HuggingFace is creating a tool that the community uses to solve NLP tasks.'}, 
{'score': 0.11349380016326904, 'token': 7208, 'token_str': ' framework', 'sequence': 'HuggingFace is creating a framework that the community uses to solve NLP tasks.'}, 
{'score': 0.052435602992773056, 'token': 5560, 'token_str': ' library', 'sequence': 'HuggingFace is creating a library that the community uses to solve NLP tasks.'}, 
{'score': 0.03493543714284897, 'token': 8503, 'token_str': ' database', 'sequence': 'HuggingFace is creating a database that the community uses to solve NLP tasks.'}, 
{'score': 0.02860250696539879, 'token': 17715, 'token_str': ' prototype', 'sequence': 'HuggingFace is creating a prototype that the community uses to solve NLP tasks.'}]
```

默认模型 `distilbert/distilroberta-base were` 315 MB.

#### 4.文本生成

使用管道工具处理文本生成任务.
代码如下:
```python
text_generator = pipeline("text-generation")
context = "As far as I am concerned, I will"
result = text_generator(context, max_length=50, do_sample=False)
print(result)
运行结果:
No model was supplied, defaulted to openai-community/gpt2 and revision 6c0e608 (https://hf-mirror.com/openai-community/gpt2).
Using a pipeline without specifying a model name and revision in production is not recommended.
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[{'generated_text': 'As far as I am concerned, I will be the first to admit that I am not a fan of the idea of a "free market." I think that the idea of a free market is a bit of a stretch. I think that the idea'}]
```

默认模型 `openai-community/gpt2` 522 MB.

#### 5.命名实体识别

使用管道工具处理命名实体识别任务, 即找出一段文本中的人名, 地名, 组织机构名等.
代码如下:
```python
ner_pipe = pipeline("ner")
sequence = \
"""
Hugging Face Inc. is a company based in New York City. 
Its headquarters are in DUMBO,
therefore very close to the Manhattan Bridge which is visible from the window.
"""
for entity in ner_pipe(sequence):
    print(entity)
运行结果:
No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://hf-mirror.com/dbmdz/bert-large-cased-finetuned-conll03-english).
Using a pipeline without specifying a model name and revision in production is not recommended.
Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'entity': 'I-ORG', 'score': 0.99957865, 'index': 1, 'word': 'Hu', 'start': 1, 'end': 3}
{'entity': 'I-ORG', 'score': 0.9909764, 'index': 2, 'word': '##gging', 'start': 3, 'end': 8}
{'entity': 'I-ORG', 'score': 0.9982224, 'index': 3, 'word': 'Face', 'start': 9, 'end': 13}
{'entity': 'I-ORG', 'score': 0.9994879, 'index': 4, 'word': 'Inc', 'start': 14, 'end': 17}
{'entity': 'I-LOC', 'score': 0.9994344, 'index': 11, 'word': 'New', 'start': 41, 'end': 44}
{'entity': 'I-LOC', 'score': 0.99931955, 'index': 12, 'word': 'York', 'start': 45, 'end': 49}
{'entity': 'I-LOC', 'score': 0.9993794, 'index': 13, 'word': 'City', 'start': 50, 'end': 54}
{'entity': 'I-LOC', 'score': 0.98625815, 'index': 19, 'word': 'D', 'start': 81, 'end': 82}
{'entity': 'I-LOC', 'score': 0.951427, 'index': 20, 'word': '##UM', 'start': 82, 'end': 84}
{'entity': 'I-LOC', 'score': 0.9336589, 'index': 21, 'word': '##BO', 'start': 84, 'end': 86}
{'entity': 'I-LOC', 'score': 0.9761654, 'index': 28, 'word': 'Manhattan', 'start': 116, 'end': 125}
{'entity': 'I-LOC', 'score': 0.9914629, 'index': 29, 'word': 'Bridge', 'start': 126, 'end': 132}
```

默认模型 `dbmdz/bert-large-cased-finetuned-conll03-english` 1.24 GB.

#### 6.文本摘要

使用管道工具处理文本摘要任务.
代码如下:
```python
summarizer = pipeline("summarization")
article = \
"""
New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.
A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband. 
Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared "I do" five more times, sometimes only within two weeks of each other.
In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her "first and only" marriage.
Barrientos, now 39, is facing two criminal counts of "offering a false instrument for filing in the first degree," referring to her false statements on the 2010 marriage license application, according to court documents.
Prosecutors said the marriages were part of an immigration scam.
On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, <NAME>, who declined to comment further.
After leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective <NAME>, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.
All occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.
Prosecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.
Any divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.
The case was referred to the Bronx District Attorney's Office by Immigration and Customs Enforcement and the Department of Homeland Security's Investigation Division. Seven of the men are from so-called "red-flagged" countries, including Egypt, Turkey, Georgia, Pakistan and Mali.
Investigation Division. Seven of the men are from so-called "red-flagged" countries, including Egypt, Turkey, Georgia, Pakistan and Mali.
Her eighth marriage was finalized after an administrative dispute with her second husband, who was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.
If convicted, Barrientos faces up to four years in prison. 
Her next court appearance is scheduled for May 18.
"""
result = summarizer(article, max_length=130, min_length=30, do_sample=False)
print(result)
运行结果:
No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://hf-mirror.com/sshleifer/distilbart-cnn-12-6).
Using a pipeline without specifying a model name and revision in production is not recommended.
c:\Users\Sapphire\.conda\envs\cuda12\Lib\site-packages\huggingface_hub\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\Users\Sapphire\.cache\huggingface\hub\models--sshleifer--distilbart-cnn-12-6. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.
To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development
  warnings.warn(message)
[{'summary_text': ' Liana Barrientos, 39, pleaded not guilty to two criminal counts of "offering a false instrument for filing in the first degree" She is believed to still be married to four men, and at one time, she was married to eight men at once .'}]
```

默认模型 `sshleifer/distilbart-cnn-12-6` 1.13 GB.


#### 7.翻译

使用管道工具处理文本翻译任务.
代码如下:
```python
translator = pipeline("translation_en_to_de")
sentence = "Hugging Face is a technology company based in New York and Paris."
result = translator(sentence, max_length=40)
print(result)
运行结果:
No model was supplied, defaulted to google-t5/t5-base and revision 686f1db (https://hf-mirror.com/google-t5/t5-base).
Using a pipeline without specifying a model name and revision in production is not recommended.
~\huggingface_hub\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\Users\Sapphire\.cache\huggingface\hub\models--google-t5--t5-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.
~\transformers\models\t5\tokenization_t5_fast.py:171: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.
For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.
- Be aware that you SHOULD NOT rely on google-t5/t5-base automatically truncating your input to 512 when padding/encoding.
- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.
- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.
  warnings.warn(
[{'translation_text': 'Hugging Face ist ein Technologieunternehmen mit Sitz in New York und Paris.'}]
```

默认模型 `google-t5/t5-base` 850 MB.

### 5.2.2.替换模型执行任务

管理工具会根据不同的任务自动分配一个模型, 如果模型不是调用者想要使用的, 可以指定具体模型.

此处执行默认工具不支持的中译英任务, 需要替换模型.
英译中过程类似.

```python
# !pip install sentencepiece
from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained("Helsinki-NLP/opus-mt-zh-en")
model = AutoModelForSeq2SeqLM.from_pretrained("Helsinki-NLP/opus-mt-zh-en")

translator = pipeline("translation", model=model, tokenizer=tokenizer)
sentence = "我叫萨拉, 我住在伦敦."
result = translator(sentence, max_length=20)
print(result)
运行结果:
[{'translation_text': 'My name is Sarah, and I live in London.'}]

tokenizer = AutoTokenizer.from_pretrained("Helsinki-NLP/opus-mt-en-zh")
model = AutoModelForSeq2SeqLM.from_pretrained("Helsinki-NLP/opus-mt-en-zh")

translator = pipeline("translation", model=model, tokenizer=tokenizer)
sentence = "My name is Sarah and I live in London."
result = translator(sentence, max_length=20)
print(result)
运行结果:
[{'translation_text': '我叫莎拉,我住在伦敦'}]
```

默认模型 `Helsinki-NLP/opus-mt-zh-en` 297 MB.
默认模型 `Helsinki-NLP/opus-mt-en-zh` 297 MB.

## 总结

本章讲解了 HuggingFace 管道工具的使用, 该工具使用非常简单, 同时也能实现非常强大的功能, 如果对预测的结果要求不高, 则可以免于再训练的繁琐步骤.