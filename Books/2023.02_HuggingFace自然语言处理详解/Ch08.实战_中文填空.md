# 第08章.实战：中文填空

[代码文件](PythonFiles/Ch08.Unmasker.py)

注意: 本章内容经过重排

人类在阅读一个句子时, 即时挖掉句子中的一两个词, 往往也能根据上下文猜出被挖掉的词是什么, 这被称为填空任务.
人类可以通过从小到大的每日听说读写交流获得的普遍性知识猜测出符合上下文语义的被去掉的字.
自然语言虽然复杂, 但有着明显的统计规律, 而神经网络擅长的就是找出统计规律, 所以本章尝试使用预训练神经网络完成填空任务.

本章使用的数据集为 `ChnSentiCorp` 情感分类数据集, 每条数据包括一句对书籍, 酒店, 计算机配件等商品的购物评价, 以及一个表明好评差评的标识.
填空任务只需要文本不需要类别标识.

在数据处理时将会把每句话的第 15 个词去掉, 替换为特殊符号 `[MASK]`, 并且每句话会被截断成固定的三十个词的长度, 神经网络根据每句话的上下文预测去掉的词.

本任务仍然使用预训练的 BERT 模型 `bert-base-chinese` 作为 Backbone 部分以提取文本数据特征, 后续接入下游任务模型来把抽取的数据特征还原为任务需要的答案. 由于答案可能是任何一个词, 所以可以视为一个多分类任务, 类别数为词表大小.

注: 同样不进行 Backbone 部分的训练.

## 8.1.数据集准备

定义设备:
```python
device = "cuda" if torch.cuda.is_available() else "cpu"
```

定义数据集:
```python
tokenizer: BertTokenizer = BertTokenizer.from_pretrained("bert-base-chinese")
print("加载分词器".center(50, "="))
print(tokenizer)
运行结果:
======================加载分词器=======================
BertTokenizer(name_or_path='bert-base-chinese', vocab_size=21128, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={
	0: AddedToken("[PAD]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	100: AddedToken("[UNK]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	101: AddedToken("[CLS]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	102: AddedToken("[SEP]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	103: AddedToken("[MASK]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
```

编码数据集:
```python
def encode(data):
    out = tokenizer.batch_encode_plus(
        batch_text_or_text_pairs = data["text"],
        truncation               = True,
        padding                  = "max_length",
        max_length               = 30,
        return_length            = True)
    return out

Encoded_Dataset = Dataset.map(
    function       = encode,
    batched        = True,
    batch_size     = 1000,
    num_proc       = 1,
    remove_columns = ["text", "label"],
)

print("编码数据集".center(50, "="))
print(Encoded_Dataset)
运行结果:
======================加载数据集=======================
DatasetDict({
    train: Dataset({
        features: ['label', 'text'],
        num_rows: 9600
    })
    validation: Dataset({
        features: ['label', 'text'],
        num_rows: 1200
    })
    test: Dataset({
        features: ['label', 'text'],
        num_rows: 1200
    })
})
```

过滤数据集:
```python
def filter_dataset(data):
    return [i >= 30 for i in data["length"]]

Filtered_Dataset = Encoded_Dataset.filter(
    function   = filter_dataset,
    batched    = True,
    batch_size = 1000,
    num_proc   = 1,
)

print("过滤数据集".center(50, "="))
print(Filtered_Dataset)
运行结果:
======================过滤数据集=======================
DatasetDict({
    train: Dataset({
        features: ['input_ids', 'token_type_ids', 'length', 'attention_mask'],
        num_rows: 9286
    })
    validation: Dataset({
        features: ['input_ids', 'token_type_ids', 'length', 'attention_mask'],
        num_rows: 1158
    })
    test: Dataset({
        features: ['input_ids', 'token_type_ids', 'length', 'attention_mask'],
        num_rows: 1157
    })
})
```

定义整理函数:
```python
def collate_fn(data):
    input_ids = [i['input_ids'] for i in data]
    attention_mask = [i['attention_mask'] for i in data]
    token_type_ids = [i['token_type_ids'] for i in data]

    input_ids = torch.LongTensor(input_ids)
    attention_mask = torch.LongTensor(attention_mask)
    token_type_ids = torch.LongTensor(token_type_ids)

    # 将第十五位替换为 MASK
    labels = input_ids[:,15].reshape(-1).clone()
    input_ids[:,15] = tokenizer.get_vocab()[tokenizer.mask_token]

    input_ids = input_ids.to(device)
    attention_mask = attention_mask.to(device)
    token_type_ids = token_type_ids.to(device)
    labels = labels.to(device)
    return input_ids, attention_mask, token_type_ids, labels

print("定义整理函数".center(50, "="))

tmp_data = [{
    "input_ids": [
        101, 2769, 3221, 3791, 6427, 1159, 2110, 5442, 117, 2110, 749, 8409,
        702, 6440, 3198, 4638, 1159, 5277, 4408, 119, 1728, 711, 2769, 3221,
        5439, 2399, 782, 117, 3791, 102,
    ],
    "token_type_ids": [0] * 30,
    "attention_mask": [1] * 30,
    },
    {
    "input_ids": [
        101, 679, 7231, 8024, 2376, 3301, 1351, 6848, 4638, 8024, 3301, 1351,
        3683, 6772, 4007, 2692, 8024, 2218, 3221, 100, 2970, 1366, 2208, 749,
        8024, 5445, 684, 1059, 3221, 102,
    ],
    "token_type_ids": [0] * 30,
    "attention_mask": [1] * 30,
    },
]

print("试运行".center(50, "="))
input_ids, attention_mask, token_type_ids, labels = collate_fn(tmp_data)
print(tokenizer.decode(input_ids[0]))
print(tokenizer.decode(labels[0]))
print(f"{input_ids.shape=}")
print(f"{attention_mask.shape=}")
print(f"{token_type_ids.shape=}")
print(f"{labels=}")
运行结果:
======================定义整理函数======================
=======================试运行========================
[CLS] 我 是 法 语 初 学 者, 学 了 78 个 课 时 [MASK] 初 级 班. 因 为 我 是 老 年 人, 法 [SEP]
的
input_ids.shape=torch.Size([2, 30])
attention_mask.shape=torch.Size([2, 30])
token_type_ids.shape=torch.Size([2, 30])
labels=tensor([4638, 2692], device='cuda:0')
```

注意: 
- 在编码数据时会丢弃掉原数据的 `text` 和 `label` 数据, 只需要编码后的结果.
- 整理数据时将第十五位的编码结果作为标签, 并将替换为 `[MASK]` 符号后的原编码结果作为数据.

定义数据加载器:
```python
loader = torch.utils.data.DataLoader(
    dataset     = Filtered_Dataset['train'],
    batch_size  = 16,
    collate_fn  = collate_fn,
    shuffle     = True,
    drop_last   = True,
)
print("定义数据加载器".center(50, "="))
print(f"{len(loader)=}")
print("查看数据样例".center(50, "="))
for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(loader):
    break
print(tokenizer.decode(input_ids[0]))
print(tokenizer.decode(labels[0]))
print(f"{input_ids.shape=}")
print(f"{attention_mask.shape=}")
print(f"{token_type_ids.shape=}")
print(f"{labels=}")
运行结果:
=====================定义数据加载器======================
len(loader)=580
======================查看数据样例======================
[CLS] 散 热 做 的 挺 好 材 质 尚 可 配 货 迅 速 [MASK] 且 赶 上 了 价 格 保 护 属 于 性 价 比 [SEP]
而
input_ids.shape=torch.Size([16, 30])
attention_mask.shape=torch.Size([16, 30])
token_type_ids.shape=torch.Size([16, 30])
labels=tensor([5445, 4692, 3766, 3018, 3221, 8024, 8024, 3683, 1469,  117, 1091, 1217,
         852, 3680, 4638,  739], device='cuda:0')
```

## 8.2.模型定义

定义预训练模型:
```python
pretrained = BertModel.from_pretrained("bert-base-chinese")
print("加载预训练模型".center(50, "="))
print("模型参数:", sum(p.numel() for p in pretrained.parameters()))
for name, param in pretrained.named_parameters():
    param.requires_grad_(False)
print("模型可训练参数:", sum(p.numel() for p in pretrained.parameters() if p.requires_grad))

print("试运行".center(50, "="))
pretrained.to(device)
out = pretrained(
    input_ids      = input_ids,
    attention_mask = attention_mask,
    token_type_ids = token_type_ids,
)
print(f"{out.last_hidden_state.shape=}")
运行结果:
=====================加载预训练模型======================
模型参数: 102267648
=======================试运行========================
out.last_hidden_state.shape=torch.Size([16, 30, 768])
```

定义下游任务模型:
```python
class Model(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.decoder = torch.nn.Linear(768, out_features=tokenizer.vocab_size, bias=False)
        self.bias = torch.nn.Parameter(torch.zeros(tokenizer.vocab_size))
        self.decoder.bias = self.bias
        self.Dropout = torch.nn.Dropout(0.5)
    
    def forward(self, input_ids, attention_mask, token_type_ids):
        with torch.no_grad():
            out = pretrained(
                input_ids      = input_ids,
                attention_mask = attention_mask,
                token_type_ids = token_type_ids,
            )
        out = self.Dropout(out.last_hidden_state[:,15])
        out = self.decoder(out)
        return out
    
model = Model().to(device)
print("定义下游模型".center(50, "="))
print("模型可训练参数:", sum(p.numel() for p in model.parameters() if p.requires_grad))

print("试运行".center(50, "="))
out = model(input_ids, attention_mask, token_type_ids)
print(f"{out.shape=}")
运行结果:
======================定义下游模型======================
模型可训练参数: 16247432
=======================试运行========================
out.shape=torch.Size([16, 21128])
```

注意这个下游任务模型, 只包含一个全连接的线性层, 尺寸为 768×21128, 即将特征还原为词表中的某个字.
只选择第 15 个词的位置的特征作为输入, 再将特征投影到整个词表空间中.

由于 768×21128 是一个巨大的矩阵, 很容易出现过拟合, 因此引入一个随机失活以防止过拟合.
预测的结果经过 Softmax 将得到每个词的概率分布, 然后取最大的作为预测结果.
注意训练时**不建议加上 Softmax 层**, 因为分类类别太多, 很多类别对应的值非常低, 经过 Softmax 后会接近零, 这在计算参数梯度时会出现问题, 导致梯度小时, 不利于模型的训练和收敛.

## 8.3.训练与测试

训练代码如下:
```python
def train():
    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1.0)
    criterion = torch.nn.CrossEntropyLoss()
    scheduler = get_scheduler(
        name               = "linear",
        num_warmup_steps   = 0,
        num_training_steps = len(loader) * 5,
        optimizer          = optimizer,
    )
    model.train()
    for epoch in range(5):
        for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(loader):
            out = model(input_ids, attention_mask, token_type_ids)
            loss = criterion(out, labels)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            scheduler.step()
            if i % 50 == 0:
                out = out.argmax(dim=1)
                accuracy = (out == labels).sum().item() / len(labels)
                lr = optimizer.state_dict()['param_groups'][0]['lr']
                print(f"{epoch=} {i=:3d} loss={loss.item():6.4e} accuracy={accuracy*100:>4.2f}% {lr=:6.4e}")

print("训练".center(50, "="))
train()
```

训练时为了防止过大的线性层出现过拟合, 所以在优化过程中加入权重参数二范数衰减, 即 `weight_decay`, 其作用原理为:

$$
    loss = loss + weight\_decay\cdot norm(W)
$$

加入了权重参数二范数能够约束矩阵中数值偏离 0 的绝对值, 防止绝对值过大的参数出现, 从而防止出现过拟合.

注: Adam 在本例的效果相当差于 AdamW.

训练过程:
```
========================训练========================
epoch=0 i=  0 loss=1.0139e+01 accuracy=0.00% lr=4.9983e-04
epoch=0 i= 50 loss=8.5318e+00 accuracy=12.50% lr=4.9121e-04
epoch=0 i=100 loss=6.8244e+00 accuracy=18.75% lr=4.8259e-04
epoch=0 i=150 loss=5.7718e+00 accuracy=25.00% lr=4.7397e-04
epoch=0 i=200 loss=6.1649e+00 accuracy=25.00% lr=4.6534e-04
epoch=0 i=250 loss=4.8266e+00 accuracy=25.00% lr=4.5672e-04
epoch=0 i=300 loss=6.1025e+00 accuracy=18.75% lr=4.4810e-04
epoch=0 i=350 loss=5.4838e+00 accuracy=37.50% lr=4.3948e-04
epoch=0 i=400 loss=3.7527e+00 accuracy=62.50% lr=4.3086e-04
epoch=0 i=450 loss=6.3017e+00 accuracy=31.25% lr=4.2224e-04
epoch=0 i=500 loss=5.6594e+00 accuracy=31.25% lr=4.1362e-04
epoch=0 i=550 loss=6.4727e+00 accuracy=18.75% lr=4.0500e-04
epoch=1 i=  0 loss=4.4556e+00 accuracy=37.50% lr=3.9983e-04
epoch=1 i= 50 loss=3.5533e+00 accuracy=43.75% lr=3.9121e-04
epoch=1 i=100 loss=3.4252e+00 accuracy=43.75% lr=3.8259e-04
epoch=1 i=150 loss=3.4368e+00 accuracy=50.00% lr=3.7397e-04
epoch=1 i=200 loss=4.2140e+00 accuracy=43.75% lr=3.6534e-04
epoch=1 i=250 loss=3.9455e+00 accuracy=37.50% lr=3.5672e-04
epoch=1 i=300 loss=3.4783e+00 accuracy=50.00% lr=3.4810e-04
epoch=1 i=350 loss=3.0415e+00 accuracy=50.00% lr=3.3948e-04
epoch=1 i=400 loss=3.8950e+00 accuracy=50.00% lr=3.3086e-04
epoch=1 i=450 loss=3.1694e+00 accuracy=43.75% lr=3.2224e-04
epoch=1 i=500 loss=3.0449e+00 accuracy=62.50% lr=3.1362e-04
epoch=1 i=550 loss=3.0913e+00 accuracy=62.50% lr=3.0500e-04
epoch=2 i=  0 loss=3.4003e+00 accuracy=50.00% lr=2.9983e-04
epoch=2 i= 50 loss=4.1388e+00 accuracy=37.50% lr=2.9121e-04
epoch=2 i=100 loss=1.9736e+00 accuracy=75.00% lr=2.8259e-04
epoch=2 i=150 loss=2.8643e+00 accuracy=43.75% lr=2.7397e-04
epoch=2 i=200 loss=1.9511e+00 accuracy=62.50% lr=2.6534e-04
epoch=2 i=250 loss=3.8267e+00 accuracy=62.50% lr=2.5672e-04
epoch=2 i=300 loss=2.1354e+00 accuracy=81.25% lr=2.4810e-04
epoch=2 i=350 loss=2.3076e+00 accuracy=50.00% lr=2.3948e-04
epoch=2 i=400 loss=3.2312e+00 accuracy=62.50% lr=2.3086e-04
epoch=2 i=450 loss=2.7239e+00 accuracy=75.00% lr=2.2224e-04
epoch=2 i=500 loss=2.9212e+00 accuracy=62.50% lr=2.1362e-04
epoch=2 i=550 loss=3.8025e+00 accuracy=43.75% lr=2.0500e-04
epoch=3 i=  0 loss=1.9319e+00 accuracy=56.25% lr=1.9983e-04
epoch=3 i= 50 loss=1.9339e+00 accuracy=62.50% lr=1.9121e-04
epoch=3 i=100 loss=2.1618e+00 accuracy=62.50% lr=1.8259e-04
epoch=3 i=150 loss=2.6811e+00 accuracy=50.00% lr=1.7397e-04
epoch=3 i=200 loss=2.8391e+00 accuracy=68.75% lr=1.6534e-04
epoch=3 i=250 loss=1.9573e+00 accuracy=68.75% lr=1.5672e-04
epoch=3 i=300 loss=2.6342e+00 accuracy=50.00% lr=1.4810e-04
epoch=3 i=350 loss=2.3769e+00 accuracy=50.00% lr=1.3948e-04
epoch=3 i=400 loss=2.2762e+00 accuracy=81.25% lr=1.3086e-04
epoch=3 i=450 loss=1.6482e+00 accuracy=75.00% lr=1.2224e-04
epoch=3 i=500 loss=2.0128e+00 accuracy=62.50% lr=1.1362e-04
epoch=3 i=550 loss=2.3819e+00 accuracy=81.25% lr=1.0500e-04
epoch=4 i=  0 loss=2.0580e+00 accuracy=50.00% lr=9.9828e-05
epoch=4 i= 50 loss=2.6954e+00 accuracy=68.75% lr=9.1207e-05
epoch=4 i=100 loss=1.8408e+00 accuracy=75.00% lr=8.2586e-05
epoch=4 i=150 loss=2.2975e+00 accuracy=62.50% lr=7.3966e-05
epoch=4 i=200 loss=1.7167e+00 accuracy=68.75% lr=6.5345e-05
epoch=4 i=250 loss=1.8718e+00 accuracy=68.75% lr=5.6724e-05
epoch=4 i=300 loss=1.6890e+00 accuracy=81.25% lr=4.8103e-05
epoch=4 i=350 loss=1.7683e+00 accuracy=62.50% lr=3.9483e-05
epoch=4 i=400 loss=2.7356e+00 accuracy=62.50% lr=3.0862e-05
epoch=4 i=450 loss=2.5946e+00 accuracy=62.50% lr=2.2241e-05
epoch=4 i=500 loss=1.9077e+00 accuracy=75.00% lr=1.3621e-05
epoch=4 i=550 loss=2.9126e+00 accuracy=56.25% lr=5.0000e-06
```

测试代码如下:
```python
def test():
    test_loader = torch.utils.data.DataLoader(
        dataset     = Filtered_Dataset['test'],
        batch_size  = 32,
        collate_fn  = collate_fn,
        shuffle     = True,
        drop_last   = True)
    model.eval()
    correct = 0
    total = 0
    for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(test_loader):
        with torch.no_grad():
            out = model(input_ids, attention_mask, token_type_ids)
        out = out.argmax(dim=1)
        correct += (out == labels).sum().item()
        total += len(labels)

    accuracy = correct / total
    print(f"accuracy={accuracy*100:4.2f}%")

print("测试".center(50, "="))
test()
```

测试结果:
```python
========================测试========================
accuracy=55.03%
```

## 总结

本章通过一个填空任务讲解使用 BERT 预训练模型抽取文本特征数据的方法, 事实上填空任务也是 BERT 模型本身在训练时的一个子任务, 所以使用 BERT 模型在做填空任务时往往较好, 而处理不同任务时应该选择合适的预训练模型.

填空任务可以视为多分类任务, 但由于全体词表空间太大, 往往有上万个词, 所以输出类别特别多, 很容易出现过拟合.
本章采用 Dropout 和权重系数衰减来缓解过拟合.

此外分类太多还可能导致梯度消失的问题, 所以在下游任务时不能使用 Softmax 函数激活.